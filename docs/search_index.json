[["index.html", "Welcome to our replication textbook!", " Welcome to our replication textbook! This textbook is designed by students for students interested in applied econometrics, under the guidance of their econometrics professors. Our goal is to present, explain, and share with you substantial Stata code chunks based on real-life examples from well-published articles. In this textbook, you’ll find questions, strategies, and results integrated with the code to enhance clarity and spark curiosity. Our content caters to a wide audience, ranging from beginners in Econometrics 101 to advanced learners. We cover various methodologies, with examples tailored to different skill levels. Our starting point is the existing Stata code available on open data platforms like Dataverse, OpenICPSR, or from the researchers themselves. From these extensive codes, we extract a few examples and refine them to give the code chunks a more pedagogical flavor. Each section replicates a paper with a main result, a figure (when available), and a robustness test (when relevant). Each replication has its own identity, style, and tone, but all include a ‘Highlights’ section explaining the replication and Stata tricks, along with buttons to download the original datasets, a student-created do-file, and a student-created codebook. We are deeply indebted to the authors of the cited articles for their original replication packages. All errors, however, remain ours. Please also note that our replication exercises are not intended to verify or validate findings. We hope you find this textbook informative and engaging as you delve into the world of econometrics. Happy learning! This ongoing project started in the 2023/2024 academic year and continues this upcoming year, with new examples to come! The authors are students in their second year of master’s in Development Economics and Sustainable Development at the Université Paris 1 Panthéon-Sorbonne. We are financed by the Service des Usages Numériques at Université Paris 1. More information about their projects and initiatives can be found here. We thank the University Paris 1 Panthéon-Sorbonne and the Sorbonne School of Economics for their continuous support in this project. "],["randomized-control-trials.html", "Chapter 1 Randomized Control Trials ", " Chapter 1 Randomized Control Trials "],["subsidies-and-the-african-green-revolution-direct-effects-and-social-network-spillovers-of-randomized-input-subsidies-in-mozambique.html", "1.1 Subsidies and the African Green Revolution: Direct effects and social network spillovers of randomized input subsidies in Mozambique", " 1.1 Subsidies and the African Green Revolution: Direct effects and social network spillovers of randomized input subsidies in Mozambique Carter, M., Laajaj, R., &amp; Yang, D. (2021). Subsidies and the African Green Revolution: Direct effects and social network spillovers of randomized input subsidies in Mozambique. American Economic Journal: Applied Economics, 13(2), 206‑229.https://doi.org/10.1257/app.20190396 Download Do-File corresponding to the explanations below Download Codebook Download Original Data (Moz1234panel.dta from Open ICPSR repository) [Link to the full original replication package paper from Open ICPSR] Highlights The paper investigates the short-term and long-term effects (i.e. diffusion through farmers’ networks) of an input subsidy program on agricultural yields. The authors rely on a standard Randomized Controlled Trial in which farmers are randomly assigned to the treatment and control groups, eliminating potential selection bias. Yet, it departs from the standard by exploring spillover effects through social networks, assessing long-term impacts, and addressing informational market failures. The study emphasizes learning processes as part of the intervention’s impact, extending beyond the typical scope of a standard RCT. This document provides a detailed explanation of the original replication package. But you will also learn: how to create a beautiful coefficient plot tailored to your needs, with estimates stored in a matrix (this is how the authors display the main results of their paper) using mkmat together with coefplot, how to create a result table with outreg2, while adding a few statistical tests tailored to your needs in the bottom part with test and scalar, how to automatically generate a new label by keeping / removing certain parts of an existing label with subinstr, how to use a local macro together with replace and append commands in order to loop over a list of variables and store all their corresponding results into a fresh table every time the code is run. 1.1.1 Background elements This study examines the impact of a one-off input subsidy program implemented in Mozambique in 2010 within the context of the Green Revolution and the Alliance for a Green Revolution in Africa. While the Green Revolution transformed Asian and Latin American agriculture, Sub-Saharan Africa lagged behind. In response, the Maputo Declaration (2003) committed African nations to invest 10% of their budgets in agriculture. Launched in 2006, the Alliance aimed to catalyze the Green Revolution in Africa through input subsidy programs (ISPs), providing technologies at discounted rates to randomly selected farmers. Focusing on Mozambique’s 2010 Programa de Suporte aos Produtores, this paper not only assesses subsidy impacts on 704 farmers but also explores post-subsidy persistence and spillover effects through social networks. Findings reveal increased technology adoption, sustained maize yield growth post-program, and notable impacts on farmers’ networks. The program also fostered enhanced beliefs about technology returns, mitigating information-related market failures. 1.1.2 Replication of Table 2 - Regressions with spillover effects This table (Table 2 in the paper) is the output you will get from the code we explain below. Table 2 examines the impact of the government-implemented input subsidy program (ISP) on various agricultural outcomes: fertilizer use on maize, adoption of improved maize seeds, maize yield, daily consumption per capita, and expected yield with the technology package. The objective of this analysis is to determine whether the ISP has a positive and statistically significant impact on the use of fertilizer on maize and on maize yields, measured as average maize yield per hectare. How to read this table. The first two lines of this table present the program’s results estimated on treated farmers (those who received the subsidy). The effects of the program are assessed both during and after the ISP implementation. This table reveals that the program has a positive impact on all farmers’ outcomes, both during and after the program. The effects are more pronounced during the treatment period for fertilizer use, improved maize seeds, and maize yields. The next lines present the impact of the ISP on social network spillovers, which is estimated on members of treated farmers’ social networks during (round 2) and after (rounds 3 and 4) the treatment period, respectively. The general pattern observed is that the coefficients for social network variables are positive and significant in the post-intervention period but not in the intervention period. The magnitude of the coefficients increases as the number of social network contacts in the treatment group increases from one to two, and then stabilizes. This pattern suggests that the effect of social networks on technology adoption and yield increases sharply at two or more social network contacts in the treatment group. For example, after the program, having one contact in the treatment group increases maize yield by 0.18 kilogram per hectare, while having two contacts increases maize yield by 0.53 kilogram per hectare. This substantial increase in yield with two or more social network contacts justifies the authors’ estimation of spillover effects on individuals having above-median (two or more) social network members in the treatment group in Figure 2. Replication code for Table 2 Data importation After having downloaded the original dataset and saved this dataset under your working directory defined with command “cd” (for current directory), you can generate a shorter version of the dataset, named here Dataset_CLY.dta, for which we provide a detailed codebook. *A/ Download and save data &quot;Moz1234panel.dta&quot; from: https://www.openicpsr.org/openicpsr/project/116761/version/V2/view?path=/openicpsr/116761/fcr:versions/V2.1/Rep-file-Moz-Input-Subsidy/data/original/Moz1234panel.dta&amp;type=file *B/ Set your working directory where Moz1234panel.dta already is: cd &quot;XXX&quot; /*XXX=the directory where you just saved the &#39;Moz1234panel.dta&#39; dataset*/ *C/ Create a shorter dataset: use &quot;Moz1234panel.dta&quot;, clear * selecting the necessary avriables for replicating the Table 2, Figure 2, and Table A7 keep sn5up_sub_dur sn2up_sub_aft nw_talkedagmoder lfertmaizr sn2_sub_dur sn2up_sub_dur sn1_sub_aft lyieldr sn3_sub_aft lexp_yield_fertr vouch_aft_r3 sn2up_sub_aft_r3 limprovedseedsr vouch_aft sn1_sub_dur vlgid_round respid sn2up_sub_aft_r4 vouch_aft_r4 vouch_dur ldailyconsr sn3_sub_dur sn4_sub_aft sn5up_sub_aft sn4_sub_dur sn2_sub_aft fertmaizr vouch round * exponentiation of the winsorized variables back to their original forms foreach x in improvedseedsr yieldr dailyconsr exp_yield_fertr{ // it is necessary in order to add the means of the control to the table 2. This way it uses the value that is not Winsorized. gen `x&#39; = exp(l`x&#39;) } save &quot;Dataset_CLY.dta&quot;, replace To start the replication, one should always start by deleting any potential data in memory using clear all. Then, open the dataset of interest, and the command use “…”, clear to indicate the dataset you want to import in Stata. Because this project uses panel data, you need to inform Stata of the structure of the data. Use the command xtset to indicate that you are working with panel data, followed by the name of the variable you want to set as the panel variable. Here, vlgid_round is an identifier for every possible combination of locality and round. clear all use &quot;Dataset_CLY.dta&quot;, clear xtset vlgid_round global sn_treatments sn1_sub_dur sn2_sub_dur sn3_sub_dur sn4_sub_dur sn5up_sub_dur sn1_sub_aft sn2_sub_aft sn3_sub_aft sn4_sub_aft sn5up_sub_aft Now, you want to modify the labels for a set of variables that were in log format in the original dataset, to indicate that they are in their original scale. Start with creating a loop to iterate the same procedure over a set of variables, using the foreach command. Stata trick: Then, you can easily modify the labels using variable label to extract the label for each variable in the loop, remove the text “log” from the label using subinstr, remove the excess spaces using trim, and update the variable label with the modified text. foreach v in fertmaizr improvedseedsr yieldr dailyconsr exp_yield_fertr { local x : variable label l`v&#39; local x = trim(subinstr(&quot;`x&#39;&quot;,&quot;(log)&quot;,&quot;&quot;,.)) label variable l`v&#39; &quot;`x&#39;&quot; } Regressions, and table creation You are now set to running the regressions and creating a table presenting results. Stata trick: Start with initializing a local macro using local, named rep_app, and set its value to replace. The purpose of this macro is to control whether the replace option is used when exporting regression results in an Excel file using the command outreg2. In other words, it means that if the file already exists in your computer, it will be replaced. local rep_app = &quot;replace&quot; Again, use a loop to iterate regressions over the same list of variables using foreach. You want to apply several commands to the variables included in the loop only if they meet the following conditions: round==2 (survey round 2, that is, during the program) and vouch==0 (farmers who did not win the voucher, that is, the control group). Then: qui sum calculates the mean, and results are stored using the local command areg runs a fixed effects regression on panel data, using absorb to absorb fixed effects, and cluster to take into account the intra-cluster correlations outreg2 creates the regression table and saves it into an Excel file (.xls extension) Then, create a nice table using key commands: bracket generates brackets for standard errors label includes variable labels in the results table to display full variable names nocons, nor2, and noni are used to avoid displaying the constant, the R-squared and missing values in the table less(1) means that only the first row of the result table will be included (only the first regression) keep allows you to keep only the variables you want adds() is used to add control variables into the brackets foreach x in fertmaizr improvedseedsr yieldr dailyconsr exp_yield_fertr { qui sum `x&#39; if vouch==0 &amp; round==2 local control_mean=r(mean) qui xi: areg l`x&#39; vouch_dur vouch_aft ${sn_treatments} i.nw_talkedagmoder*i.round, /// absorb(vlgid_round) cluster(respid) outreg2 using &quot;table 2.xls&quot;, `rep_app&#39; bracket label nocons noni /// less(1) nor2 keep(vouch_dur vouch_aft ${sn_treatments}) adds(mean_control,`control_mean&#39;) local rep_app = &quot;append&quot; } 1.1.3 Replication of Figure 2 - Direct and Spillover Impacts of Subsidies This figure (Figure 2 in the paper) is the output you will get from the code we explain below. This figure highlights the Intention-to-Treat estimates through the direct and indirect effects on five outcomes of the subsidy program on the farmers and on their social networks. The outcomes of interest include the adoption of fertilizer and adoption of improved maize seeds, the maize yields, the consumption of the household (an indirect measure of agricultural profit), and the expected yields to the technology package. In this figure, indirect effects are estimated on individuals having at least two contacts in the treatment group, because they are the ones for whom effects are the largest, as demonstrated in Table 2. How to read this figure. The coefficient of the regressions of interest are represented with dots, while the lines represent 95 percent confidence intervals. Direct impacts of the subsidies on the voucher recipient group are estimated for all five outcomes for both the “during” (subsidized) and the “after” (post-subsidy) periods. The Figure shows that the program had a large and significant impact during the subsidy period on the adoption of fertilizers and improved seeds, also allowing for higher maize yields. The effects on consumption were only significant after the subsidy period. Yield expectations were stable across periods for treated farmers. While the effect on fertilizer use decreases in magnitude after the subsidy period, it remains substantial and statistically significant. The impact on agricultural yields also persists over time. Regarding spillover effects, impacts of the program are positive and significant on the adoption of fertilizers, on improved seeds, maize yields, and expected returns to the technology package. Replication code for Figure 2 Data preparation and confidence intervals calculation So far, a lot of modifications have been made to the variables. To construct the figure, you need the original data, without saving the transformations to the variables made until now. use &quot;Dataset_CLY.dta&quot;, clear Install the parmest package that allows for semi-parametric estimation of partially linear models. ssc install parmest Then, regress the outcome variables on the treated individuals (farmers who won the vouchers) and on individuals who have at least two contacts in the treatment group. Regressions are run separately for each of the five outcomes, using a loop - see section 1.2.2 for more information on how to create a loop. Cluster the standard error at the level of respid (respid is the variable for the respondent’s identifier, that is, cluster at the individual level), and use parmest to save the regression results in the .dta format. For each variable, you will obtain the estimated coefficients and standard errors for the periods during and after the program, for treated farmers and those with at least two contacts in the treatment group. foreach x in lfertmaizr limprovedseedsr lyieldr ldailyconsr lexp_yield_fertr { qui xi: reg `x&#39; vouch_dur vouch_aft sn2up_sub_dur sn2up_sub_aft /// i.nw_talkedagmoder*i.round i.vlgid_round, cluster(respid) parmest, saving(`x&#39;, replace) } To generate the final figure, you must create a new dataset. To do so, you need to iterate through variable names and perform certain operations on our variables. First, use the local command to define a local macro y with a value of 1. Then create a loop using foreach which includes our variables of interest. Then, apply the following commands on variables in the loop: use `x'.dta loads the dataset corresponding to the variables in the loop use the command gen to create a time variable using the observation number (_n), drop the observations for which time is greater than 4 using drop This graph aiming to generate a visually informative figure by plotting mean estimates along with their corresponding 95% confidence intervals for a set of variables. Only keep the variables estimate, stderr and time Use the round command to round the values of the variables to 2 decimal points Generate, using gen, two new variables with missing values which capture the estimates of each regression after the treatment and the standard error of these coefficients (respectfully estimate_NV and sd_NV, followed by the value taken by y). The command replace allows us to assign specific values to the new variables. With the command rename, include a suffix based on the local macro y’ to the selected variables. Drop all periods after round 2 because you only want the first two periods in the figure Creating a local y equal to y + 1, increment y to the next rank to move on to the next variable estimate_Vy+1 and estimate_NV+1 Eventually, save the variables created with the command save. local y = 1 foreach x in lfertmaizr limprovedseedsr lyieldr ldailyconsr lexp_yield_fertr { use `x&#39;.dta gen time=_n drop if time&gt;4 keep estimate stderr time replace estimate=round(estimate,0.01) replace stderr=round(stderr,0.01) gen estimate_NV`y&#39;=. replace estimate_NV`y&#39;=estimate[3] in 1 replace estimate_NV`y&#39;=estimate[4] in 2 gen sd_NV`y&#39;=. replace sd_NV`y&#39;=stderr[3] in 1 replace sd_NV`y&#39;=stderr[4] in 2 rename estimate estimate_V`y&#39; rename stderr sd_V`y&#39; local y = `y&#39;+1 drop if time&gt;2 save, replace } Merge the quantity of fertilizer used with each variable of interest (again, using a loop) based on the time variable. use lfertmaizr, clear foreach x in limprovedseedsr lyieldr ldailyconsr lexp_yield_fertr{ merge 1:1 time using `x&#39;.dta, nogen save figure2.dta, replace } Clear existing matrices and results in Stata’s memory, and load the dataset named “figure2.dta”. clear matrix clear results use &quot;figure2.dta&quot;, clear Calculate lower and upper limits of the 95% confidence interval: Create a foreach loop to iterate over a list of variables(V1, V2, …, NV5). Use the mean estimate (estimate_) and standard deviation (sd_) to calculate the lower and upper bounds of the 95% confidence interval for each variable by subtracting 1.96 times the standard deviation from the values taken by each variable in the loop. Set the upper limit to 1 if it’s greater than 1, in other words, substitute 1 for any variable value exceeding 1. Create a matrix with mkmat to store the results (estimate, lower limit, and upper limit) for each variable in the loop. foreach i in V1 V2 V3 V4 V5 NV1 NV2 NV3 NV4 NV5{ gen ll95_`i&#39; = estimate_`i&#39; - 1.96*sd_`i&#39; gen ul95_`i&#39; = estimate_`i&#39; + 1.96*sd_`i&#39; replace ul95_`i&#39;=1 if ul95_`i&#39;&gt;1 mkmat estimate_`i&#39; ll95_`i&#39; ul95_`i&#39;, matrix(`i&#39;) } Package installation and graph style settings These preprocessing steps are crucial for ensuring accurate and visually compelling representations of the estimated coefficients with their associated confidence intervals. The resulting figure is a valuable tool for conveying the uncertainty surrounding the mean estimates, aiding in the interpretation and communication of statistical findings. Install and initialize additional packages (grstyle and coefplot) for graph styling and coefficient plotting. Set graph style settings, such as background color and major grid color. ssc install grstyle ssc install coefplot grstyle init grstyle color background white grstyle color major_grid white Graph creation and exportation The command coefplot creates a graphic displaying the coefficients of the regression and the confidence interval. - matrix selects the values to be used for the graph in the matrix. - ci selects the upper and lower bounds of the confidence interval - msize is used to select the size of the graph lines. - xline(0, lpattern(solid) lw(thick)) adds a vertical line to the x-axis at position 0, with a line style of “solid” and a line thickness of “thick”. - xlabel(-0.5(0.5)1) adds axis labels from -0.5 to 1, with a step of 0.5. - bylabel is used to define labels and byopts is used to define the chart title. - ciopts(lwidth(thick thick)) defines the line thickness for confidence interval. Reproduce 5 times this process for each variable, during and after the treatment. coefplot /* */ (matrix(V1[,1]), ci((V1[,2] V1[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV1[,1]), ci((NV1[,2] NV1[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1))|| /* */ ||, bylabel( ) byopts(title(&quot;Fertilizer on maize&quot;)) xsize(2) scale(1.35) /// ylabel(,labsize(vlarge)) ciopts(lwidth(thick thick)) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(fertilizer) coefplot /* */ (matrix(V2[,1]), ci((V2[,2] V2[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV2[,1]), ci((NV2[,2] NV2[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ ||, byopts( title(&quot;Improved maize seeds&quot;)) bylabel( ) xsize(2) scale(1.35) /// ylabel(,labsize(vlarge)) ciopts(lwidth(thick thick)) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(improved) coefplot /* */ (matrix(V3[,1]), ci((V3[,2] V3[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV3[,1]), ci((NV3[,2] NV3[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1))|| /* */ ||, byopts( title(&quot;Maize yield&quot;)) bylabel( ) xsize(2) scale(1.35) /// ylabel(,labsize(vlarge))ciopts(lwidth(thick thick) ) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(yield) coefplot /* */ (matrix(V4[,1]), ci((V4[,2] V4[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV4[,1]), ci((NV4[,2] NV4[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ ||, byopts( title(&quot;Consumption&quot;)) bylabel( ) xsize(2) scale(1.35) /// ylabel(,labsize(vlarge))ciopts(lwidth(thick thick) ) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(consumption) coefplot /* */ (matrix(V5[,1]), ci((V5[,2] V5[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV5[,1]), ci((NV5[,2] NV5[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ ||, byopts( title(&quot;Expected yield with technology package&quot;)) bylabel( ) xsize(2) /// scale(1.35) ylabel(,labsize(vlarge)) ciopts(lwidth(thick thick)) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(expected) Combine all the graphs in one graph and edit it: b1title(Estimated coefficients, size(small) color(black)) adds a title to the top of the combined chart, stating “Estimated coefficients.” The title size is “small” and the text color is “black.” cols(1) charts are combined into a single column. altshrink altering scaling of text iscale(*3.2) is used to select the graph scale. The graphs here are enlarged by 3.2 times. xcommon means that the x is common to all graphs. imargine define the interior margins. graph combine fertilizer improved yield consumption expected, /// b1title(Estimated coefficients, size(small) color(black)) cols(1) altshrink iscale(*3.2) /// xsize(3) xcommon imargin(b=1 t=1) title(&quot;Direct impact&quot; &quot;on treatment group&quot;, /// size(small) color(black) position(11)) subtitle(&quot;Spillover impact via social&quot; /// &quot;network contacts&quot;, size(small) color(black) position(1)) Finally, export the graph in .png and .pdf with the function gr export. gr export figure2.png, replace gr export figure2.pdf, replace 1.1.4 Replication of Table 7 - Separated Estimation of Spillover Effects for the First and Second Years After the Program In both Table 2 and Figure 2 presented above, researchers estimate a single “after” treatment effect, pooling the two years after the intervention period. In Table 7, the researchers distinguish between spillover effects one year after, and two years after the program, separately. The objective of this table is to push the analysis of spillovers further, and is similar to conduct a robustness check in the context of this study. To dissect spillover effects over time, researchers introduce two different “after” indicators for each of the post-subsidy years. As expected, due to reduced power, fewer coefficients reach statistical significance. There is no clear systematic pattern to the coefficients across the two years, and the hypothesis that the direct and spillover effects are consistent between the first and second “after” years cannot be rejected. Replication code for Table 7 Data preparation Start with initializing a local macro using local, named rep_app and set its value to replace. For more information, see section explaining Table 2 above. use &quot;Dataset_CLY.dta&quot;, clear local rep_app = &quot;replace&quot;&quot; Begin by correctly labeling the variables that will appear in Table 7. local rep_app = &quot;replace&quot; label var vouch_dur &quot;Direct impacts during&quot; label var vouch_aft_r3 &quot;Direct impacts 1 year after&quot; label var vouch_aft_r4 &quot;Direct impacts 2 year after&quot; label var sn2up_sub_dur &quot;Spillover impacts during&quot; label var sn2up_sub_aft_r3 &quot;Spillover impacts 1 year after&quot; label var sn2up_sub_aft_r3 &quot;Spillover impacts 2 year after&quot; Regressions, and table creation The code for creating Table 7 is very similar to that for creating Table 2 (see above). The interesting novelties are the following: Using a loop, run a regression with areg, adding the prefix xi- to indicate that the regression includes indicator values (equal to either 0 or 1), and including fixed effects which will be absorbed for the variable vlgid_round with the option absorb. Standard errors are clustered at the individual level (respid variable). The command qui, for “quietly”, means that the results will not be displayed. Then test the equality of coefficients between vouch_aft_r3 and vouch_aft_r4, and between sn2up_sub_aft_r3 and sn2up_sub_aft_r4 using the test command. This command performs Wald tests of simple and composite linear hypotheses about the parameters of the most recently fitted model. This allows us to test whether there is a significant difference between the coefficients for period 3 and period 4. Next, store the p-value results using scalar. The last step is to export the results you have just obtained into our results folder using outreg2. To create a nice table easily, refer to explanations from Table 2. foreach x in lfertmaizr limprovedseedsr lyieldr ldailyconsr lexp_yield_fertr { qui xi: areg `x&#39; vouch_dur vouch_aft_r3 vouch_aft_r4 sn2up_sub_dur sn2up_sub_aft_r3 /// sn2up_sub_aft_r4 i.nw_talkedagmoder*i.round, absorb(vlgid_round) cluster(respid) test vouch_aft_r3 = vouch_aft_r4 scalar vou_di = r(p) test sn2up_sub_aft_r3 = sn2up_sub_aft_r4 scalar sn_di = r(p) outreg2 using &quot;table A7.xls&quot;, `rep_app&#39; bracket label nocons noni less(1) nor2 /// keep(vouch_dur vouch_aft_r3 vouch_aft_r4 sn2up_sub_dur sn2up_sub_aft_r3 /// sn2up_sub_aft_r4) adds(&quot;vouch_aft dif (r3-r4) p-value&quot;, vou_di, /// &quot;sn2up_sub_aft dif (r3-r4) p-value&quot; , sn_di ) local rep_app = &quot;append&quot; } Authors: Elvire Jégu, Ambre Delaunay, Chiara Balducci, Yagmur Helin Aslan, students in the Master program in Development Economics and Sustainable Development (2023-2024), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2023 "],["instrumental-variables.html", "Chapter 2 Instrumental Variables ", " Chapter 2 Instrumental Variables "],["winners-and-losers-from-agrarian-reform-evidence-from-danish-land-inequality-16821895.html", "2.1 Winners and losers from Agrarian Reform: Evidence from Danish land inequality 1682–1895", " 2.1 Winners and losers from Agrarian Reform: Evidence from Danish land inequality 1682–1895 Boberg-Fazlić, N., Lampe, M., Lasheras, P. M., &amp; Sharp, P. (2022). Winners and losers from Agrarian Reform: Evidence from Danish land inequality 1682–1895. Journal of Development Economics, 155, 102813. https://doi.org/10.1016/j.jdeveco.2021.102813 Download Do-File corresponding to the explanations below Download Codebook Download Original Data (OSF repository: click on WinnersandLosers_finaldata.dta) [Link to the full original replication package paper on OSF] Highlights The paper examines the distributional effects of land reforms between 1682-1895 in Denmark. The authors use an Instrumental Variable (IV) to deal with endogeneity issues stemming from reverse causality between the variable of interest (land productivity) and the dependent variable (land inequalities). This methodology is standard by the choice of a geological instrument. It follows the usual steps of the IV procedure, including tests of the instrument’s pertinence (F-statistic of the 1st stage estimation). Our added value to the original replication package lies mainly in the detailed explanation of the provided code. Additionally, we show how to extract various tables and figures directly from the Stata code. We explain these Stata tricks: Diagnostic Statistics: We leverage the estat firststage command to obtain diagnostic statistics pertaining to the first-stage of instrumental variable estimation, which includes extracting and storing the F-statistics for further analysis. Looping: The foreach loop structure is used, which enables the iteration over multiple years for instrumental variable regressions and allows to avoid repetitive commands in the code. Advanced Formatting: Through the utilization of the esttab command along with various formatting options, we create well-formatted tables, which results in the production of tables that are not only clear and organized but also visually appealing. 2.1.1 Introduction As a rule, agrarian reforms are viewed as fundamental for economic development, allowing, on one hand, to fuel agricultural productivity and on the other, to reallocate the necessary productive resources to industrialization. Their design has to fulfill two competing objectives: stimulating farms’ productivity and ensuring an equitable access to land. As numerous reforms have been criticized for failing the latter, this paper provides the first quantitative long-term assessment of the Danish agrarian reforms’ effects on both economic efficiency and land inequalities. To do so, the authors combine several sources of historical data on farms at the parish level with population and agricultural censuses to cover the 1682-1895 period. First, they document that the agrarian reform happened between 1784 and 1810, with a unification of property rights, favoring middle-sized farms. Then, they get the number and sizes of farms per parish for the years 1682, 1834, 1850, 1860, 1873, 1885 and 1895 from Danish land registers, as well as their productive capacity measured in units of barrel of hard grain (HK), which was the unified measure used for tax collection. Thanks to this data, they can calculate a measure for inequality within parishes for the several data points (their explained variable). They opt for the use of the Theil index to measure land inequality due to its analytically desirable properties. One key advantage is that the Theil index adheres to the principle of transfers, ensuring that a redistribution from one individual to a less affluent one results in a proportional decline in the Theil index, which is particularly convenient for focusing on changes in inequalities over time. Moreover, the Theil index provides unambiguous rankings of distributions, ensuring that two regions with identical Theil indices exhibit identical income distributions (not guaranteed by other common measures such as the Gini index). If a parish have all farms of equal productive size, then its Theil index is 0 ; if only one farm holds all land, its Theil index is the logarithm of the number of households living in the parish. Finally, they consider the natural logarithm of the total value of the land (measured in HK) of parish p as their main explanatory variable. Recall that their objective is to estimate the effect of productive capacity on inequality. From an econometric point of view, the most obvious approach would be to regress changes in parish-level land inequalities on a measure of soil productivity. Nonetheless, such a specification would be exposed to endogeneity issues, due to the reverse causality between the explained variable (evolution in land inequalities) and the variable of interest (land productivity): as higher agricultural productivity leads to Malthusian dynamics, in parishes with more fertile soils, there would be more smallholders and landless individuals, whose socio-economic status would then be deteriorated by the reforms. Consequently, areas with a higher agricultural productivity were more exposed to rises in land inequalities. At the same time, population growth has beneficial effects on innovation and thus, agricultural productivity. To tackle this, the authors adopt an IV strategy and choose as an instrument for land productivity the distribution of “Boulder Clay”, the sediment type most adapted to barley, resulting from the Weichselian glaciation. Geological variables are generally viewed as reliable instruments, since they capture long-term determinants of development that are independent from human factors. So their IV is the share of parish area classified as boulder clay (invariant in time). 2.1.2 Identification strategy When deciding to use a 2SLS strategy, several points need to be discussed in order to allow the identification of robust causal effects. The first hypothesis to be considered and which cannot be tested statistically is the exclusion restriction. It is necessary to rule out any direct effect of the instrument (boulder-clay) on the dependent variable (land inequalities). In this specific case, it can be assumed that soil composition doesn’t directly affect the level of land inequalities. In fact, the authors argue that the rise of inequalities is driven by a stronger demographic growth, due to the productive capacity of the land. This implies, beside the soil fertility, adequate land management practices and efficient agricultural technologies. The authors also have to exclude any effect of the dependent variable on the instrument. Here, once again, land inequalities and the soil fertility seem to be unrelated, as the soil’s share of boulder clay stems from the Weichselian glaciation, which occurred approximately 18,000 years ago. Moreover, the sediment classification they use was made below the impact area of cultivation practices and technologies, which allows to infirm a potential effect of inequalities on this measure of land fertility. The second hypothesis to be respected is the instrument’s relevance. The authors need to convincingly describe how the instrument affects the endogenous variable. In our case, the soil composition represents a key determinant of the land’s productivity and thus is supposed to be positively correlated with the total production of an area measured by the variable TotalHK. Unlike the aforementioned exclusion restriction, this assumption can be tested statistically. It can be done, for instance, by verifying whether after estimating the first-stage specification (1), the coefficient of the instrumental variable is statistically significant or whether the F-statistic is superior to the conventionally fixed value of 10. \\[\\begin{equation} \\tag{1} ln(TotalHK)_{p}=α_{0}+βBoulderClay_{p}+λ_{r}+X_{p}γ+ϵ_{p} \\end{equation}\\] As we can see in column 2 of Table 1, the effect of the soil composition on total production is statistically significant at 1% level. Also, the value of the F-statistic is equal to 280. Hence, we can conclude that both steps necessary to ensure the relevance and the exogeneity of the instrument have been fulfilled. That said, they further estimate the second-stage specification (2). \\[\\begin{equation} \\tag{2} ΔTheil_{p}=α_{0}+β\\widehat{ln(TotalHK)}_{p}+λ_{r}+X_{p}γ+ϵ_{p} \\end{equation}\\] After estimating both the OLS and the second-stage IV specifications, the authors find statistically significant positive effects of land productivity on land inequalities during the agrarian reforms. To ascertain the robustness of these findings, they estimate additional specifications, using alternative measures of land inequalities – such as Gini index, an alternative to the Theil index – and of land productivity – the amount of barley paid in tax. As no significant change in the results was detected, we can conclude that the econometric estimation allowed them to confirm their predictions, exposed in the theoretical part of the paper. In the next part of our narrative, we will discuss some of the main figures of the paper. We will also indicate the necessary commands to replicate them using Stata. 2.1.3 From the article to practice: exploring the replication code 2.1.3.1 Getting started: database access and required packages In order to open the Stata database and execute the following lines of code, several packages need to be downloaded and installed. In this section, we guide you through the process of accessing the database and briefly refer to these packages. In what follows, you have to download the original dataset from a link, then run these first lines of code to get the shorter version of the dataset, for which we also provide you with a codebook (see header). ***Open the database*** *download and save WinnersandLosers_finaldata.dta from: https://osf.io/jmn5y/files/osfstorage?view_only=f18f6d51efe44f04abef6e9042c0163c *define your working directeory, where you also just stored the dataset cd &quot;C:\\Users\\&quot; /*C:\\Users\\ = path where you also store the dataset*/ use &quot;WinnersandLosers_finaldata.dta&quot;, clear keep ID BygLG Lat Long year MLmean Theil_c AggTheil_c gini1682 gini1834 region ln_area LnDistCoast LnDistCPH ln_TotalFarmHK ln_Theil_1682c ln_Theil_1834c ln_Theil_1850c ln_Theil_1860c ln_Theil_1873c ln_Theil_1885c ln_Theil_1895c year_1 Gini ln_TotalFarmHK1682 ln_TotalFarmHK1834 ln_TotalFarmHK1850 ln_TotalFarmHK1860 ln_TotalFarmHK1873 ln_TotalFarmHK1885 ln_TotalFarmHK1895 ln_TotalFarmHK1682_nohouses ln_TotalFarmHK1834_nohouses ln_TotalFarmHK1850_nohouses ln_TotalFarmHK1860_nohouses ln_TotalFarmHK1873_nohouses ln_TotalFarmHK1885_nohouses ln_TotalFarmHK1895_nohouses save &quot;Dataset_AA_ALC_AT.dta&quot;, replace Subsequently, several Stata packages are necessary to execute the replication code successfully. The first package needed is the estout package. This package allows to make regression tables using regressions previously stored in the Stata memory. The second package required is the ivreg2 package. This package allows to run instrumental variables regressions. The third package is the coefplot package. This package is used to create coefficients plots which visually represent the estimated coefficients and their confidence intervals. The fourth package is outreg2. This package is used to produce illustrative tables of regression outputs. This package is able to write LaTex-format tables. All the packages can be installed using the following lines of code. ***Install the required packages*** ssc install estout ssc install ivreg2 ssc install coefplot ssc install outreg2 2.1.3.2 Understanding the replication process: code analysis of Table 1 Table 1: IV estimation using total HK and the share of parish area classified as boulder clay ***Table 1*** use &quot;Dataset_AA_ALC_AT.dta&quot;, clear ***Column (1) : OLS*** eststo ols1: qui reg D.Theil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast /// i.region if year==1834, vce(robust) eststo ols2: qui reg D.AggTheil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast /// i.region i.year if year==1834, vce(robust) ***Column (2) : First-stage*** eststo fiv1: qui reg ln_TotalFarmHK MLmean ln_area LnDistCPH Lat Long LnDistCoast /// i.region if year==1834, vce(robust) ***Column (4) : Reduced form*** eststo ziv1: qui reg D.Theil_c MLmean ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834, vce(robust) ***Column (3) : Second-stage (with diffTheil)*** eststo ivdiff1: qui ivregress 2sls D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (5) : Second-stage (with diffAggTheil)*** eststo ivdiff2: qui ivregress 2sls D.AggTheil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (6) : Second-stage (with Gini)*** eststo ginihkdiff: qui ivregress 2sls D.Gini ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Formation of Table 1*** esttab ols1 fiv1 ivdiff1 ziv1 ivdiff2 ginihkdiff, se star(* 0.10 ** 0.05 *** 0.01) b(3) /// r2 var(15) model(12) wrap keep(MLmean ln_TotalFarmHK) mtitles(&quot;OLS&quot; &quot;first stage&quot; /// &quot;second stage (diffTheil)&quot; &quot;reduced form (diff Theil)&quot; &quot;second stage (diffAggTheil)&quot; /// &quot;second stage (diffGini)&quot; ) stats(N r2 Fstat, labels(&quot;Observations&quot; &quot;R-squared&quot; /// &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) indicate(&quot;Region FE = 2.region&quot; /// &quot;Geography = LnDistCPH&quot; , labels(Y N)) label OLS regressions by replicating the columns 1,2 and 4 of Table 1 ***Table 1*** eststo clear ***Column (1) : OLS*** eststo ols1: qui reg D.Theil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast /// i.region if year==1834, vce(robust) eststo ols2: qui reg D.AggTheil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast /// i.region i.year if year==1834, vce(robust) ***Column (2) : First-stage*** eststo fiv1: qui reg ln_TotalFarmHK MLmean ln_area LnDistCPH Lat Long LnDistCoast /// i.region if year==1834, vce(robust) ***Column (4) : Reduced form*** eststo ziv1: qui reg D.Theil_c MLmean ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834, vce(robust) Columns 1, 2, and 4 of Table 1 represent two Ordinary Least Squares (OLS) regressions, a first-stage, and a reduced form, respectively, representing distinct stages through OLS regressions. These different model specifications are often used in econometric analyses to examine causal relationships between variables. These columns offer a comprehensive understanding of underlying economic relationships and allow for nuanced interpretation of results. The singular variable differing across the two OLS regressions is the initial variable, corresponding to the dependent variable: the change in the Theil index from 1682 to 1834 D.Theil_c and the change in the Theil index of 1834 aggregated to the size categories of 1682 D.AggTheil_c, respectively. In line with our approach of conducting both the first-stage and the reduced form analyses, we introduce the instrumental variable MLmean in the two last regression equations. The replication process of these columns involves several crucial steps in estimating econometric models. Firstly, the use of the eststo clear command ensures a reset of previous estimation results, creating a clean environment before running new estimations. Subsequently, the inclusion of qui – for quietly – with the reg – for regress – function allows for the temporary storage of regression results in a named matrix, such as ols1 in the first model. Moreover, the model specification, denoted as D.Theil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast i.region, details the variables included in the regression equation. It is crucial to note that this specification is essential for understanding the relationships between key variables and the dependent variable – here D.Theil_c or D.AggTheil_c. Furthermore, the use of the conditional if year==1834 filters the data, retaining only observations where the year variable is equal to 1834. This filtering step enables a focus on a particular year – here 1834 –, allowing for a more targeted analysis. Finally, the vce(robust) option is specified to estimate robust standard errors, thereby addressing potential heteroskedasticity issues. These robust standard errors are particularly important to ensure the reliability of estimation results, especially when residuals exhibit heterogeneous variations. IV regressions by replicating columns 3,5 and 6 of Table 1 ***Table 1*** ***Column (3) : Second-stage (with diffTheil)*** eststo ivdiff1: qui ivregress 2sls D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (5) : Second-stage (with diffAggTheil)*** eststo ivdiff2: qui ivregress 2sls D.AggTheil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (6) : Second-stage (with Gini)*** eststo ginihkdiff: qui ivregress 2sls D.Gini ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] The columns 3, 5, and 6 which represent second-stages, are estimated through instrumental variable regressions. Similar to OLS regressions, the command eststo ivdiff1 : qui is used to store the estimation results in a matrix named ivdiff1, ensuring an organized storage of relevant information. Secondly, the function ivregress 2sls is applied to conduct a two-stage IV regression, as indicated by 2sls. Additionally, the explanatory variables D.Theilc lnarea LnDistCPH Lat Long LnDistCoast i.region mirror those used in the OLS regressions, providing continuity in the model specification. The only variable differing across each column specification is the initial variable, corresponding to the dependent variable : the change in the Theil index from 1682 to 1834 D.Theilc, the change in the Theil index of 1834 aggregated to the size categories of 1682 D.AggTheilc, and the change in the Gini coefficient instead of the Theil index D.Gini, respectively. This nuanced change reflects the distinct focus on different dependent variables in each instance, while the remaining explanatory variables remain constant, allowing for a systematic exploration of the impact of the selected variables on the varied dependent variables. The inclusion of (lnTotalFarmHK = MLmean) specifies the endogenous variable lnTotalFarmHK and its instrument MLmean. This crucial specification distinguishes the endogenous variable and its corresponding instrument, a fundamental aspect of instrumental variable estimation. Furthermore, to address heteroskedasticity and autocorrelation, the vce(robust) command employs a robust variance-covariance matrix, ensuring the reliability of standard errors in the estimation. The upcoming lines of code that we are about to describe initially seemed somewhat unclear to us. However, despite the initial ambiguity, they prove to be valuable and, upon closer examination, are now better understood. This is why we will take the time to provide a clear explanation of these lines. First, the line estat firststage is employed to display statistics from the first-stage of the IV regression. This step is typically utilized to assess the validity of instruments and ascertain their suitability for the estimation process. Secondly, storing the first-stage results in a matrix named ‘fstat’ is accomplished through the line mat fstat = r(singleresults). This matrix captures relevant statistics from the first-stage, providing a comprehensive view of the instrumental variable performance. Finally, the line estadd scalar Fstat = fstat[1,4] introduces a new scalar variable, Fstat, into the main regression results. This step allows to extract the F-statistic from the first-stage matrix fstat and assigns it to Fstat. The F-statistic is a crucial metric in instrumental variable regression, serving as a diagnostic tool to assess the overall validity of the instruments used. A high F-statistic, that is superior to 10, suggests that the instruments collectively have a strong explanatory power for the endogenous variable. In summary, these lines of code are essential for evaluating the quality and relevance of the instrumental variables employed in the two-stage IV regression. Formatting Table 1, an additional but optional step ***Table 1*** ***Formation of Table 1*** esttab ols1 fiv1 ivdiff1 ziv1 ivdiff2 ginihkdiff, se star(* 0.10 ** 0.05 *** 0.01) b(3) /// r2 var(15) model(12) wrap keep(MLmean ln_TotalFarmHK) mtitles(&quot;OLS&quot; &quot;first stage&quot; /// &quot;second stage (diffTheil)&quot; &quot;reduced form (diff Theil)&quot; &quot;second stage (diffAggTheil)&quot; /// &quot;second stage (diffGini)&quot; ) stats(N r2 Fstat, labels(&quot;Observations&quot; &quot;R-squared&quot; /// &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) indicate(&quot;Region FE = 2.region&quot; /// &quot;Geography = LnDistCPH&quot; , labels(Y N)) label The next stage in our analysis involves the creation of a comprehensive table summarizing the results of the previously conducted regressions. It is essential to note that this step does not stand alone, rather it complements the preceding two steps in which variables were defined and regressions were estimated. The resulting table serves as a visual representation of the relationships captured in the regression models, enhancing the interpretability and communicative power of the findings. First, the command esttab ols1 fiv1 ivdiff1 ziv1 ivdiff2 ginihkdiff specifies which models to include in the table, incorporating the Ordinary Least Squares (OLS), first-stage instrumental variable (IV), and second-stage IV regression results. To enhance the clarity of the table, additional formatting options are applied. The se star(* 0.10 ** 0.05 *** 0.01) command displays standard errors with significance stars, denoting significance levels with asterisks. Thirdly, the b(3) command allows to limit coefficient estimates to three decimal places, contributing to a cleaner presentation of results. Moreover, the inclusion of r2 displays the coefficient of determination, the R-squared, in the table, providing insights about the explanatory power of the model. The var(15) option limits the display of R-squared to 15 decimal places. Additionally, with model(12), we specify the maximum number of models to display in the table, accommodating up to 12 different model specifications. The wrap command facilitates the wrapping of long variable names onto multiple lines, ensuring readability. The keep(MLmean lnTotalFarmHK) option selectively includes only the variables MLmean and lnTotalFarmHK in the table, which are our instrument and endogenous variables. Furthermore, mtitles() assigns model titles to each specified model, contributing to a more informative and organized presentation. The stats(N r2 Fstat, labels(\"Observations\" \"R-squared\" \"KP F-statistic\") fmt(%9.0fc 2 2)) option adds relevant statistics to the table, including the number of observations (N), the R-squared, and the F-statistic. The indicate (\"Region FE = 2.region\" \"Geography = LnDistCPH\", labels(Y N)) command introduces indicator in a regression model. The first part, \"Region FE = 2.region\", includes fixed effects for a specific region, here denoted as 2, representing Jutland. This accounts for unobserved variation specific to Jutland. The second part, \"Geography = LnDistCPH\", introduces an indicator variable related to geography. The labels Y and N are assigned to indicate the presence or absence of fixed effects or controls in each observation, contributing to the model’s interpretability. This step allows for a nuanced interpretation of how fixed effects and geographic controls influence the results. Finally, the label option is appended to the esttab command, indicating that variable labels should be included in the table for clarity and precision. In the replication code provided by the authors, no specific instruction regarding the exportation of tables has been included. Therefore, we recommend users to add the command using \"NameOfYourTable.tex\" right after the last variable mentioned in esttab, just before the comma in front of se star, if they intend to export the table in LaTeX format. Alternatively, users can use the command using NameOfYourTable.txt\", at the same location in the code, if they prefer the table in a text format. This flexibility allows users to choose the desired output format for the tables based on their specific needs. This last step – which involves the formation of the table – can be used in the context of other studies to generate a clear and visually appealing table summarizing regression results. However, it is crucial to emphasize that this step is not indispensable for obtaining replication results. The results can be independently observed directly using the same code without the qui option, starting from reg or ivreg. This underscores the flexibility of the code, allowing researchers to directly access and analyze the regression outcomes without relying on the table generation step. While the table contributes to a more organized presentation, it is not a prerequisite for the replication process. Users can tailor their analysis based on their preferences and requirements before proceeding to this step. The study relies on an instrumental variable method that can be challenging to comprehend. Explaining the replication code of Table 1 is deemed essential for us, as it allows a detailed exploration and validation of the authors’ instrumental variable approach. This importance is further underscored by the fact that Table 1 showcases the primary results of the study. Having elucidated the intricacies of the Table 1 replication code, we will now transition to describing the replication code for Figure 5. 2.1.3.3 Understanding the replication process: code analysis of Figure 5 Figure 5 ***Figure 5*** eststo clear foreach x in 1682 1834 1850 1860 1873 1885 1895 { qui ivregress 2sls ln_Theil_`x&#39;c ln_area LnDistCPH Lat Long LnDistCoast i.region /// (ln_TotalFarmHK`x&#39; = MLmean) if year==`x&#39;, vce(clust ID) estimates store coef`x&#39; } coefplot coef*, vert yline(0) keep(ln_TotalFarmHK*) graphregion(color(white)) /// ciopts(recast(rcap) lcol(black)) mcolor(black) xtick(1(1)7) /// xlabel(1 &quot;1682&quot; 2 &quot;1834&quot; 3 &quot;1850&quot; 4 &quot;1860&quot; 5 &quot;1873&quot; 6 &quot;1885&quot; 7 &quot;1895&quot;, ) /// grid(b) legend(off) graph export &quot;outputfile.png&quot;, replace The provided Stata code segment serves to illustrate the coefficients for second-stage estimations conducted over the years 1682 to 1895. The dependent variable under consideration is ln(Theil), and the instrumental variable utilized is the share of boulder clay MLmean. The use of the natural logarithm for the dependent variable ln(Theil) in the second-stage regression is intentional. This choice allows for the presentation of estimates for the second-stage coefficients in levels, offering insight into the relationships over the years 1682 to 1895. The focus on ln(Theil) in different years underscores a preference for examining the absolute values of Theil index rather than changes in Theil index, providing a comprehensive perspective on the dynamics of the variable across the specified temporal range. Initially, eststo clear ensures a clean slate by clearing any previously stored estimation results. Subsequently, the foreach loop in the provided Stata code serves as an iterative mechanism, allowing the execution of specified commands for each value in the specified range or list. In this case, the loop iterates over the years 1682, 1834, 1850, 1860, 1873, 1885, and 1895. For each iteration, the code within the loop conducts a 2-stage least squares regression using the ivregress 2sls command, estimating a model for the given year. The model includes various independent variables such as lnTheil’x’c, lnarea, LnDistCPH, Lat, Long, LnDistCoast as well as the endogenous variable lnTotalFarmHK instrumented by MLmean and there are fixed effects for region. The purpose of the loop in this context is to efficiently run the same regression model for multiple years, automating the process and avoiding redundant code. This is particularly useful when dealing with time-series data or when conducting analyses for various time points. The resulting coefficient plot provides a concise and visual representation of the dynamics of the variable of interest across different years. The line estimates store coef’x’ facilitates the storage of estimation results in matrices named coef’x’, where x represents the specific year. Following the loop, the coefplot coef* command generates a coefficient plot based on the stored estimation results, specifically focusing on coefficients related to the variable lnTotalFarmHK across the years. In terms of visual representation, the plot includes a vertical line at 0 for reference with the vert yline(0) command, retains only coefficients related to lnTotalFarmHK with the keep(lnTotalFarmHK*) command, and employs a white background for enhanced clarity thanks to the graphregion(color(white)). Confidence intervals are displayed using a horizontal line in black with ciopts(recast(rcap) lcol(black)), and the markers – dots – representing coefficient estimates are colored black for visibility with the mcolor(black) option. Additionally, xtick(1(1)7) and xlabel(1 \"1682\" 2 \"1834\" 3 \"1850\" 4 \"1860\" 5 \"1873\" 6 \"1885\" 7 \"1895\"), allow stick marks and corresponding labels on the x-axis to be strategically positioned to represent each year from 1682 to 1895. Gridlines are incorporated for ease of interpretation grid(b), and the legend is turned off for a clean and uncluttered visual representation thanks to the legend(off) command. The final line of code, graph export \"outputfile.png\", replace, is added by us to the replication code for the purpose of exporting the coefficient plot as a PNG file. This command utilizes Stata’s graph export feature, allowing us to save the generated graph to an external file named “outputfile.png” in the current working directory. The option \"replace\" ensures that if a file with the same name already exists, it will be overwritten. This line of code enhances the replicability of the study by facilitating the export of the coefficient plot in a widely used PNG format for further analysis or inclusion in reports and presentations. This meticulous approach allows for a comprehensive exploration of coefficient dynamics over time, offering insights into the relationship between the dependent variable, ln(Theil), and the instrumental variable MLmean – share of boulder clay – with controls included, across the specified years. With the explanation of the coefficient plots for second-stage estimations complete, our attention now shifts to describing the replication code for Table A.3 found in the Appendix. 2.1.3.4 Understanding the replication process: code analysis of Table A.3 Table 3: Robustness check: Second-stage IV estimates using Gini coefficient ***Table A.3*** ***Creation of a loop*** eststo clear foreach x in 1682 1834 { eststo ginihk`x&#39;: ivregress 2sls gini`x&#39; ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==`x&#39; &amp; gini1834!=., vce(clust ID) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] } ***Formation of Table A.3*** esttab ginihk1682 ginihk1834, se star(* 0.10 ** 0.05 *** 0.01) b(3) r2 var(15) model(11) /// wrap keep(ln_TotalFarmHK) mtitles(&quot;2nd stage: Gini diff&quot; &quot;2nd stage: gini 1682&quot; /// &quot;2nd stage: gini 1834&quot;) stats(N r2 Fstat, labels(&quot;Observations&quot; &quot;R-squared&quot; /// &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) indicate(&quot;Region FE = 2.region&quot; /// &quot;Geography = LnDistCPH&quot; , labels(Y N)) label Table A.3 in the Appendix presents robustness check results, specifically second-stage instrumental variable estimates using the Gini coefficient. This additional analysis aims to verify the robustness of the findings by employing an alternative measure. The choice of the Gini coefficient not only enhances interpretability but also provides a basis for comparing and validating the study’s results against a broader scholarly context. Creation of a loop ***Table A.3*** ***Creation of a loop*** eststo clear foreach x in 1682 1834 { eststo ginihk`x&#39;: ivregress 2sls gini`x&#39; ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==`x&#39; &amp; gini1834!=., vce(clust ID) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] } Firstly, the use of the eststo clear command ensures a reset of previous estimation results, creating a clean environment before running new estimations. The loop designated by foreach x in 1682 1834 iterates over two specific years, namely 1682 and 1834. This looping mechanism, as explained in the preceding section – Section 3.3 –, provides a concise and efficient way to conduct repetitive tasks for multiple years. The eststo ginihk‘x’ command within the loop facilitates the storage of results in matrices named ginihk‘x’, with x representing the current year in each iteration. For a more comprehensive understanding of the loop creation and its purpose, referring to the preceding Section 3.3 is recommended. Moreover, the ivregress 2sls function is employed to conduct a two-stage instrumental variable (IV) regression. The model’s explanatory variables include D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast i.region. In specifying (ln_TotalFarmHK = MLmean), the endogenous variable – the natural logarithm of the total value of the land measured in barrel of hard grain of parish – is denoted as ln_TotalFarmHK and its instrument is identified as MLmean. To ensure that the analysis includes only observations for the specified year x where the variable gini1834 is not missing, the condition if year==x &amp; gini1834!=. is incorporated. Furthermore, the command vce(clust ID) is used to adjust standard errors in the regression model, accounting for within-cluster correlation. The code that we will now describe mirrors the structure found in Table 1. Following this, estat firststage is employed to display statistics from the first-stage of the IV regression. This step is crucial for assessing the validity of instruments. The subsequent lines involve the storage of first-stage results in a matrix named fstat using the mat fstat = r(singleresults) command. This matrix captures relevant statistics from the initial stage, providing insights into the instrumental variable performance. Finally, estadd scalar Fstat = fstat[1,4] introduces a new scalar variable Fstat into the main regression results. This step extracts the F-statistic from the first-stage matrix fstat and assigns it to Fstat. For a more comprehensive understanding of the F-statistics and its purpose, referring to the preceding Section 3.3.2 is recommended. Formatting table A.3, an additional but optional step ***Table A.3*** ***Formation of Table A.3*** esttab ginihk1682 ginihk1834, se star(* 0.10 ** 0.05 *** 0.01) b(3) r2 var(15) model(11) /// wrap keep(ln_TotalFarmHK) mtitles(&quot;2nd stage: Gini diff&quot; &quot;2nd stage: gini 1682&quot; /// &quot;2nd stage: gini 1834&quot;) stats(N r2 Fstat, labels(&quot;Observations&quot; &quot;R-squared&quot; /// &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) indicate(&quot;Region FE = 2.region&quot; /// &quot;Geography = LnDistCPH&quot; , labels(Y N)) label Moving forward in our analysis, we proceed to construct a comprehensive table that consolidates the outcomes of the earlier regression analyses. Importantly, it’s crucial to emphasize that this stage is not conducted in isolation, instead it builds upon the groundwork laid in the preceding step where variables were defined and regressions within the loop were executed. The resultant table serves as a visual representation, effectively summarizing the relationships captured in the regression models. This approach enhances the interpretability and communicative power of the analytical findings. The code employed in the creation of the comprehensive table A.3 aligns with the methodology elucidated in Section 3.2.3, specifically used for Table 1. Indeed, the Stata code provided encompasses the construction of a table using the esttab command, incorporating results from the two-stage instrumental variable regressions conducted for the years 1682 and 1834. Thus, the esttab ginihk1682 ginihk1834 line specifies the models whose results will be included in the table, representing these two specific years. To enhance the clarity and readability of the table, several formatting commands are employed. The se star(* 0.10 ** 0.05 *** 0.01) line introduces significance stars denoted by asterisks, indicating the levels of statistical significance. Additionally, b(3) limits the coefficient estimates to three decimal places. The inclusion of the coefficient of determination R-squared is facilitated by r2 command, with var(15) limiting the number of decimals for R-squared to 15 digits. The model(11) command specifies the maximum number of models displayed in the table, accommodating 11 different models. Moreover, the wrap command assists in managing long variable names, allowing them to span multiple lines for improved readability. The keep(lnTotalFarmHK) line specifies the variable lnTotalFarmHK to be included in the table. Furthermore, model titles are assigned using mtitles(), and additional statistics, such as the number of observations (N), R-squared, and the F-statistic, are incorporated with stats(N r2 Fstat, labels(\"Observations\" \"R-squared\" \"KP F-statistic\")fmt(%9.0fc 2 2)). The indicate(\"Region FE = 2.region\" \"Geography = LnDistCPH\", labels(Y N)) section introduces indicator variables in a regression model. The first part, \"Region FE = 2.region\", includes fixed effects for a specific region, here denoted as 2, representing Jutland. This accounts for unobserved variation specific to Jutland. The second part, \"Geography = LnDistCPH\" introduces an indicator variable related to geography. The labels Y and N are assigned to indicate the presence or absence of fixed effects or controls in each observation, contributing to the model’s interpretability. Finally, the label option appended to the end of the esttab command ensures that variable labels are included in the table, enhancing the interpretability of the presented results. Once again, in the provided replication code by the authors, no specific instruction regarding the exportation of tables has been included. Therefore, we recommend users to add the command using \"NameOfYourTable.tex\" right after the last variable mentioned in esttab, just before the comma in front of se star, if they intend to export the table in LaTeX format. Alternatively, users can use the command using NameOfYourTable.txt\", at the same location in the code, if they prefer the table in a text format. This flexibility allows users to choose the desired output format for the tables based on their specific needs. This last step to form a table can be employed in the context of other studies to generate a clear and visually appealing table summarizing regression results. However, it is crucial to emphasize that this step is not indispensable for obtaining replication results. 2.1.3.5 Understanding the replication process: code analysis of Table A.4 Table 4: IV estimation using barley payments and the share of parish area classified as boulder clay ***Table A.4*** eststo clear ***Column (1) : OLS*** eststo ols1: qui reg D.Theil_c BygLG ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834, vce(robust) eststo ols2: qui reg D.AggTheil_c BygLG ln_area LnDistCPH Lat Long LnDistCoast i.region /// i.year if year==1834, vce(robust) ***Column (2) : First-stage*** eststo fiv1: qui reg BygLG MLmean ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834 &amp; ln_TotalFarmHK!=., vce(robust) ***Column (3) : Second-stage (diffTheil)*** eststo ivdiff1: qui ivregress 2sls D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (BygLG = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (4) : Second-stage (diffAggTheil)*** eststo ivdiff2: qui ivregress 2sls D.AggTheil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (BygLG = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Formation of Table A.4*** esttab ols1 fiv1 ivdiff1 ivdiff2, se star(* 0.10 ** 0.05 *** 0.01) b(3) r2 var(15) /// model(11) wrap keep(MLmean BygLG) mtitles(&quot;OLS&quot; &quot;first stage&quot; /// &quot;second stage (diffTheil)&quot; &quot;second stage (diffAggTheil)&quot; ) stats(N r2 Fstat, /// labels(&quot;Observations&quot; &quot;R-squared&quot; &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) /// indicate(&quot;Region FE = 2.region&quot; &quot;Geography = LnDistCPH&quot; , labels(Y N)) label Transitioning to another robustness test, presented in Table A.4 in the Appendix, we explore an alternative measure of land quality. In this test, rather than considering the total value of the parish’s HK, the authors focus solely on the amount of barley paid in tax as an indicator of land quality. This measure is derived from the digitization of a map presented by Frandsen in 1988. The results of this robustness check closely resemble the main estimations of table 1, underscoring the consistency and reliability of the analytical findings. OLS regressions by replicating the columns 1 and 2 of Table A.4 ***Table A.4*** eststo clear ***Column (1) : OLS*** eststo ols1: qui reg D.Theil_c BygLG ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834, vce(robust) eststo ols2: qui reg D.AggTheil_c BygLG ln_area LnDistCPH Lat Long LnDistCoast i.region /// i.year if year==1834, vce(robust) ***Column (2) : First-stage*** eststo fiv1: qui reg BygLG MLmean ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834 &amp; ln_TotalFarmHK!=., vce(robust) In this analysis, we employ familiar Stata commands, akin to those utilized in Table 1, with the only variable differing across each column specification is the initial variable, corresponding to the dependent variable: the change in the Theil index from 1682 to 1834 D.Theilc, the change in the Theil index of 1834 aggregated to the size categories of 1682 D.AggTheilc, and the amount of barley paid in tax as an indicator of land quality BygLG, respectively. In line with our approach of conducting the first-stage analysis, we introduce the instrumental variable MLmean to the last regression equation. First, the command eststo ols1 is used to store the estimation results, and the results matrix is designated as ols1. The qui function, signifying quietly, enables the temporary storage of results in the matrix ols1 for the initial regression. Importantly, this function ensures that the results of the regression are not displayed at this point but are preserved under the name ols1 for subsequent utilization. This approach proves particularly beneficial when authors later aim to present a comprehensive table with multiple specifications. Moreover, the reg function is utilized for Ordinary Least Squares (OLS) estimation, running a regression with specified variables. In this instance, the regression equation includes Theilc BygLG lnarea LnDistCPH Lat Long LnDistCoast i.region. A conditional statement, if year==1834 &amp; lnTotalFarmHK!=., restricts the analysis to observations where the year variable is equal to 1834, and lnTotalFarmHK is not missing. This condition ensures a focused examination of data relevant to the specified year and variable condition. Furthermore, the use of the conditional if year==1834 filters the data, retaining only observations where the year variable is equal to 1834. Lastly, the vce(robust) command is incorporated to specify the use of robust standard errors. This adjustment is made to account for potential heteroskedasticity and correlation of errors, enhancing the reliability of the estimation results. IV regressions by replicating the columns 3 and 4 of the Table A.4 ***Table A.4*** ***Column (3) : Second-stage (diffTheil)*** eststo ivdiff1: qui ivregress 2sls D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (BygLG = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (4) : Second-stage (diffAggTheil)*** eststo ivdiff2: qui ivregress 2sls D.AggTheil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (BygLG = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] In this specific analysis, we focus on the second-stage of the estimations with two dependent variables: D.Theilc, representing the change in the Theil index from 1682 to 1834, D.AggTheilc, representing the change in the Theil index of 1834 aggregated to the size categories of 1682. The endogenous variable, the amount of barley paid in tax as an indicator of land quality BygLG, is instrumented by the share of boulder clay, utilizing MLmean as the instrumental variable. This nuanced approach allows us to enhance the robustness of the estimates and address potential biases in the endogenous variable. In executing the second-stage of our analysis, employing the eststo ivdiff1: qui command allows to store the estimation results in a matrix named ivdiff1. This matrix captures the outcomes of the two-stage instrumental variable regression facilitated by the ivregress 2sls command. The regression equation includes explanatory variables D.Theilc lnarea LnDistCPH Lat Long LnDistCoast i.region, mirroring the structure of OLS regressions. Moreover, the specification (BygLG = MLmean) designates BygLG as the endogenous variable, with MLmean serving as its instrumental variable. An instrumentalization that is crucial to address potential endogeneity concerns and fortifying the validity of the estimations. The use of the conditional if year==1834 filters the data, retaining only observations where the year variable is equal to 1834. To account for heteroskedasticity and autocorrelation, the vce(robust) command employs a robust variance-covariance matrix. Subsequently, we encounter the lines of code that initially appeared daunting, but now, through regular utilization, their functionality has become comprehensible. Indeed, the estat firststage command is utilized to display statistics from the first-stage of the IV regression, offering insights into the validity of the instrumental variables. Further, the mat fstat = r(singleresults) line stores the first-stage results in a matrix named fstat, facilitating a comprehensive view of the instrumental variable performance. Lastly, the estadd scalar Fstat = fstat[1,4] command introduces a new scalar variable Fstat into the main regression results. This step extracts the F-statistic from the first-stage matrix fstat and assigns it to Once again, for a more comprehensive understanding of the F-statistics and its purpose, referring to the preceding Section 3.2.2 is recommended. We would like to remind you that the results can be independently observed directly using the same code without the qui option, starting from reg or ivreg. This underscores the flexibility of the code, allowing researchers to directly access and analyze the regression outcomes without relying on the table generation step. Formatting Table A.4, an additional but optional step ***Table A.4*** ***Formation of Table A.4*** esttab ols1 fiv1 ivdiff1 ivdiff2, se star(* 0.10 ** 0.05 *** 0.01) b(3) r2 var(15) /// model(11) wrap keep(MLmean BygLG) mtitles(&quot;OLS&quot; &quot;first stage&quot; /// &quot;second stage (diffTheil)&quot; &quot;second stage (diffAggTheil)&quot; ) stats(N r2 Fstat, /// labels(&quot;Observations&quot; &quot;R-squared&quot; &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) /// indicate(&quot;Region FE = 2.region&quot; &quot;Geography = LnDistCPH&quot; , labels(Y N)) label In the next step of our analysis, we construct a comprehensive table to concisely present the findings from our prior regression analyses. It’s important to emphasize that this stage isn’t isolated but rather complements the groundwork laid in the preceding steps, where variables were defined and regressions were conducted. The resultant table serves as a visual tool to depict the relationships identified in our regression models, thereby enhancing the clarity and communicative impact of our analytical insights. In this stage, we use the esttab command to generate a comprehensive table summarizing the results from various models, including OLS, first-stage IV, and second-stage IV regressions. The specified models, denoted as ols1 fiv1 ivdiff1 ivdiff2, capture different aspects of the regressions. The se star(* 0.10 ** 0.05 *** 0.01) option is employed to display standard errors with significance stars, where asterisks indicate different levels of significance with * for 0.10, ** for 0.05, and *** for 0.01. Setting b(3) limits, once again, the number of decimal places for coefficient estimates to 3, contributing to a cleaner presentation. Including the R-squared r2 and limiting the number of decimals for the R-squared to 15 digits var(15) provide additional insights into the explanatory power of the models. Furthermore, the model(11) option ensures that a maximum of 11 different models is displayed in the table. To accommodate long variable names, the wrap command is utilized, allowing for a more organized presentation. Moreover, the keep(MLmean BygLG) option specifies the variables to be included in the table, focusing on MLmean and BygLG. Model titles are designated using the mtitles() option for clarity and organization. Additional statistics, such as the number of observations (N), R-squared, and the F-statistic, are included using the stats(N r2 Fstat, labels(\"Observations\" \"R-squared\" \"KP F-statistic\")fmt(%9.0fc 2 2)) option, providing a more comprehensive overview. Moreover, indicator variables for fixed effects related to a specific region – here 2 representing Jutland – 2.region and geography LnDistCPH are incorporated using the indicate() option. The labels(Y N)) allows to specify these fixed effects as Y if the fixed effects are included or as N if they are not This offers further control in understanding the impact of these factors on the results. Finally, the label option ensures that variable labels are included in the table, enhancing the interpretability of the presented information. Once again, in the provided replication code by the authors, no specific instruction regarding the exportation of tables has been included. Therefore, we recommend users to add the command using \"NameOfYourTable.tex\" right after the last variable mentioned in esttab, just before the comma in front of se star, if they intend to export the table in LaTeX format. Alternatively, users can use the command using NameOfYourTable.txt\", at the same location in the code, if they prefer the table in a text format. This flexibility allows users to choose the desired output format for the tables based on their specific needs. This last step to form a table can be employed in the context of other studies to generate a clear and visually appealing table summarizing regression results. However, it is crucial to emphasize that this step is not indispensable for obtaining replication results. And although we conducted a multitude of regressions in Section 3.5.1, it is noteworthy that the final Table A.4 does not include the ols2 regression. Finally, the results can be independently used directly with the same code without the qui option, starting from reg or ivreg. This highlights the flexibility of the code, allowing researchers to directly access and analyze the regression outcomes without relying on the table generation step. While the table contributes to a more organized presentation, it is not a prerequisite for the replication process, providing users with the option to tailor their analysis based on specific preferences and requirements. Our thorough replication process aimed to transparently convey each step of the analysis, ensuring clarity and reproducibility. We hope that our detailed descriptions provide a comprehensive understanding of the code and methodology employed. Moreover, we hope that this clarity enhances the accessibility of the article and facilitates further examination and validation by other researchers. We trust that transparency is crucial for fostering rigorous and collaborative research practices. Authors: Auvray Adrien, Carette Anne-Laure, Tarlapan Alina, students in the Master program in Development Economics and Sustainable Development (2023-2024), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2023 "],["can-you-move-to-opportunity-evidence-from-the-great-migration.html", "2.2 Can you move to opportunity? Evidence from the great migration", " 2.2 Can you move to opportunity? Evidence from the great migration Derenoncourt, Ellora. Can you move to opportunity? Evidence from the Great Migration. American Economic Review 112.2 (2022): 369-408. https://www.aeaweb.org/articles?id=10.1257/aer.20200002 Download the do-file corresponding to the explanations from here Download the Codebook Highlights This paper examines the impact of racial composition shocks during the Great Migration (1940-1970) on the upward mobility of Black families in northern U.S. cities, exploring the mechanisms through which these demographic changes influenced the long-term socio-economic outcomes. The research uses a shift-share IV strategy, leveraging historical migration patterns and southern economic shocks. In this context, it effectively isolates exogenous variation in the Black population influx to northern cities, addressing endogeneity concerns and identifying causality, validated through pre-trend checks and robust instrument strength. This methodology is standard as it builds off the identification strategy developed by Boustan (2010) and has been used in subsequent papers on the Great Migration, such as Tabellini (2018) and Fouka, Mazumder, &amp; Tabellini (2022), making it a recognized approach in the field of migration studies. Firstly, the document begins by presenting a concise summary of the paper, ensuring readers have a solid understanding of its key themes and objectives. Then, a step-by-step replication of the paper’s important elements is provided, offering insights into the methodology and the validity of its findings. The final section highlights the key STATA techniques broken down in this document, including dynamic file management, string matching, macros, nested loops, IV regressions, and seamless LaTeX export, all contributing to enhancing the accessibility of the article and its code. 2.2.1 Introduction and historical background The Great Migration, a pivotal demographic shift in U.S. history, involved millions of Black Americans moving from the South to the North of the country between 1910 and 1970. This exodus, driven by the quest for freedom and better opportunities, occurred in two waves: the first from 1915 to 1930 and the second from 1940 to 1970, which is the period this paper focuses on. The second wave was largely fueled by an increased labor demand for war-production and defense industries during WWII, as well as in the automotive sector in subsequent decades. At the same time, important changes in the southern economy, including mechanization in agriculture combined with the ongoing political, social and economic repression of the black families under Jim Crow laws, further drove this out-migration from the south. For decades, the North seemed to fulfill these aspirations, offering greater access to education and better living conditions. However, the Migration’s legacy has seemed to be complex. By the 2000s, many northern regions that once fostered upward mobility for Black families now exhibit some of the poorest outcomes. This historical shift in the geography of opportunity highlights the nuanced impacts of the Great Migration, making it a critical subject for understanding racial inequality and economic mobility in the United States. 2.2.2 Data on upward mobility and city demographics The author utilized historical U.S. census data from the IPUMS 1940 Complete Count Census (“CC”), which includes detailed information on educational outcomes for teenagers and their parents within the same household, as well as data on location, race, and other demographics. This was combined with a dataset on localized upward mobility developed by Chetty et al. (2018) and Chetty and Hendren (2018b). The latter dataset, derived from U.S. federal tax records, provides information on parental income and the adult income of their children, with parents and children linked through dependent claims on tax forms. These data sources offer measures of upward mobility for the 1980s birth cohorts—the most recent cohorts for which adult outcomes can be reliably assessed. Additionally, the datasets were internally linked to census data to incorporate racial information, which is not captured in tax records. As mentioned earlier, the study focuses on understanding upward mobility, defined broadly as the adult outcomes of children relative to their parents’ economic status. Prior to the 1950s, the analysis centers on educational upward mobility, using measures such as the proportion of teenagers in a commuting zone with nine or more years of schooling, whose parents had between five and eight years of education. This approach leverages prior research to examine geographic and racial disparities in mobility during this period. For earlier periods, when detailed educational data were unavailable, school attendance among teenagers from low socioeconomic families is used as a proxy. In contrast, upward mobility in the 2000s is assessed using income data, specifically the average income rank of individuals based on their parents’ income rank within their birth cohort. These rankings are calculated nationally, offering a broader perspective on economic mobility. Although educational upward mobility (historical periods) and income upward mobility (contemporary periods) are distinct measures that had been used due to data constraints, the study demonstrates a strong correlation between the two. This suggests that the geography of educational mobility in the 1940s provides meaningful insights into patterns of income mobility today. In addition, the study concentrates on 130 urban commuting zones in the non-southern United States (referred to in the paper as “North” for simplicity), selected based on demographic changes during the Great Migration and net Black migration into the regions. These zones include cities with populations of 25,000 or more and states in the Northeast, Midwest, West, and a few southern locations like Maryland, Delaware, and Washington, D.C. The sample covers a significant portion of the population, including 85% of the non-southern U.S. population and 97% of its Black population, ensuring comprehensive representation for the analysis. 2.2.3 Initialization and organization of the directories All the data used for the creation of the final dataset are publicly accessible. For utilizing the ICPSR and IPUMS USA data, users should sign-up to their website, download the data from the DOI and unzip and store them and then follow the instruction of the related do files in the original replication package to create the final dataset. Then, we install the essential packages (ssc install) and increase the maximum number of variables (set maxvar) that Stata can handle since our final dataset has more than the usual default number of variables Stata accepts. After compiling the final dataset, to store our directories and main variables that will be used throughout the do-file, we use macros. Macros in Stata are placeholders that store text, numbers, or code to avoid repetition. Local macros store temporary information that can’t be accessed again outside of that specific block of code, while global macros can be used over again throughout the entirety of the do-file. Here, we create global macros for our directories, as well as OLS, IV, and baseline controls variables to avoid writing them everytime. This allows us to be efficient. * Clear all and set max matsize capture log close clear all set more off set maxvar 9000 *installing necessary packages *estout stores estimates and allows us to make regression tables ssc install estout, replace * binscatter allows us to visualize grouped data trends ssc install binscatter, replace *ivreg2 allows us to run instrumental variables regressions ssc install ivreg2, replace * Setting up directories global XX &quot;YOUR DIRECTORY GOES HERE&quot; global data &quot;$XX/directory of the data folder&quot; global logs &quot;$XX/directory of the logs folder&quot; global figtab &quot;$XX/directory of the figures and tables folder&quot; global x_ols GM global x_iv GM_hat2 global baseline_controls frac_all_upm1940 mfg_lfshare1940 v2_blackmig3539_share1940 reg2 reg3 reg4 Then, we define a program (a user-defined set of instructions and commands) named Printest to automate repetative tasks (e.g, exporting coefficients and results) and ensure consistency when saving outputs and make it easier to import results into LaTeX or Word for documentation. This customized command does 4 things: 1. takes the estimate (e.g., median or mean values) 2. formats it with specific text (e.g., units like “percentage points”). 3. rounds the estimate to a specified number of decimal places. 4. writes the formatted result to a text file in the designated directory for documentation. Even when we’re producing a figure, using this command, we get to store the estimates in a txt file under a standardized format. Let’s go through the functions of this command: args specifies the inputs (arguments) of the program. cap (i.e. capture) prevents Stata from stopping or displaying an error if the next line of code fails. Here, it ensures that if the program PrintEst already exists, it is quietly deleted so we can redefine it. mkdir (make directory) creates a directory even if it doesn’t exist, tempname creates a temporary reference to the name of the file, file write adds the text that you want to write to the file, and file close closes the file. The program starts by specifying the argument, displays the desired estimate, displays the name of the estimate, creates a temporary file name, creates the directory where that file will be saved, opens the file, formats the estimate to a specified number of decimals, and writes the desired output before closing the file. The format of our output using this program can be seen in the first line: args est name txtb4 txtaftr decim. est is the estimate, name is the name of the output file, txtb4 is any text we want to insert before the estimate, txtaftr is any text or notes we want to insert after the estimate, and decim is the number of decimals we want to see after the comma. For example, in Figure 2 that we will see below, we can read: local slc_pctbpopchng4070 = `r(mean)' PrintEst `slc_pctbpopchng4070' \"slc_pctbpopchng4070\" \"\" \"th percentile%\" \"2.0\" First line computes the mean of the variable local slc_pctbpopchng4070, and the second uses PrintEst to save that mean to a file named slc_pctbpopchng4070.txt and formats the output as [value]th percentile% at 2 decimals after the comma. If the mean is, for instance, 45, we will read in the text file: 45.00th percentile%. This provides a standardized way to document estimates throughout this do-file. capture program drop PrintEst program PrintEst args est name txtb4 txtaftr decim di `est&#39; di &quot;`name&#39;&quot; tempname sample cap mkdir $figtab/text file open `sample&#39; using &quot;$figtab/text/`name&#39;.txt&quot;, text write replace *local est_rounded = round(`est&#39;, 0`decim&#39;) local est_rounded : di %`decim&#39;f `est&#39; file write `sample&#39; `&quot;`txtb4&#39;`est_rounded&#39;`txtaftr&#39;&quot;&#39; file close `sample&#39; end 2.2.4 Descriptive Statistics 2.2.4.1 Figure 2: Quantiles of urban black share increases, 1940-1970 Running descriptive statistics is essential. Good descriptive statistics further highlights the main points the paper attempts to drive. As such, we recreate Figure 2, which provides a comprehensive summary of urban Black population increases in Northern US cities during the Great Migration (bpopchange1940_1970) . The figure allows for both city-specific insights and a broader view of population trends across all commuting zones. The statistical calculations for key cities help contextualize the findings, making this graphic essential for understanding regional variations in population changes and migration patterns during the great migration in the United States. Before breaking down the code, let’s start with understanding some repeated commands in this section and throughout the paper: Locals save estimates. When we read local GM_`y' = _b[$x_ols], the line is saving the OLS ($x_ols) coefficient (_b) of GM when the outcome variable is `y’. Similarly, when we read qui sum GM, local ols_SD = ``r(sd)' , the lines quietly summarize GM and extract its standard deviation saved into a local named ols_SD. regexm helps us find whether an expression is present in a string variable (a variable that contains text). If that variable contains the expression we’re looking for, regexm will return 1 (true) if it is found, and 0 (false) if it is not. For example, say we have a string variable address with the following entries 123 Detroit St., 456 San Francisco Ave., and 101 Washington Blvd.. the code gen has_detroit = regexm(address, \"Detroit”) will create a new dummy variable called has_detroit that will be equal to 1 if Detroit is present in the addresses or 0 if not. In this case we’ll have 1, 0, 0. This condition | means either. If we have gen x = 1 if x1 = 2 | x2 = 3, this means that the new variable x = 1 if x1 = 2 or if x2 = 3. In both cases, we record x as 1. The xtile command in Stata is used to create a new variable by dividing an existing variable into quantiles. It gives us the option to choose how many quantiles we’d like to split the variable. For instance, xtile pctX = X, nq(4) generates a new variable, pctX, that assigns each observation a percentile rank (from 1 to 4) based on the values X. The first step loads the dataset, which contains the data for urban Black population increases from 1940 to 1970 across commuting zones (CZs). The clear option ensures any previously loaded data is removed before loading the new dataset. Next, a new variable mylabel is created to label specific cities of interest. First, the regexm function identifies cities such as Steubenville, Milwaukee, Washington, Gary, and Detroit. my label will return cz_name + an empty string of custom labels that is then specified in the following lines of code. For instance, ,if cz_name contains any of the specified cities, the entry of mylabel for that row will be cz_name +. Then, we apply replace mylabel by the label we actually want for that city if regexm(cz_name, “X”) i.e. if regexm found the city’s name in the cz_name string variable. This allows for highlighting key cities that are discussed in the paper. use ${data}/GM_cz_final_dataset.dta, clear gen mylabel=cz_name+&quot; &quot; if regexm(cz_name, &quot;Steubenville&quot;) | regexm(cz_name, &quot;Milwaukee&quot;) /// | regexm(cz_name, &quot;Washington&quot;) | regexm(cz_name, &quot;Gary&quot;) | regexm(cz_name, &quot;Detroit&quot;) replace mylabel=&quot;Washington, D.C.&quot; if regexm(cz_name, &quot;Washington&quot;) replace mylabel=&quot;Detroit, MI&quot; if regexm(cz_name, &quot;Detroit&quot;) replace mylabel=&quot;Gary, IN&quot; if regexm(cz_name, &quot;Gary&quot;) replace mylabel=&quot;Steubenville, OH&quot; if regexm(cz_name, &quot;Steubenville&quot;) replace mylabel=&quot;Milwaukee, WI&quot; if regexm(cz_name, &quot;Milwaukee&quot;) In this part of the code, the two-part scatter plot is generated using twoway graph which allows multiple types of plots in one figure. In this case, it’s used for two combined scatterplots. The first scatter plot visualizes the relationship between the urban Black population change (bpopchange1940_1970) on the Y-axis and our main independent variable, the percentile of urban Black population increase (GM) on the X-axis, highlighting cities of interest with labels (if mylabel =! “”, where ! means not equal and “” means empty, so when mylabel is not missing, which is only true for the cities we specified earlier). As for formatting options, graph formatting options in Stata allow you to customize the appearance of graphs by adjusting elements such as marker size, symbol type, label placement, and color. The points for these cities are marked with large (msize(ylarge), orange (jmporange), hollow circles msymbol(circle_hollow) and labeled accordingly mlabel(mylabel), making the cities easily identifiable. The color of the labels is black mlabcolor(black) and positioned above to the right mlabposition(11). Legends usually are added automatically when generating a graph. Here, we remove legends using legend(off). Finally, both the background and plotting area colors are set at white graphregion(color(white)) &amp; plotregion(ilcolor(white)). For further guidance on visual formatting in STATA, refer to A Visual Guide to STATA Graphics by Michael M. Mitchell. The second scatter plot displays the same data but includes all cities. As such, it offers a view of the population changes across the entire sample, drawn in green instead of orange. The plot is customized with titles on both axes, describing the population increase on the y-axis and the percentile of increase on the x-axis. We add a graph title using ytitle. The motivation behind this dual approach is to present both detailed, city-specific information and a general overview of the data. graph twoway (scatter bpopchange1940_1970 GM if mylabel!=&quot;&quot;, legend(off) mcolor(orange) msymbol(circle_hollow) /// msize(vlarge) mlabel(mylabel) mlabcolor(black) mlabposition(11) graphregion(color(white)) plotregion(ilcolor(white)) /// ylabel(,nogrid)) || (scatter bpopchange1940_1970 GM, xtitle(&quot;Percentile of urban Black pop increase 40-70&quot;) /// ytitle(&quot;Incr. in urban Black pop &#39;40-70 as ppt of 1940 urban pop&quot;) legend(off) mcolor(green) graphregion(color(white)) plotregion(ilcolor(white))) After generating the graph, the output is saved as a PNG file in the specified directory ($figtab). Cd “$figtable” specifies the directory where the table will be saved. The code then calculates summary statistics for the bpopchange1940_1970 variable, such as the median and mean. We also generate pctbpopchange1940_1970, which is a percentile rank of the existing variable bpopchange1940_1970 divided into 100 quartiles. For each selected city (e.g., Pittsburgh, Detroit, Salt Lake City, Washington, DC), the summ command computes the mean for both the raw population change (bpopchange1940_1970) and its percentile distribution (pctbpopchange1940_1970). These statistics are stored in local variables and printed using the PrintEst command for clarity. cd &quot;$figtab&quot; graph export bpopchange_percentiles.png, replace * Point estimates cited in text: * median summ bpopchange1940_1970, d local p50_bpopchng4070 = `r(p50)&#39; PrintEst `p50_bpopchng4070&#39; &quot;p50_bpopchng4070&quot; &quot;&quot; &quot; percentage points%&quot; &quot;3.1&quot; xtile pctbpopchange1940_1970 = bpopchange1940_1970, nq(100) *Pittsburgh summ bpopchange1940_1970 if cz_name==&quot;Pittsburgh, PA&quot; local pitt_bpopchng4070 = `r(mean)&#39; PrintEst `pitt_bpopchng4070&#39; &quot;pitt_bpopchng4070&quot; &quot;&quot; &quot; percentage points%&quot; &quot;3.1&quot; summ pctbpopchange1940_1970 if cz_name==&quot;Pittsburgh, PA&quot; local pitt_pctbpopchng4070 = `r(mean)&#39; PrintEst `pitt_pctbpopchng4070&#39; &quot;pitt_pctbpopchng4070&quot; &quot;&quot; &quot;rd percentile%&quot; &quot;2.0&quot; *Detroit summ bpopchange1940_1970 if cz_name==&quot;Detroit, MI&quot; local detr_bpopchng4070 = `r(mean)&#39; PrintEst `detr_bpopchng4070&#39; &quot;detr_bpopchng4070&quot; &quot;&quot; &quot; percentage points%&quot; &quot;3.1&quot; summ pctbpopchange1940_1970 if cz_name==&quot;Detroit, MI&quot; local detr_pctbpopchng4070 = `r(mean)&#39; PrintEst `detr_pctbpopchng4070&#39; &quot;detr_pctbpopchng4070&quot; &quot;&quot; &quot;th percentile%&quot; &quot;2.0&quot; *Salt Lake City summ bpopchange1940_1970 if cz_name==&quot;Salt Lake City, UT&quot; local slc_bpopchng4070 = `r(mean)&#39; PrintEst `slc_bpopchng4070&#39; &quot;slc_bpopchng4070&quot; &quot;&quot; &quot; percentage points%&quot; &quot;3.1&quot; summ pctbpopchange1940_1970 if cz_name==&quot;Salt Lake City, UT&quot; local slc_pctbpopchng4070 = `r(mean)&#39; PrintEst `slc_pctbpopchng4070&#39; &quot;slc_pctbpopchng4070&quot; &quot;&quot; &quot;th percentile%&quot; &quot;2.0&quot; *Washington, DC summ bpopchange1940_1970 if cz_name==&quot;Washington DC, DC&quot; local wadc_bpopchng4070 = `r(mean)&#39; PrintEst `wadc_bpopchng4070&#39; &quot;wadc_bpopchng4070&quot; &quot;&quot; &quot; percentage points%&quot; &quot;3.1&quot; summ pctbpopchange1940_1970 if cz_name==&quot;Washington DC, DC&quot; local wadc_pctbpopchng4070 = `r(mean)&#39; PrintEst `wadc_pctbpopchng4070&#39; &quot;wadc_pctbpopchng4070&quot; &quot;&quot; &quot;th percentile%&quot; &quot;2.0&quot; 2.2.5 Identification strategy Causal Framework &amp; Exclusion Restriction In this paper, the author faces endogeneity concerns stemming from two issues: 1. Reverse causality because upward mobility in northern cities may have influenced the magnitude and direction of Black migration during the Great Migration, as migrants were likely to select destinations with better economic opportunities, and/or 2. Omitted variable bias because of unobserved factors, such as pre-existing local economic conditions, racial dynamics, or industrialization levels, that could simultaneously influence both the extent of Black migration and future upward mobility. To address this, the author employs a shift-share instrumental variable (IV) strategy, which leverages exogenous variation in migration “push factors” from southern counties combined with historical settlement patterns in northern cities. Indeed, Black southern migrants tended to move where previous migrants from their communities had settled, thus generating correlated origin-destination flows similar to those observed in the international migration context. Shocks to migrants’ origin locations (“push factors”) are plausibly orthogonal to shocks to the destinations (“pull factors”) that could also influence the location choices of future migrants. Interacting exogenous shifts in migration at the origin level with historical migration patterns in the destinations yields a potential instrument for Black population changes in the North. The IV is constructed as the following: 1. Share: The historical distribution of southern Black migrants across northern cities in 1940, i.e. original migration patterns. 2. Shift: Exogenous “push factors” that caused migration from southern counties, such as declines in cotton production, economic shocks in agriculture, and World War II-related defense spending in southern counties. The predicted Black population increase in a northern CZ is the shift-share instrument used in this paper. It is constructed as: \\[ \\widehat{\\Delta b}_{\\text{urban,CZ,1940–1970}} = \\sum_{j \\in S} \\sum_{c \\in CZ} \\omega_{jc}^{1935–1940} \\cdot \\widehat{m}_j^{1940–1970} \\] Where: \\(\\omega_{jc}^{1935–1940}\\) : The share of pre-1940 Black migrants from southern county \\(j\\) who settled in city \\(c\\) by 1940. This captures pre-existing linkages between southern counties and northern cities. \\(\\widehat{m}_j^{1940–1970}\\) : The predicted migration from southern county \\(j\\) between 1940 and 1970. After computing predicted increases in the urban Black population in northern CZs using this method, the author uses the percentile of predicted increases, , to instrument for the percentile of observed increases in the Black population, GMCZ. 2.2.6 Model Equations: 1. First Stage Equation The first stage equation relates the predicted percentile increase in the Black population (\\(\\widehat{GMCZ}\\)) to the observed percentile increase in the Black population (\\(GMCZ\\)): \\[ GMCZ = \\gamma + \\delta \\widehat{GMCZ} + X_{CZ}&#39; \\mu + \\epsilon_{CZ} \\] Where: \\(GMCZ\\): The observed percentile increase in the Black population in the CZ (1940–1970). \\(\\widehat{GMCZ}\\): The predicted percentile increase in the Black population based on the instrument. \\(X_{CZ}\\): Vector of control variables (defined in the codebook under Baseline controls) \\(\\epsilon_{CZ}\\): The error term. 2. Second Stage (2SLS) Equation The second stage equation estimates the causal effect of the observed Black population increase (\\(GMCZ\\)) on upward mobility: \\[ \\overline{y}_{p, CZ} = \\alpha + \\beta GMCZ + X_{CZ}&#39; \\Gamma + \\epsilon&#39;_{CZ} \\] Where: \\(y_{p, CZ}\\): The average income rank of children with parents at rank \\(p\\) in the CZ. \\(\\beta\\): The causal effect of the Great Migration (\\(GMCZ\\)) on upward mobility (\\(y_{p, CZ}\\)). \\(X_{CZ}\\): Vector of control variables. \\(\\epsilon&#39;_{CZ}\\): The error term. 2.2.7 Results on upward mobility To test the impact of the Great Migration on the upward mobility of Black and White families, the author regresses key outcomes (detailed in the codebook) on the percentile of observed Black population increase. In this section, we’re replicating two tables for the main results: the great migration impact on white families, and the great migration impact on black families. Each table includes the results of the first-stage regression, the ordinary least squares regression, the reduced form regression, and the two stage least squares regressions. Each regression is run on our set of six outcome variables, for both black and white families. Given the large amount of work, we use a loop for efficiency. A loop is an interactive mechanism that allows the execution of specified commands for each value in the specified list. Here, we loop multiple times. Our first essential loop differentiates between white and black families. In the line (foreach r in “black” “white””), our “list” is “black” and “white” and “foreach” black and white, the loop will execute the code inside it. Inside the code, we loop over outcomes to run each regression with a different y, defined inside the first main loop. Indeed, each loop for each model (RF, 2SLS, OLS, and First-Stage) starts with (foreach y in `outcomes'{) before executing the actual regression on different Ys. Before explaining the specificities of each model, let’s go through potentially tricky commands used in all of them to make it easier to read. qui stands for quietly, where we execute a command without seeing its result in the stata window. test $x_iv = 0 tests for whether the IV is significantly correlated with X. Estadd scalar: adds a row to the final table that includes values (or scalars) we choose to save. When we read a line like sum `y', estadd scalar basemean=r(mean), estadd scalar sd=r(sd) , estadd scalar gm_sd=`ols_SD', we know that we are summarizing the outcome variable, and adding its mean, SD, and the ols_SD we saved earlier as an additional row down the table we’re compiling. Finally, eststo saves estimates. eststo clear makes sure the memory is empty of previously saved estimates. esttab compiles the table, with options for us to polish it. Note: we will be splitting the loop for this section into multiple parts to explain its specifities. In the do-file, it can be run all together. The first section creates a loop to capitalize the labels black and white. Then, we define local outcomes which we will loop over. * Replicating Tables 5 and 6 use &quot;${XX}/GM_cz_final_dataset.dta&quot;, clear * Start the loop for racial groups foreach r in &quot;black&quot; &quot;white&quot; { local race &quot;Black&quot; if &quot;`r&#39;&quot; == &quot;black&quot; { local race &quot;White&quot; } * Define outcomes for the current racial group local outcomes kfr_`r&#39;_pooled_p252015 kir_`r&#39;_female_p252015 kir_`r&#39;_male_p252015 kfr_`r&#39;_pooled_p752015 kir_`r&#39;_female_p752015 kir_`r&#39;_male_p752015 2.2.7.1 First-stage regression The loop clears any estimates in memory, summarizes GM and extracts its SD into the local ols_SD, regresses the instrument on the main independent variable, tests its significance and saves the F-stat which is necessary to check whether it matches the relevance condition for a good instrument (F-stat &gt; 10). Although we see lines of code related to y, these are irrelevant at the first-stage. As we will see in the tables, the coefficient and F-stat for all 6 Ys is the same. * First Stage Analysis eststo clear foreach y in `outcomes&#39; { use &quot;${XX}/GM_cz_final_dataset.dta&quot;, clear keep if `y&#39; != . qui sum GM local ols_SD = `r(sd)&#39; replace `y&#39; = 100 * `y&#39; reg $x_ols $x_iv ${baseline_controls} if `y&#39; != . eststo `y&#39; test $x_iv = 0 estadd scalar fstat = `r(F)&#39; } cd &quot;$XX&quot; esttab `outcomes&#39; using &quot;`r&#39;_hh_table.tex&quot;, frag replace varwidth(25) label se /// stats(fstat, labels( F-Stat)) keep($x_iv) mgroups(&quot;&quot;, prefix(\\multicolumn{6}{c}{First Stage on GM})) nonumber /// nostar nomtitle nonotes nolines nogaps prefoot(\\cmidrule(lr){2-7}) postfoot(\\cmidrule(lr){2-7}) substitute({table} {threeparttable}) Here, we’ll learn how to export a table and make it publication ready. First, the table is exported using esttab into a .tex file, since LateX provides consistent formatting that is widely accepted by academic publishers. The frag option allows us to fragment the table into multiple sections to be able to insert formatting options like horizontal lines to demarcate different sections. replace replaces the table in the directory if it is already saved. varwidth(25) makes sure the variables are aligned in columns with consistent widths, or in this case, 25 characters. label shows the variable labels instead of variable names. The se option adds standard errors to the table next to the regression coefficient stats() specifies the summary statistics in the table. Here, we are keeping the fstat keep() only keeps necessary variables. Here, we are only showcasing the coefficient of our x_iv in the table. The code ensures clear formatting: Suppresses stars (nostar) , titles (nomtitle), notes (nonotes) , unnecessary lines (nogaps). These are options that can be used to extract any table, even if not in the tex format. Now we move to .tex specific options: mgroups is used to groups columns under a common label. It will appear on top of the columns that are relative to our first stage regression, indicating that they are “under the same umbrella”. Further enhancements are also made: prefix is used. It allows us to edit the title. The name of the mgroup is empty ”” because it will be added in the prefix, which has the following form: prefix(\\multicolumn{span}{centering}{text}’. Span specifies over how many columns the title will span over. Since we’re looping over 6 outcomes, that will be 6. {c} centers the title at the center of the columns. Finally, we insert the title in {text}. prefoot() inserts a horizontal line before the footer of the table in latex, and postfoot() adds a line after the footer marking the end of the table. For instance, prefoot(\\cmidrule(lr){2-7}) spans the line from column 2 to column 7, and lr aligns the line to the left (l) and right (r) edges of the table columns. Finally, substitute({table} {threeparttable}) subsidies the “table” environment to a “threeparttable” environment that gives a more structure approach in latex as it splits the table into three distinct parts: the caption above the table, the main body, and the notes below the table. 2.2.7.2 OLS and Reduced form Regressions As part of the same ongoing loop, we then conduct Ordinary Least Squares (OLS) and Reduced form (RF) regressions for a series of outcome variables (outcomes) to evaluate the impact of the treatment variable \\$x_ols on each outcome while controlling for baseline characteristics \\${baseline_controls} and the direct impact of the instrument on y, respectively. It iteratively loads the dataset, removes missing values for the current outcome, and rescales outcomes to percentile ranks for interpretability. The regression results for each outcome are stored with eststo. We then append (esttab, append) the regression results to our first LaTeX table file, structured to compare pooled, women, and men groups across low and high income categories as you can read in this command: prehead(&quot;\\\\&quot; &quot;&amp;\\multicolumn{3}{c}{Low Income}&amp;\\multicolumn{3}{c}{High Income}\\\\&quot; /// &quot;&amp;\\multicolumn{1}{c}{Pooled}&amp;\\multicolumn{1}{c}{Women}&amp;\\multicolumn{1}{c}{Men}&amp;\\multicolumn{1}{c}{Pooled}&amp;\\multicolumn{1}{c}{Women}&amp;\\multicolumn{1}{c}{Men} It names each column accordingly, and gives an overarching title for low income vs high income individuals with multicolumn and subtitles with multicolumn(1), multicolumn(2), ect. with the same options for table design that we explained earlier. * OLS eststo clear foreach y in `outcomes&#39;{ use &quot;${XX}/GM_cz_final_dataset.dta&quot;, clear keep if `y&#39;!=. * Rescale treatment in terms of standard deviations qui sum GM local ols_SD=`r(sd)&#39; * Rescale outcome in percentile ranks replace `y&#39;=100*`y&#39; reg `y&#39; $x_ols ${baseline_controls} if `y&#39;!=. eststo `y&#39; } cd &quot;$XX&quot; esttab `outcomes&#39; using &quot;`r&#39;_hh_table.tex&quot;, frag append varwidth(25) label se /// prehead(&quot;\\\\&quot; &quot;&amp;\\multicolumn{3}{c}{Low Income}&amp;\\multicolumn{3}{c}{High Income}\\\\&quot; /// &quot;&amp;\\multicolumn{1}{c}{Pooled}&amp;\\multicolumn{1}{c}{Women}&amp;\\multicolumn{1}{c}{Men}&amp;\\multicolumn{1}{c}{Pooled}&amp;\\multicolumn{1}{c}{Women}&amp;\\multicolumn{1}{c}{Men} \\\\\\cmidrule(lr){2-7}&quot;) /// stats( r2, labels( R-squared)) keep($x_ols) mgroups(&quot;&quot;, prefix(\\multicolumn{6}{c}{Ordinary Least Squares})) nonumber /// nostar nomtitle nonotes nolines nogaps prefoot(\\cmidrule(lr){2-7}) postfoot(\\cmidrule(lr){2-7}) substitute({table} {threeparttable}) * RF eststo clear foreach y in `outcomes&#39;{ use &quot;${XX}/GM_cz_final_dataset.dta&quot;, clear keep if `y&#39;!=. * Rescale treatment in terms of standard deviations qui sum GM local ols_SD=`r(sd)&#39; * Rescale outcome in percentile ranks replace `y&#39;=100*`y&#39; reg `y&#39; $x_iv ${baseline_controls} if `y&#39;!=. eststo `y&#39; } cd &quot;$XX&quot; esttab `outcomes&#39; using &quot;`r&#39;_hh_table.tex&quot;, frag append varwidth(25) label se /// stats( r2, labels( R-squared)) keep($x_iv) mgroups(&quot;&quot;, prefix(\\multicolumn{6}{c}{Reduced Form})) nonumber /// nostar nomtitle nonotes nolines nogaps prefoot(\\cmidrule(lr){2-7}) postfoot(\\cmidrule(lr){2-7}) substitute({table} {threeparttable}) 2.2.7.3 Two-Stage Least Squares (2SLS) regressions We then perform Two-Stage Least Squares (2SLS) regressions to address endogeneity and estimate causal effects between the Great Migration $x_ols and outcomes outcomes. For 2SLS, we have to use the command ivreg2. We write ivreg2 y (x_ols = x_iv) with = to specify that we are concerned about endogeneity in variable x_ols and we’re using the IV x_iv to address it. The instrument $x_iv predicts the endogenous treatment in the first stage, while the second stage regresses the outcomes on the predicted treatment, controlling for baseline_controls. Outcome variables are again scaled. After running the 2SLS, we save the coefficient, absolute value of the coefficient, and standard error into locals. We also export the coefficients using our Printest program. We finish by saving the Mean Rank, SD Rank, and SD GM as scalars and appending the tex table to our former outputs. Finally, we create a footer by exporting a table that includes everything we want to add (e.g. our scalars) and appending it to the original. * 2SLS eststo clear foreach y in `outcomes&#39;{ use &quot;${XX}/GM_cz_final_dataset.dta&quot;, clear keep if `y&#39;!=. * Rescale treatment in terms of standard deviations qui sum GM local ols_SD=`r(sd)&#39; * Rescale outcome in percentile ranks replace `y&#39;=100*`y&#39; ivreg2 `y&#39; ($x_ols = $x_iv ) ${baseline_controls} if `y&#39;!=., first local GM_`y&#39; = _b[$x_ols] local GM_`y&#39;_abs = abs(_b[$x_ols]) local GM_`y&#39;_se : di %4.3f _se[$x_ols] PrintEst `GM_`y&#39;&#39; &quot;GM_`y&#39;&quot; &quot;&quot; &quot; percentile points (s.e. = `GM_`y&#39;_se&#39;)%&quot; &quot;4.3&quot; PrintEst `GM_`y&#39;_abs&#39; &quot;GM_`y&#39;_abs&quot; &quot;&quot; &quot; percentile points (s.e. = `GM_`y&#39;_se&#39;)%&quot; &quot;4.3&quot; eststo `y&#39; use &quot;${XX}/GM_cz_final_dataset.dta&quot;, clear keep if `y&#39;!=. sum `y&#39; estadd scalar basemean=r(mean) estadd scalar sd=r(sd) estadd scalar gm_sd=`ols_SD&#39; } cd &quot;$XX&quot; esttab `outcomes&#39; using &quot;`r&#39;_hh_table.tex&quot;, frag append varwidth(25) label se /// keep($x_ols) mgroups(&quot;&quot;, prefix(\\multicolumn{6}{c}{Two-stage least squared})) nonumber /// nostar nomtitle nonotes nolines nogaps prefoot(\\cmidrule(lr){2-7}) substitute({table} {threeparttable}) * Footer cd &quot;$XX&quot; esttab `outcomes&#39; using &quot;`r&#39;_hh_table.tex&quot;, frag append varwidth(25) label se /// stats( N basemean sd gm_sd, labels(&quot;Observations&quot; &quot;Mean Rank&quot; &quot;SD Rank&quot; &quot;SD GM&quot;)) drop(*) nonumber /// nostar nomtitle nonotes nolines nogaps prefoot(\\cmidrule(lr){2-7}) substitute({table} {threeparttable}) } We arrive to this outcome: 2.2.8 Robustness checks Table 9 tests the robustness of the paper’s results on the effect of the great migration on the upward mobility of black men by adding a new set of controls to check the robustness of the results to alternative specifications. First, we store the variables used in each of 8 columns of the table in macros for efficiency and convenience global nocon &quot;v2_blackmig3539_share1940&quot; global divfe &quot;v2_blackmig3539_share1940 reg2 reg3 reg4&quot; global baseline &quot;frac_all_upm1940 mfg_lfshare1940 v2_blackmig3539_share1940 reg2 reg3 reg4&quot; global emp &quot;frac_all_upm1940 mfg_lfshare1940 v2_blackmig3539_share1940 reg2 reg3 reg4 emp_hat&quot; global flexbpop40 &quot;frac_all_upm1940 mfg_lfshare1940 v2_blackmig3539_share1940 reg2 reg3 reg4 i.bpopquartile&quot; global swmig &quot;frac_all_upm1940 mfg_lfshare1940 v2_blackmig3539_share1940 reg2 reg3 reg4 GM_hat8&quot; global eurmig &quot;frac_all_upm1940 mfg_lfshare1940 v2_blackmig3539_share1940 reg2 reg3 reg4 eur_mig&quot; global supmob &quot;frac_all_upm1940 mfg_lfshare1940 v2_blackmig3539_share1940 reg2 reg3 reg4 vm_blacksmob1940&quot; eststo clear use &quot;${XX}/GM_cz_final_dataset.dta&quot;, clear Before we break down the rest of the code, let’s look at a new commands. estadd local &lt;name&gt; \"Y\"/\"N\" saves labels indicating which controls are included to track model features (Yes if the controls are added /No if not). When we compile the table, we’ll add name as a statistic in separate table rows. local y kir_black_male_p252015 replace `y&#39; = 100 * `y&#39; reg $x_ols $x_iv $nocon test $x_iv = 0 local fstat=`r(F)&#39; reg `y&#39; $x_ols $nocon eststo nocon_ols estadd local hasdivfe &quot;N&quot; estadd local hasbaseline &quot;N&quot; estadd local hasemp &quot;N&quot; estadd local hasflexbpop40 &quot;N&quot; estadd local hasswmig &quot;N&quot; estadd local haseurmig &quot;N&quot; estadd local hassupmob &quot;N&quot; estadd local precisionwt &quot;Y&quot; ivreg2 `y&#39; ($x_ols = $x_iv) $nocon eststo nocon estadd scalar fstat = `fstat&#39; // Now `fstat` is correctly defined reg $x_ols $x_iv $divfe test $x_iv = 0 local fstat = r(F) // Update `fstat` for the new test local y kir_black_male_p252015 and replace `y' = 100 * `y' prepare the dependent variable by turning it into a local then converting it to percentages, respectively. Then, each specification (table column) goes through the same process: first-stage regression, OLS regression, and then IV regression. The first estimation with no controls has been run first and the we run the first-stage regression and save the F-stat into the local scalar named fstat, then we run an OLS, saves the estimation of the results using eststo under the name nocon_ols, and adds the specifications of the model using estadd. Finally, We run the IV regression for which the coefficient is saved with eststo under the name nocon, and save the F-stat value under the same fstat local. reg `y&#39; $x_ols $divfe eststo divfe_ols estadd local hasdivfe &quot;Y&quot; estadd local hasbaseline &quot;N&quot; estadd local hasemp &quot;N&quot; estadd local hasflexbpop40 &quot;N&quot; estadd local hasswmig &quot;N&quot; estadd local haseurmig &quot;N&quot; estadd local hassupmob &quot;N&quot; estadd local precisionwt &quot;Y&quot; ivreg2 `y&#39; ($x_ols = $x_iv ) $divfe eststo divfe estadd scalar fstat=`fstat&#39; reg $x_ols $x_iv $baseline test $x_iv = 0 local fstat=`r(F)&#39; reg `y&#39; $x_ols $baseline eststo baseline_ols estadd local hasdivfe &quot;Y&quot; estadd local hasbaseline &quot;Y&quot; estadd local hasemp &quot;N&quot; estadd local hasflexbpop40 &quot;N&quot; estadd local hasswmig &quot;N&quot; estadd local haseurmig &quot;N&quot; estadd local hassupmob &quot;N&quot; estadd local precisionwt &quot;Y&quot; ivreg2 `y&#39; ($x_ols = $x_iv ) $baseline eststo baseline estadd scalar fstat=`fstat&#39; reg $x_ols $x_iv $emp test $x_iv = 0 local fstat=`r(F)&#39; reg `y&#39; $x_ols $emp eststo emp_ols estadd local hasdivfe &quot;Y&quot; estadd local hasbaseline &quot;Y&quot; estadd local hasemp &quot;Y&quot; estadd local hasflexbpop40 &quot;N&quot; estadd local hasswmig &quot;N&quot; estadd local haseurmig &quot;N&quot; estadd local hassupmob &quot;N&quot; estadd local precisionwt &quot;Y&quot; ivreg2 `y&#39; ($x_ols = $x_iv ) $emp eststo emp estadd scalar fstat=`fstat&#39; reg $x_ols $x_iv $flexbpop40 test $x_iv = 0 local fstat=`r(F)&#39; reg `y&#39; $x_ols $flexbpop40 eststo flexbpop40_ols estadd local hasdivfe &quot;Y&quot; estadd local hasbaseline &quot;Y&quot; estadd local hasemp &quot;N&quot; estadd local hasflexbpop40 &quot;Y&quot; estadd local hasswmig &quot;N&quot; estadd local haseurmig &quot;N&quot; estadd local hassupmob &quot;N&quot; estadd local precisionwt &quot;Y&quot; ivreg2 `y&#39; ($x_ols = $x_iv ) $flexbpop40 eststo flexbpop40 estadd scalar fstat=`fstat&#39; reg $x_ols $x_iv $swmig test $x_iv = 0 local fstat=`r(F)&#39; reg `y&#39; $x_ols $swmig eststo swmig_ols estadd local hasdivfe &quot;Y&quot; estadd local hasbaseline &quot;Y&quot; estadd local hasemp &quot;N&quot; estadd local hasflexbpop40 &quot;N&quot; estadd local hasswmig &quot;Y&quot; estadd local haseurmig &quot;N&quot; estadd local hassupmob &quot;N&quot; estadd local precisionwt &quot;Y&quot; ivreg2 `y&#39; ($x_ols = $x_iv ) $swmig eststo swmig estadd scalar fstat=`fstat&#39; reg $x_ols $x_iv $eurmig test $x_iv = 0 local fstat=`r(F)&#39; reg `y&#39; $x_ols $eurmig eststo eurmig_ols estadd local hasdivfe &quot;Y&quot; estadd local hasbaseline &quot;Y&quot; estadd local hasemp &quot;N&quot; estadd local hasflexbpop40 &quot;N&quot; estadd local hasswmig &quot;N&quot; estadd local haseurmig &quot;Y&quot; estadd local hassupmob &quot;N&quot; estadd local precisionwt &quot;Y&quot; ivreg2 `y&#39; ($x_ols = $x_iv ) $eurmig eststo eurmig estadd scalar fstat=`fstat&#39; reg $x_ols $x_iv $supmob test $x_iv = 0 local fstat=`r(F)&#39; reg `y&#39; $x_ols $supmob eststo supmob_ols estadd local hasdivfe &quot;Y&quot; estadd local hasbaseline &quot;Y&quot; estadd local hasemp &quot;N&quot; estadd local hasflexbpop40 &quot;N&quot; estadd local hasswmig &quot;N&quot; estadd local haseurmig &quot;N&quot; estadd local hassupmob &quot;Y&quot; estadd local precisionwt &quot;Y&quot; ivreg2 `y&#39; ($x_ols = $x_iv ) $supmob eststo supmob estadd scalar fstat= `fstat&#39; The process re-iterates while progressively adding new controls. To compile the table and convert it to a LateX format, we use the following code: cd &quot;$figtab&quot; esttab nocon divfe baseline flexbpop40 supmob swmig eurmig emp using &quot;main_robust_table_bmp25.tex&quot;, frag replace varwidth(25) label se /// stats( fstat, labels(&quot;First Stage F-Stat&quot;)) keep($x_ols) coeflabel(GM &quot;GM (2SLS)&quot;) nonumber /// nostar nomtitle nonotes nolines nogaps substitute({table} {threeparttable}) prefoot(\\cmidrule(lr){2-9}) esttab nocon_ols divfe_ols baseline_ols flexbpop40_ols supmob_ols swmig_ols eurmig_ols emp_ols using &quot;main_robust_table_bmp25.tex&quot;, frag append varwidth(25) label se /// prehead(&quot;\\\\&quot;) coeflabel(GM &quot;GM (OLS)&quot;) /// stats( r2 N precisionwt hasdivfe hasbaseline hasflexbpop40 hassupmob hasswmig haseurmig hasemp , /// labels( &quot;R-squared (OLS)&quot; N &quot;Precision Wt&quot; &quot;Census Div FE&quot; &quot;Baseline Controls&quot; &quot;1940 Black Share Quartile FEs&quot; &quot;Southern Mob&quot; /// &quot;White South Mig&quot; &quot;Eur Mig&quot; &quot;Emp Bartik&quot; )) keep($x_ols) nonumber /// nostar nomtitle nonotes nolines prefoot(\\cmidrule(lr){2-9}) postfoot(\\cmidrule(lr){2-9}) substitute({table} {threeparttable}) We create two tables and append them. For both tables, the code ensures the same formatting as results tables. The first table displays results from the 2SLS regressions for different specifications, and showcases the coefficient of interest and the F-stat, labelled as GM(2SLS) using the command coeflabel(GM \"GM (2SLS)\"). The second table showcases and appends OLS regression results to the same LaTeX file using append. We include model specifications using stats. Compiled in a LateX reader, we get: 2.2.9 Contribution Throughout this document we explained these STATA tricks: How to manage dynamic file creation and output, including generating files programmatically, organizing results, and saving them in a structured manner using tempname, file open, and file write. How to handle string matching with regexm() to manipulate and process string variables based on specific patterns. How to use macros and loops effectively, automating repetitive tasks with nested loops and running multiple regressions for different groups or variables using both local and global macros, and learning to identify the correct use for each. How to seamlessly export to LaTeX for Publication-Ready Tables, making it incredibly easy to present your regression outputs in publication-ready tables. Using esttab and eststo, we create formatted tables that are immediately ready for inclusion in LaTeX documents, eliminating the need for manual formatting. How to implement instrumental variable (IV) regressions using the ivreg2 package, understanding how to address endogeneity by using external instruments and analyzing first-stage results for instrument strength. Authors: Gaia Bechara, Evely Geroulakous, Oleksandra Povstiana, Fatemeh Razavi, Students in Master program in Development Economics (2024-2025), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2024 "],["regression-discontinuity-design.html", "Chapter 3 Regression Discontinuity Design ", " Chapter 3 Regression Discontinuity Design "],["the-persistent-effects-of-perus-mining-mita.html", "3.1 The Persistent Effects of Peru’s Mining Mita", " 3.1 The Persistent Effects of Peru’s Mining Mita Dell, M. (2010). The Persistent Effects of Peru’s Mining Mita. Econometrica, 78(6), 1863-1903. Retrieved 2023-11-20, from http://www.jstor.org/stable/40928464 Download Do-File to download/generate/save the dataset for replication Download Do-File corresponding to the explanations below Download Codebook [Link to the full original replication package paper on Melissa Dell’s webpage] Highlights Dell (2010) examines the long run impacts of the mita, an extensive forced mining labor system in effect in Peru and Bolivia between 1573 and 1812. We will code a spatial regression discontinuity design that is used to compare outcomes of households on each side of the mita boundaries. This paper is a pioneering work in the application of spatial regression discontinuity designs and lays the foundations for a burgeoning literature on the impact of spatially delimited policies. In this document, we provide a detailed explanation on how to implement spatial RDD in your software Stata. A key element of our contribution consists in capturing Conley standard errors using the “x_ols” program (see Install x_ols line 142). In the appendix you will find the codes that allow the automatization of formatted output tables in a ready to publish style with minimal required manual manipulation using matrices and the “appendmodels\" program. 3.1.1 Introduction A useful method for analyzing the impact of a policy when it is implemented in a geographically delimited area is the Spatial Regression Discontinuity Design (Spatial RDD). The paper focuses on the causal effect of the mita, an extensive forced mining labor system in effect in Peru and Bolivia between 1573 and 1812, on household consumption and stunted growth in children. This identification strategy is based on a cut-off defined by geographic borders, specifically the assignment variables are examined by their distance from the mita boundary (the threshold in this research) using a discontinuity in longitude-latitude space. This framework is based on the assumption that all the characteristics of the treatment and the control groups, except the variables of interest, must vary smoothly at the mita cutoff to be able to do a comparison. Therefore, the treatment effect is firstly computed by using cubic polynomials in latitude and longitude in a multidimensional RD polynomial. Then, to strengthen the results, other two single geographical dimension specifications were used. The first one uses the cubic polynomial Euclidian distance to Potosì and the second one the cubic polynomial distance to the mita boundary. 3.1.2 Good Practices 3.1.2.1 Necessary libraries ssc install estout, replace x_ols (Appendix) appendmodels (Appendix) 3.1.2.2 Organization of the directories capture log close clear clear matrix global dirroot &quot;YOUR DIRECTORY GOES HERE&quot; global dirdata &quot;${dirroot}/&quot; global dirlogs &quot;${dirroot}/&quot; global dirtables &quot;${dirroot}/&quot; log using &quot;${dirlogs}/Replication_Dell.log&quot;, replace 3.1.2.3 Building the three different data frames Note: to generate the dataset Dataset_Dell.dta, you first need to download and run this do-file: Download Do-File to download/generate/save the dataset for replication use &quot;${dirdata}Dataset_Dell.dta&quot;, clear keep if gis_db==1 drop gis_db save &quot;${dirdata}gis_grid.dta&quot;, replace use &quot;${dirdata}Dataset_Dell.dta&quot;, clear keep if consumption_db==1 drop consumption_db save &quot;${dirdata}consumption.dta&quot;, replace use &quot;${dirdata}Dataset_Dell.dta&quot;, clear keep if height_db==1 drop height_db save &quot;${dirdata}height.dta&quot;, replace 3.1.3 Table 1 - Summary Statistics Table 1: Summary Statistics 3.1.3.1 Code for Table 1 A critical assumption for the identification strategy is that all the characteristics of the treatment and control vary smoothly at the mita cutoff. Table 1 presents summary statistics for sample characteristics within and outside of the mita boundaries. Different distances from the Mita boundaries are used in the selection of observations to include in each group. Reassuringly, there are no significant differences. We are going to use a loop to move through each distance specification. We are using for this the data gis_grid.dta. Then, with the command tabstat we are going to create a table with the mean and the number of observations for the elevation by group (pothuan_mita, which is 0 if it’s outside the Mita and 1 otherwise) and the same for the slope. This has the option not which is an abbreviation for nototal because we are not interested in the overall statistics. After this we’re going to regress the elevation and the slope with the dummy of pothuan_mita in order to capture the robust standard error of the difference in mean between the two groups. // Elevation and Slope foreach Y of num 100 75 50 25 { //Elevation use &quot;${dirdata}gis_grid.dta&quot;, clear drop if d_bnd&gt;`Y&#39; tabstat elev, by(pothuan_mita) statistics(mean n) not regress elev pothuan_mita, robust //Slope tabstat slope, by(pothuan_mita) statistics(mean n) not regress slope pothuan_mita, robust } We then repeat this with the quantity of indigenous people. We are going to be using the consumption.dta dataset. // Indigenous foreach Y of num 100 75 50 25 { use &quot;${dirdata}consumption.dta&quot;, clear tabstat QUE if (d_bnd&lt;`Y&#39; &amp; cusco!=1), by(pothuan_mita) statistics(mean n) not regress QUE pothuan_mita if (d_bnd&lt;`Y&#39; &amp; cusco!=1), robust cluster (ubigeo) } Finally, we are going to be capturing the Conley Standard Error. This is done using the program x_ols which we install at the beginning of the code. We are also going to do it for the elevation, slope and indigenous people. This command works by specifying the latitude (“xcord\"), longitude (”ycord\"), two cut points in each coordinate and the dependent (“elev\") and independent variable (”pothuan_mita\". xreg(2) and coord(2) are necessary, xreg denotes the number of regressor and coord the dimensions of the coordinates. For more information on this command: https://economics.uwo.ca/people/conley_docs/data_GMMWithCross_99/x_ols.ado //Conley Standard Errors // Elevation and Slope foreach Y of num 100 75 50 25 { // Elevation use &quot;${dirdata}gis_grid.dta&quot;, clear keep if elev&lt;. gen xcut1=1 gen ycut1=1 gen const=1 drop if d_bnd&gt;`Y&#39; x_ols xcord ycord xcut1 ycut1 elev const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 // Slope use &quot;${dirdata}gis_grid.dta&quot;, clear drop if slope==. gen xcut1=1 gen ycut1=1 gen const=1 drop if d_bnd&gt;`Y&#39; x_ols xcord ycord xcut1 ycut1 slope const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 } // Indigenous foreach Y of num 100 75 50 25 { use &quot;${dirdata}consumption.dta&quot;, clear drop if QUE==. drop if cusco==1 gen xcut1=1 gen ycut1=1 gen const=1 drop if d_bnd&gt;`Y&#39; x_ols x y xcut1 ycut1 QUE const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 } 3.1.4 Table 2 - Main Results Table 2: Main Results 3.1.4.1 Code for Table 2 The paper is interested in the impact of Mita on both economic outcomes (household consumption) and outcomes in terms of health (stunted growth) so both results are presented in the main table with the following code. Again, a loop is used to move through each distance specification. The loop ensures that the same regressions are run for each distance specification specified in the local Y. Within the loop, we use the two main datasets on consumption and height and run the same regressions on the two main dependent variables lhhequiv and desnu on the cubic polynomial of the observation’s district capital, controlling for relevant variables. The if conditions specify that the regression omits observations in Cusco and indicates the distance specification that should be used. The robust option specifies that heteroskedasticity-robust standard errors should be reported. The cluster (ubigeo) option indicates that standard errors should be clustered by district. local Y &quot;100 75 50&quot; foreach Y in `Y&#39; { // Log Consumption use &quot;${dirdata}consumption.dta&quot;, clear reg lhhequiv pothuan_mita x y x2 y2 xy x3 y3 x2y xy2 infants children adults elv_sh /// slope bfe4* if (cusco != 1 &amp; d_bnd &lt; `Y&#39;), robust cluster (ubigeo) // Stunted Growth use &quot;${dirdata}height.dta&quot;, clear reg desnu pothuan_mita x y x2 y2 xy x3 y3 x2y xy2 elv_sh slope bfe4* /// if (cusco != 1 &amp; d_bnd &lt; `Y&#39;), robust cluster (ubigeo) } 3.1.5 Table 3 - Robustness Results Table 3: Robustness 3.1.5.1 Code for Table 3 The multidimensional RD polynomial approach was a novel methodology at the time and therefore there was no empirical background able to state a priori why this strategy performs better. Given these concerns, two other single-dimension specifications were applied to examine the robustness of the findings. The first is the polynomial in distance to Potosi, which is likely to capture variation in unobservable characteristics but it is not the most precise approach in mapping RD setup. The second specification controls for a polynomial in distance to the Mita boundary, which is closer to traditional one-dimensional RD designs even if it lacks a historical explanation that states its relevancy. Looking at the results across the three specifications is not possible to reject that they are statistically identical and consequently admit the robustness of the main findings. Another time, we use the two main datasets on consumption and height to capture the causal effect on the dependent variables lhhequiv and desnu. In the loop, first, we consider the effect of the mita on the consumption by including in the regression the Euclidean distance to Potosi in its linear, quadratic, and cubic form (together with relevant control variables and excluding the observation in Cusco) within 50, 70 and 100 km (Y). The second regression is run using the same dependent variable and controls but using the distance to the mita boundary in its linear, quadratic, and cubic terms. The following two regressions have exactly the same specifications but as dependent variable is used stunted growth. Finally, the last two regressions are peculiar to the previous ones but use the observation at the three border limits (again 50, 75, and 100 km from the Mita border). local Y &quot;100 75 50&quot; foreach Y in `Y&#39; { //Log Consumption use &quot;${dirdata}consumption.dta&quot;, clear // Cubic polynomial in distance to Potosi reg lhhequiv pothuan_mita dpot dpot2 dpot3 infants children adults elv_sh slope bfe4* /// if (cusco!=1 &amp; d_bnd&lt;`Y&#39;),robust cluster (ubigeo) // Cubic polynomial in distance to the mita boundary reg lhhequiv pothuan_mita dbnd_sh dbnd_sh2 dbnd_sh3 infants children adults elv_sh /// slope bfe4* if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) // Stunted Growth use &quot;${dirdata}height.dta&quot;, clear // Cubic polynomial in distance to Potosi reg desnu pothuan_mita dpot dpot2 dpot3 elv_sh slope bfe4* if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), /// robust cluster (ubigeo) // Cubic polyonomial in distance to the mita boundary reg desnu pothuan_mita dbnd_sh dbnd_sh2 dbnd_sh3 elv_sh slope bfe4* /// if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) } reg desnu pothuan_mita dpot dpot2 dpot3 elv_sh slope bfe4* if (cusco!=1 &amp; border==1), /// robust cluster (ubigeo) reg desnu pothuan_mita dbnd_sh dbnd_sh2 dbnd_sh3 elv_sh slope bfe4* /// if (cusco!=1 &amp; border==1), robust cluster (ubigeo) 3.1.6 Appendix: Automatization of table outputs The following code allows the creation of each table without minimal manual manipulation. We use two possible approaches, one is through the use of matrices and the other through the use of “eststo\" with the”append models\" program written by Ben Jann (more information here: https://repec.sowi.unibe.ch/stata/estout/other/901.do). 3.1.6.1 Automatic Output for Table 1 eststo clear scalar drop _all clear matrix // Elevation foreach Y of num 100 75 50 25 { //Elevation use &quot;${dirdata}gis_grid.dta&quot;, clear drop if d_bnd&gt;`Y&#39; tabstat elev, by(pothuan_mita) statistics(mean) not save matrix insi_elev_`Y&#39;=r(Stat2) matrix outs_elev_`Y&#39;=r(Stat1) quietly regress elev pothuan_mita, robust matrix V=e(V) scalar se_elev_`Y&#39; = sqrt(V[1,1]) *Conley Standard Errors keep if elev&lt;. gen xcut1=1 gen ycut1=1 gen const=1 x_ols xcord ycord xcut1 ycut1 elev const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 scalar con_se_elev_`Y&#39;=con_se2 //Slope use &quot;${dirdata}gis_grid.dta&quot;, clear drop if d_bnd&gt;`Y&#39; tabstat slope, by(pothuan_mita) statistics(mean) not save matrix insi_slope_`Y&#39;=r(Stat2) matrix outs_slope_`Y&#39;=r(Stat1) tabstat slope, by(pothuan_mita) statistics(n) not save matrix N_insi_slope_`Y&#39;=r(Stat2) matrix N_outs_slope_`Y&#39;=r(Stat1) quietly regress slope pothuan_mita, robust matrix V=e(V) scalar se_slope_`Y&#39; = sqrt(V[1,1]) *Conley Standard Errors drop if slope==. gen xcut1=1 gen ycut1=1 gen const=1 x_ols xcord ycord xcut1 ycut1 slope const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 scalar con_se_slope_`Y&#39;=con_se2 } // Indigenous foreach Y of num 100 75 50 25 { use &quot;${dirdata}consumption.dta&quot;, clear tabstat QUE if (d_bnd&lt;`Y&#39; &amp; cusco!=1), by(pothuan_mita) statistics(mean) not save matrix insi_indig_`Y&#39;=r(Stat2)*100 matrix outs_indig_`Y&#39;=r(Stat1)*100 tabstat QUE if (d_bnd&lt;`Y&#39; &amp; cusco!=1), by(pothuan_mita) statistics(n) not save matrix N_insi_indig_`Y&#39;=r(Stat2) matrix N_outs_indig_`Y&#39;=r(Stat1) quietly regress QUE pothuan_mita if (d_bnd&lt;`Y&#39; &amp; cusco!=1), robust cluster (ubigeo) matrix V=e(V) scalar se_indig_`Y&#39; = sqrt(V[1,1])*100 drop if QUE==. drop if cusco==1 gen xcut1=1 gen ycut1=1 gen const=1 drop if d_bnd&gt;`Y&#39; x_ols x y xcut1 ycut1 QUE const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 scalar con_se_indig_`Y&#39;=con_se2*100 } // Inputting everything on a Matrix // Submatrix 1: Elevation matrix elev = (insi_elev_100, outs_elev_100, con_se_elev_100, insi_elev_75, /// outs_elev_75, con_se_elev_75, insi_elev_50, outs_elev_50, con_se_elev_50, /// insi_elev_25, outs_elev_25, con_se_elev_25 \\ ., ., se_elev_100, ., ., se_elev_75, ., /// ., se_elev_50, ., ., se_elev_25) // Submatrix 2: Slope matrix slope = (insi_slope_100, outs_slope_100, con_se_slope_100, insi_slope_75, /// outs_slope_75, con_se_slope_75, insi_slope_50, outs_slope_50, con_se_slope_50, /// insi_slope_25, outs_slope_25, con_se_slope_25 \\ ., ., se_slope_100, ., ., se_slope_75, /// ., ., se_slope_50, ., ., se_slope_25) // Submatrix 3: Number of observations for elevation and slope matrix num_slope = (N_insi_slope_100, N_outs_slope_100, ., N_insi_slope_75, /// N_outs_slope_75, ., N_insi_slope_50, N_outs_slope_50, ., N_insi_slope_25, /// N_outs_slope_25, .) // Submatrix 4: Indigenous matrix indig = (insi_indig_100, outs_indig_100, con_se_indig_100, insi_indig_75, /// outs_indig_75, con_se_indig_75, insi_indig_50, outs_indig_50, con_se_indig_50, /// insi_indig_25, outs_indig_25, con_se_indig_25 \\ ., ., se_indig_100, ., ., se_indig_75, /// ., ., se_indig_50, ., ., se_indig_25) // Submatrix 5: Number of observations for Indigenous matrix num_indig = (N_insi_indig_100, N_outs_indig_100, ., N_insi_indig_75, /// N_outs_indig_75, ., N_insi_indig_50, N_outs_indig_50, ., N_insi_indig_25, /// N_outs_indig_25, .) // Final Matrix matrix Table1 = (elev\\slope\\num_slope\\indig\\num_indig) matrix rownames Table1 = Elevation &quot;.&quot; Slope &quot;.&quot; Observations %Indigenous &quot;.&quot; Observations matrix colnames Table1 = Inside Outside &quot;Conley SE/SE&quot; Inside Outside &quot;Conley SE/SE&quot; /// Inside Outside &quot;Conley SE/SE&quot; Inside Outside &quot;Conley SE/SE&quot; // Show in command esttab matrix(Table1, fmt(&quot;0 0 2 0 0 2 0 0 &quot; &quot;0 0 2 0 0 2 0 0 &quot; &quot;2 2 2 2 0 2 2&quot;)), /// refcat(Elevation &quot;GIS Measures&quot; %Indigenous &quot;&quot;, nolab) /// coef(. &quot; &quot;) /// title(&quot;Table 1: Summary Statistics&quot;) nomti /// addn(&quot;The unit of observation is 20 × 20 km grid cells for the geospatial measures and the household for % indigenous. Conley standard errors for the difference in means between mita and non-mita observations are first in each SE columns. Robust standard errors for the difference in means are second. For % indigenous, the robust standard errors are corrected for clustering at the district level. The geospatial measures are calculated using elevation data at 30 arc second (1 km) resolution (SRTM (2000)). The unit of measure for elevation is 1000 meters and for slope is degrees. A household is indigenous if its members primarily speak an indigenous language in the home (ENAHO (2001)). In the first three columns, the sample includes only observations located less than 100 km from the mita boundary, and this threshold is reduced to 75, 50, and finally 25 km in the succeeding columns.&quot;) replace 3.1.6.2 Automatic Output for Table 2 eststo clear scalar drop _all clear matrix local Y &quot;100 75 50&quot; local j = 1 foreach Y in `Y&#39; { // Log Consumption use &quot;${dirdata}consumption.dta&quot;, clear quietly reg lhhequiv pothuan_mita x y x2 y2 xy x3 y3 x2y xy2 infants children adults /// elv_sh slope bfe4* if (cusco != 1 &amp; d_bnd &lt; `Y&#39;), robust cluster (ubigeo) local i = 1 scalar Nclusters_`j&#39;_`i&#39; =e(N_clust) scalar N_`j&#39;_`i&#39; =e(N) scalar R_`j&#39;_`i&#39; =e(r2) matrix V=e(V) matrix B=e(b) scalar se_`j&#39;_`i&#39; = sqrt(V[1,1]) scalar b_`j&#39;_`i&#39; = (B[1,1]) local i = `i&#39; + 1 // Stunted Growth use &quot;${dirdata}height.dta&quot;, clear quietly reg desnu pothuan_mita x y x2 y2 xy x3 y3 x2y xy2 elv_sh slope bfe4* /// if (cusco != 1 &amp; d_bnd &lt; `Y&#39;), robust cluster (ubigeo) scalar Nclusters_`j&#39;_`i&#39; =e(N_clust) scalar N_`j&#39;_`i&#39; =e(N) scalar R_`j&#39;_`i&#39; =e(r2) matrix V=e(V) matrix B=e(b) scalar se_`j&#39;_`i&#39; = sqrt(V[1,1]) scalar b_`j&#39;_`i&#39; = (B[1,1]) local j = `j&#39; + 1 } matrix Main = (b_1_1, b_2_1, b_3_1 \\ se_1_1, se_2_1, se_3_1 \\ R_1_1, R_2_1, R_3_1 \\ /// N_1_1, N_2_1, N_3_1 \\ Nclusters_1_1, Nclusters_2_1, Nclusters_3_1 \\ b_1_2, b_2_2, /// b_3_2 \\ se_1_2, se_2_2, se_3_2 \\ R_1_2, R_2_2, R_3_2 \\ N_1_2, N_2_2, N_3_2 \\ /// Nclusters_1_2, Nclusters_2_2, Nclusters_3_2) matrix rownames Main = Mita1 &quot;.&quot; R2 &quot;Number of Observations&quot; &quot;Number of Clusters&quot; Mita2 /// &quot;.&quot; R2 &quot;Number of Observations&quot; &quot;Number of Clusters&quot; matrix colnames Main = &quot;&lt;100 km of Bound.&quot; &quot;&lt;75 km of Bound.&quot; &quot;&lt;50 km of Bound.&quot; esttab matrix(Main, fmt(&quot;3 3 3 0 0 3 3 3 0 0&quot;)), /// model(17) /// varwidth(40) /// not /// refcat(Mita1 &quot;Log Equiv. Household Consumption (2001)&quot; /// Mita2 &quot;Stunted Growth, Children 6-9 (2005)&quot;, nolab) /// coef(. &quot; &quot; Mita1 &quot;Mita&quot; Mita2 &quot;Mita&quot;) nomti /// title(&quot;Table 2: Main Results for replication&quot;) 3.1.6.3 Automatic Output for Table 3 eststo clear scalar drop _all clear matrix local Y &quot;100 75 50&quot; local i=1 foreach Y in `Y&#39; { //Log Consumption use &quot;${dirdata}consumption.dta&quot;, clear ren pothuan_mita row1 //Cubic polynomial in distance to Potosi eststo id`i&#39;: quietly reg lhhequiv row1 dpot dpot2 dpot3 infants children adults /// elv_sh slope bfe4* if (cusco!=1 &amp; d_bnd&lt;`Y&#39;),robust cluster (ubigeo) quietly matrix R_`i&#39; =e(r2) quietly scalar Nclusters_`i&#39; = e(N_clust) quietly scalar N_`i&#39; = e(N) //Cubic polynomial in distance to the mita boundary ren row1 row2 local i = `i&#39;+ 1 eststo id`i&#39;: quietly reg lhhequiv row2 dbnd_sh dbnd_sh2 dbnd_sh3 infants children /// adults elv_sh slope bfe4* if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) quietly matrix R_`i&#39; =e(r2) // Stunted Growth use &quot;${dirdata}height.dta&quot;, clear //Cubic polynomial in distance to Potosi ren pothuan_mita row1 local i = `i&#39;+ 1 eststo id`i&#39;: quietly reg desnu row1 dpot dpot2 dpot3 elv_sh slope bfe4* /// if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) quietly matrix R_`i&#39; =e(r2) quietly scalar Nclusters_`i&#39; = e(N_clust) quietly scalar N_`i&#39; = e(N) //Cubic polyonomial in distance to the mita boundary ren row1 row2 local i = `i&#39;+ 1 eststo id`i&#39;: quietly reg desnu row2 dbnd_sh dbnd_sh2 dbnd_sh3 elv_sh slope bfe4* /// if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) quietly matrix R_`i&#39; =e(r2) local i = `i&#39;+ 1 } use &quot;${dirdata}height.dta&quot;, clear ren pothuan_mita row1 eststo id13: quietly reg desnu row1 dpot dpot2 dpot3 elv_sh slope bfe4* /// if (cusco!=1 &amp; border==1), robust cluster (ubigeo) quietly matrix R_13 =e(r2) quietly scalar Nclusters_13 = e(N_clust) quietly scalar N_13 = e(N) ren row1 row2 eststo id14: quietly reg desnu row2 dbnd_sh dbnd_sh2 dbnd_sh3 elv_sh slope bfe4* /// if (cusco!=1 &amp; border==1), robust cluster (ubigeo) quietly matrix R_14 =e(r2) local i=&quot;1 5 9 3 7 11 13&quot; local j=&quot;2 6 10 4 8 12 14&quot; foreach i in `i&#39; { quietly matrix coln R_`i&#39; = R_1 } foreach j in `j&#39; { quietly matrix coln R_`j&#39; = R_2 } // Append Models local i=&quot;1 5 9 3 7 11 13&quot; local k=1 foreach i in `i&#39; { eststo c`k&#39;: appendmodels id`i&#39; id`++i&#39; quietly estadd local geoc &quot;Yes&quot; quietly estadd local bound &quot;Yes&quot; quietly estadd local obs = (N_`--i&#39;) quietly estadd local clu = (Nclusters_`i&#39;) estadd matrix R2_1=R_`i&#39; estadd matrix R2_2=R_`++i&#39; local k = `k&#39; + 1 } esttab c1 c2 c3 c4 c5 c6 c7, /// keep(row1 row2 R_1 R_2) /// coeflabels(row1 &quot;Mita&quot; row2 &quot;Mita&quot; R_1 &quot;R2&quot; R_2 &quot;R2&quot;) /// cells(b(star fmt(3)) se(par fmt(3)) R2_1(fmt(3)) R2_2(fmt(3))) /// star(* 0.1 ** 0.05 *** 0.01) noobs /// title(&quot;TABLE II LIVING STANDARDS&quot;) /// ml(&quot;&lt;100 km of Bound&quot; &quot;&lt;75 km of Bound&quot; &quot;&lt;50 km of Bound&quot; &quot;&lt;100 km of Bound&quot; /// &quot;&lt;75 km of Bound&quot; &quot;&lt;50 km of Bound&quot; &quot;Border District&quot;) /// model(17) varwidth(30) /// posth(&quot;Sample within: &quot; `&quot;{hline @width}&quot;&#39;) hlinechar(&quot;-&quot;) /// mgroups(&quot;&quot; &quot;Log Equiv. Household Consumption (2001)&quot; /// &quot;Stunted Growth, Children 6-9 (2005)&quot;, pattern(1 1 0 0 1 0 0) span) /// s(geoc bound clu obs, labels(&quot;Geo. controls&quot; /// &quot;Boundary F.E.s&quot; &quot;Clusters&quot; &quot;Observations&quot;)) /// order(row1 R_1 row2 R_2) /// refcat(row1 &quot;Panel B. Cubic Polynomial in Distance to Potosi&quot; row2 /// &quot;Panel C. Cubic Polynomial in Distance to Mita Boundary&quot;, nolabel) nonote /// addnote(&quot;The unit of observation is the household in columns 1–3 and the individual in columns 4–7. Robust standard errors, adjusted for clustering by district, are in parentheses. The dependent variable is log equivalent household consumption (ENAHO (2001)) in columns 1–3, and a dummy equal to 1 if the child has stunted growth and equal to 0 otherwise in columns 4–7 (Ministro de Educación (2005a)). Mita is an indicator equal to 1 if the household&#39;s district contributed to the mita and equal to 0 otherwise (Saignes (1984), Amat y Juniet (1947, pp. 249, 284)). Panel B includes a cubic polynomial in Euclidean distance from the observation&#39;s district capital to Potosí, and panel C includes a cubic polynomial in Euclidean distance to the nearest point on the mita boundary. All regressions include controls for elevation and slope, as well as boundary segment fixed effects (F.E.s). Columns 1–3 include demographic controls for the number of infants, children, and adults in the household. In columns 1 and 4, the sample includes observations whose district capitals are located within 100 km of the mita boundary, and this threshold is reduced to 75 and 50 km in the succeeding columns. Column 7 includes only observations whose districts border the mita boundary. 78% of the observations are in mita districts in column 1, 71% in column 2, 68% in column 3, 78% in column 4, 71% in column 5, 68% in column 6, and 58% in column 7. Coefficients that are significantly different from zero are denoted by the following system: *10%, **5%, and ***1%.&quot;) tex replace 3.1.6.4 Install “x_ols” and “appendmodels” programs x_ols capt prog drop x_ols program define x_ols version 6.0 #delimit ; /*sets `;&#39; as end of line*/ /*FIRST I TAKE INFO. FROM COMMAND LINE AND ORGANIZE IT*/ local varlist &quot;req ex min(1)&quot;; /*must specify at least one variable... all must be existing in memory*/ local options &quot;xreg(int -1) COord(int -1)&quot;; /* # indep. var, dimension of location coordinates*/ parse &quot;`*&#39;&quot;; /*separate options and variables*/ if `xreg&#39;&lt;1{; if `xreg&#39;==-1{; di in red &quot;option xreg() required!!!&quot;; exit 198}; di in red &quot;xreg(`xreg&#39;) is invalid&quot;; exit 198}; if `coord&#39;&lt;1{; if `coord&#39;==-1{; di in red &quot;option coord() required!!!&quot;; exit 198}; di in red &quot;coord(`coord&#39;) is invalid&quot;; exit 198}; /*Separate input variables: coordinates, cutoffs, dependent, regressors*/ parse &quot;`varlist&#39;&quot;, parse(&quot; &quot;); local a=1; while `a&#39;&lt;=`coord&#39;{; tempvar coord`a&#39;; gen `coord`a&#39;&#39;=``a&#39;&#39;; /*get coordinates*/ local a=`a&#39;+1}; local aa=1; while `aa&#39;&lt;=`coord&#39;{; tempvar cut`aa&#39;; gen `cut`aa&#39;&#39;=``a&#39;&#39;; /*get cutoffs*/ local a=`a&#39;+1; local aa=`aa&#39;+1}; tempvar Y; gen `Y&#39;=``a&#39;&#39;; /*get dep variable*/ local depend : word `a&#39; of `varlist&#39;; local a=`a&#39;+1; local b=1; while `b&#39;&lt;=`xreg&#39;{; tempvar X`b&#39;; local ind`b&#39; : word `a&#39; of `varlist&#39;; gen `X`b&#39;&#39;= ``a&#39;&#39;; local a=`a&#39;+1; local b=`b&#39;+1}; /*get indep var(s)...rest of list*/ /*NOW I RUN THE REGRESSION AND COMPUTE THE COV MATRIX*/ quietly{; /*so that steps are not printed on screen*/ /*(1) RUN REGRESSION*/ tempname XX XX_N invXX invN; scalar `invN&#39;=1/_N; if `xreg&#39;==1 {; reg `Y&#39; `X1&#39;, noconstant robust; mat accum `XX&#39;=`X1&#39;,noconstant; mat `XX_N&#39;=`XX&#39;*`invN&#39;; mat `invXX&#39;=inv(`XX_N&#39;)}; /* creates (X&#39;X/N)^(-1)*/ else{; reg `Y&#39; `X1&#39;-`X`xreg&#39;&#39;, noconstant; mat accum `XX&#39;=`X1&#39;-`X`xreg&#39;&#39;,noconstant; mat `XX_N&#39;=`XX&#39;*`invN&#39;; mat `invXX&#39;=inv(`XX_N&#39;)}; /* creates (X&#39;X/N)^(-1)*/ predict epsilon,residuals; /* OLS residuals*/ /*(2) COMPUTE CORRECTED COVARIANCE MATRIX*/ tempname XUUX XUUX1 XUUX2 XUUXt; tempvar XUUk; mat `XUUX&#39;=J(`xreg&#39;,`xreg&#39;,0); gen `XUUk&#39;=0; gen window=1; /*initializes mat.s/var.s to be used*/ local i=1; while `i&#39;&lt;=_N{; /*loop through all observations*/ local d=1; replace window=1; while `d&#39;&lt;=`coord&#39;{; /*loop through coordinates*/ if `i&#39;==1{; gen dis`d&#39;=0}; replace dis`d&#39;=abs(`coord`d&#39;&#39;-`coord`d&#39;&#39;[`i&#39;]); replace window=window*(1-dis`d&#39;/`cut`d&#39;&#39;); replace window=0 if dis`d&#39;&gt;=`cut`d&#39;&#39;; local d=`d&#39;+1}; /*create window*/ capture mat drop `XUUX2&#39;; local k=1; while `k&#39;&lt;=`xreg&#39;{; replace `XUUk&#39;=`X`k&#39;&#39;[`i&#39;]*epsilon*epsilon[`i&#39;]*window; mat vecaccum `XUUX1&#39;=`XUUk&#39; `X1&#39;-`X`xreg&#39;&#39;, noconstant; mat `XUUX2&#39;=nullmat(`XUUX2&#39;) \\ `XUUX1&#39;; local k=`k&#39;+1}; mat `XUUXt&#39;=`XUUX2&#39;&#39;; mat `XUUX1&#39;=`XUUX2&#39;+`XUUXt&#39;; scalar fix=.5; /*to correct for double-counting*/ mat `XUUX1&#39;=`XUUX1&#39;*fix; mat `XUUX&#39;=`XUUX&#39;+`XUUX1&#39;; local i=`i&#39;+1}; mat `XUUX&#39;=`XUUX&#39;*`invN&#39;; }; /*end quietly command*/ tempname V VV; mat `V&#39;=`invXX&#39;*`XUUX&#39;; mat `VV&#39;=`V&#39;*`invXX&#39;; matrix cov_dep=`VV&#39;*`invN&#39;; /*corrected covariance matrix*/ /*THIS PART CREATES AND PRINTS THE OUTPUT TABLE IN STATA*/ local z=1; local v=`a&#39;; di _newline(2) _skip(5) &quot;Results for Cross Sectional OLS corrected for Spatial Dependence&quot;; di _newline _col(35) &quot; number of observations= &quot; _result(1); di &quot; Dependent Variable= &quot; &quot;`depend&#39;&quot;; di _newline &quot;variable&quot; _col(13) &quot;ols estimates&quot; _col(29) &quot;White s.e.&quot; _col(42) &quot;s.e. corrected for spatial dependence&quot;; di &quot;--------&quot; _col(13) &quot;-------------&quot; _col(29) &quot;----------&quot; _col(42) &quot;-------------------------------------&quot;; while `z&#39;&lt;=`xreg&#39;{; tempvar se1`z&#39; se2`z&#39;; local beta`z&#39;=_b[`X`z&#39;&#39;]; local se`z&#39;=_se[`X`z&#39;&#39;]; gen `se1`z&#39;&#39;=cov_dep[`z&#39;,`z&#39;]; gen `se2`z&#39;&#39;=sqrt(`se1`z&#39;&#39;); di &quot;`ind`z&#39;&#39;&quot; _col(13) `beta`z&#39;&#39; _col(29) `se`z&#39;&#39; _col(42) `se2`z&#39;&#39;; scalar con_se`z&#39;=`se2`z&#39;&#39;; // ADDED BY THE REPLICATORS: This line is to // capture Conley S.E. local z=`z&#39;+1}; end appendmodels capt prog drop appendmodels program appendmodels, eclass *! version 1.0.0 14aug2007 Ben Jann // using first equation of model version 8 syntax namelist tempname b V tmp foreach name of local namelist { qui est restore `name&#39; mat `tmp&#39; = e(b) local eq1: coleq `tmp&#39; gettoken eq1 : eq1 mat `tmp&#39; = `tmp&#39;[1,&quot;`eq1&#39;:&quot;] local cons = colnumb(`tmp&#39;,&quot;_cons&quot;) if `cons&#39;&lt;. &amp; `cons&#39;&gt;1 { mat `tmp&#39; = `tmp&#39;[1,1..`cons&#39;-1] } mat `b&#39; = nullmat(`b&#39;) , `tmp&#39; mat `tmp&#39; = e(V) mat `tmp&#39; = `tmp&#39;[&quot;`eq1&#39;:&quot;,&quot;`eq1&#39;:&quot;] if `cons&#39;&lt;. &amp; `cons&#39;&gt;1 { mat `tmp&#39; = `tmp&#39;[1..`cons&#39;-1,1..`cons&#39;-1] } capt confirm matrix `V&#39; if _rc { mat `V&#39; = `tmp&#39; } else { mat `V&#39; = /// ( `V&#39; , J(rowsof(`V&#39;),colsof(`tmp&#39;),0) ) \\ /// ( J(rowsof(`tmp&#39;),colsof(`V&#39;),0) , `tmp&#39; ) } } local names: colfullnames `b&#39; mat coln `V&#39; = `names&#39; mat rown `V&#39; = `names&#39; eret post `b&#39; `V&#39; eret local cmd &quot;whatever&quot; end; exit; Authors: Alejandro Arciniegas Herrera, Marcella De Giovanni, Anselm Rabaté, Kenan Topalovic, students in the Master program in Development Economics and Sustainable Development (2023-2024), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2023 "],["political-fragmentation-and-government-stability-evidence-from-local-governments-in-spain.html", "3.2 Political Fragmentation and Government Stability: Evidence from Local Governments in Spain", " 3.2 Political Fragmentation and Government Stability: Evidence from Local Governments in Spain Carozzi, F., Cipullo, Da. &amp; Repetto, L. (2022). Political Fragmentation and Government Stability: Evidence from Local Governments in Spain. American Economic Journal: Applied Economics, 14 (2): 23-50. https://www.aeaweb.org/articles?id=10.1257/app.20200128 Download Do-File corresponding to the explanations below Download Codebook Download Original Data (from Open ICPSR: click on Data_final.dta and Data_final_allparties.dta) [Link to the full original replication package paper on OSF] Highlights The paper investigates the impact of political fragmentation on government stability. The authors employ a fuzzy Regression Discontinuity Design (RDD) model. It belongs to the the regression discontinuity design broad methodology, which is a quasi experimental method used to estimate the causal effect of an intervention over a threshold or a cutoff point. It enables the comparison of individuals immediately above and below this specific cutoff point to assess the impact of a treatment on a particular outcome. In a fuzzy RDD design the assignment rule determining how units are selected into the treatment is probabilistic. The probability of treatment exhibits a discontinuity at the cutoff, though not as a definitive 0 to 1 jump. In regards to the original replication package, we present here a simplified version of its main elements for an easy replication with only one dataset and one do-file. Through this exercise, on top of standard Stata commands, we present a more in-depth explanation on how to make results’ tables using the esttab command, make more complex graphs with the twoway command and, specially, discover some commands from the rdrobust package, key for RDD analysis in Stata. 3.2.1 Introduction The replication of research papers plays a pivotal role for Economics students. It allows them to learn about the implementation of econometrics methods, and to enhance the use of statistical software programs to further develop empirical works. This document provides a replication exercise for a fuzzy regression discontinuity design (RDD) model that estimates the impact of political fragmentation on government stability. It builds on a paper by Carozzi, Cipullo and Repetto (2022). Specifically, this project aims to provide a comprehensive guide on executing an RDD model in Stata. It details the execution of the model through the use of commands and code, while explaining the fundamental concepts of this methodology in the process. The content is structured in six parts. Section one provides an explanation on RDD methodology and a summary of the paper. Section two briefly explains how to set up your Stata environment to be able to do smoothly run this replication. Section three explains how to obtain and present summary statistics. Section four addresses the empirical strategy, explaining how to plot discontinuity graphs. Section five presents the main results, while showing how to perform RDD regressions and store and present results in tables. Finally, section six discusses the importance of robustness checks and provides explanations on the helpful commands to perform them in the context of the presented methodology. 3.2.2 RDD Methodology and Paper Summary The regression discontinuity design (RDD) is a quasi experimental method used to estimate the causal effect of an intervention over a threshold or a cutoff point. It enables the comparison of individuals immediately above and below this specific cutoff point to assess the impact of a treatment on a particular outcome. The method relies on the assumption that individuals near the cutoff point share similar observed and unobserved characteristics. An RDD may adopt a fuzzy design, wherein the assignment rule determining how units are selected into the treatment is probabilistic. The probability of treatment exhibits a discontinuity at the cutoff, though not as a definitive 0 to 1 jump. The fuzziness in the design derives from factors such as imperfect compliance or manipulation of the selected threshold. In terms of results, an RDD estimates local average treatment effects around the cutoff point, where treatment and comparison individuals are most similar. In the paper we replicate, the authors adopt a fuzzy RDD model in order to study how political fragmentation affects government stability. The data employed is a panel of Spanish municipalities from 1979 to 2014. The time dimension corresponds to each legislature, indexed by the year of the corresponding municipal election (1979 to 2011). The main data sources consist of electoral records, data on individual mayors and mayoral changes, municipal demographics (population, density, etc.), and data on the composition of regional and national governments. Electoral outcomes in municipal elections were obtained from the Ministry of Internal Affairs and residential registry. The sample consists of municipalities with more than 250 inhabitants for a total of 51,000 elections. The final dataset includes 42,259 elections because of additional data restrictions. To obtain causal estimates of the effect of fragmentation – measured as the number of parties in the council – on government stability, they exploit the existence of a 5% vote-share threshold for admission to the local council. This threshold causes parties with vote-shares just below 5% to be excluded from the council, generating exogenous variation in the number of parties with representation. Results show that each additional party with representation in the parliament increases the probability that the incumbent government is unseated by 5 percentage points. 3.2.3 Good Practices Before Starting In this section, we advise on two procedures before starting an empirical analysis in Stata. First, the creation of the toy dataset from the original replication package, for which we also provide a codebook (see header for downloads). Then, the creation of folders, which assures an organized storage of the analysis’ inputs and outputs. Finally, the installation of all packages required to successfully carry out the replication. Toy dataset clear set more off *download datasets Data_final.dta and Data_final_allparties.dta from https://www.openicpsr.org/openicpsr/project/125341/version/V1/view?path=/openicpsr/125341/fcr:versions/V1/PFGS_Replication_package_AEJ/Data/Clean&amp;type=folder global dt &quot;PUT YOUR OWN WORKING DIRECTORY HERE (where you stored the data)&quot; ******************************************************************************* *DATASET FOR DESCRIPTIVE STATISTICS ******************************************************************************* use &quot;$dt\\Data_final.dta&quot;, clear * Drop problematic obs. keep if tag == 0 | tag == 6 *Keep some variables keep id_towns log_surface surface pop* tag* election_year nominated_date3 total_seats CCAA party1 party2 population_scrutiny votes_blank votes_void province nmayors votes_total_muni turnout nparties_seats nparties allvalidvotes log_pop log_surface party_five_percent* abs_majority party_mayor* seats_total mocion_5 egen population_average=mean(population_legal), by(id_towns) replace population_average=population_average/1000 egen number_elections=sum(1), by(id_towns) label var population_average &quot;Mean Population 000s (1979-2014)&quot; label var surface &quot;Surface (in km2)&quot; label var number_elections &quot; # of Elections in sample &quot; gen PP_mayor=(party_mayor1==&quot;PP&quot;| party_mayor1==&quot;AP/PDP/UL&quot;)*100 gen PSOE_mayor=(party_mayor1==&quot;PSOE&quot;)*100 gen IU_mayor=(party_mayor1==&quot;IU&quot;| party_mayor1==&quot;PCE&quot;)*100 gen CIU_mayor=(party_mayor1==&quot;CIU&quot;)*100 gen mocion_5_100=mocion_5*100 replace abs_majority=abs_majority*100 label var mocion_5_100 &quot;Vote of No Confidence (%)&quot; label var nparties &quot;# of Parties Running&quot; label var seats_total &quot;# of Council Seats&quot; label var abs_majority &quot;Single-party Majority (%)&quot; label var nparties_seats &quot;# of Parties in Council&quot; label var PP_mayor &quot;1st Mayor - PP (%)&quot; label var PSOE_mayor &quot;1st Mayor - PSOE (%)&quot; label var IU_mayor &quot;1st Mayor - IU (%)&quot; label var CIU_mayor &quot;1st Mayor - CIU (%)&quot; gen Descrip_sample = 1 label var Descrip_sample &quot;1 if observation corresponds to sample data to replicate the descriptive statistics&quot; keep id_towns election_year population_average surface number_elections nparties nparties_seats seats_total mocion_5_100 abs_majority PP_mayor PSOE_mayor IU_mayor CIU_mayor abs_majority nparties_seats mocion_5 Descrip_sample save &quot;$dt\\Database1.dta&quot;, replace ****************************************************************************** *DATASET FOR FIGURE 3 AND MAIN RESULTS ****************************************************************************** use &quot;$dt\\Data_final_allparties.dta&quot;, clear * Use as r.v. the vote share distance of each party from 5% * replace rv = all_rv replace d = all_d replace rv_d = all_rv_d *Drop problematic obs. keep if tag == 0 | tag == 6 *Label label var mocion_5 &quot;Mayor uns.&quot; *keeping useful variables keep log_pop log_surface election_year seats_total id_towns nparties nparties_seats rv d rv_d mocion_5 gen Mainresults_sample = 1 label var Mainresults_sample &quot;1 if observation corresponds to sample data to replicate the main results and robustness&quot; save &quot;$dt\\Database2.dta&quot;, replace ******************************************************************************* *FINAL REPLICATION DATASET ******************************************************************************* use &quot;$dt\\Database1.dta&quot;, clear append using &quot;$dt\\Database2.dta&quot; save &quot;$dt\\Dataset_CCR.dta&quot;, replace Folder creation Create the appropriate folders and globals to store your datasets and results. A global is a named storage location that can hold a value or a string of text. We use the command “global” to define it and the prefix “$” to access it. clear all set more off global dt &quot;C:\\Users\\Dell\\Desktop\\laurine_stata_files\\CCR&quot; Package Installation To carry out the replication of this paper, the following packages must be installed: ssc install missings // Provides tools and commands for working with missing data. ssc install rdrobust // Provides tools and commands to execute a regression discontinuity // design methodology. ssc install ivreg2 // Extends Stata’s built-in instrumental variables (IV) estimation // capabilities. ssc install estout // Provides additional formatting and exporting options for regression // results. ssc install outreg2 // Provides additional options formatting regression results for // output in tables. ssc install ranktest // Provides additional tests for rank-related issues, such as rank // correlation coefficients. 3.2.4 Descriptive Statistics After setting the Stata environment, we can begin to explore the data characteristics through the computing of descriptive statistics to achieve the replication of Table 1. The dataset provided for this replication exercise contains two samples: The first sample, used to make the descriptive statistics, consists of a town panel by election year. Therefore, each observation corresponds to a town on an specific election year. To access it, either keep at the beginning only the observations for which the sample indicator is equal to 1 or state it as a condition at the end of every estimation code line. We suggest using the first option: keep if Descrip_sample == 1 The second sample, used to produce the main results and robustness tests for the paper, consists of a party dataset, where in each observation a party appears once per town and election year. To access it, either keep at the beginning only the observations for which the sample indicator is equal to 1 or state it as a condition at the end of every estimation code line. We suggest using the first option: keep if Mainresults_sample == 1 Table 1: Descriptive Statistics Now, in order to replicate each panel from Table 1. Descriptive Statistics we first open the dataset and, as stated before, keep only the observations needed for the replication of the descriptive statistics: use &quot;$dt\\Dataset_CCR.dta&quot;, clear keep if Descrip_sample == 1 Panel A. General information We now use the ”preserve” command to create a copy of the dataset in the Stata memory, so that any changes or modifications will only affect this temporary copy. This is done as subsequently the variable “election year” is dropped and the town duplicates too. This leaves us with a new dataset that corresponds to one observation per town and their respective population, surface and number of elections averages measures. preserve drop election_year duplicates drop id_towns, force Now, we compute some summary statistics (mean, standard deviation, minimum, and maximum) using the “tabstat” command and store them in a matrix with the help of the “eststo: estpost” commands. After, we restore the dataset to its initial state before the “preserve” command was initialized. eststo clear //Clear any previously stored matrix eststo panelA: estpost tabstat population_average surface number_elections, /// stat(mean sd min max) columns(statistics) restore Finally, we can replicate the table of Panel A of the Descriptive statistics table using the “esttab” command, which takes stored results from one or more estimation commands and formats them into a table. esttab panelA, cells((mean(fmt(%18.2fc)) sd(fmt(2)) min(fmt(1)) max(fmt(0)))) nomtitles /// nonum collabels(&quot;Mean&quot; &quot;Std. dev.&quot; &quot;Min&quot; &quot;Max&quot;) /// title(&quot;TABLE 1. DESCRIPTIVE STATISTICS&quot;) /// refcat(population_average &quot;A. General information&quot;, nolabel) /// obslast varwidth(35) label /// addnotes(Notes: &quot;Panel A provides descriptives at the municipal level for all municipalities that appear at least once in our sample.&quot;) Explanation of “esttab” syntaxis and options: cells: specifies which results to show. nomtitles: removes the variable names in the columns’ titles. nonum: removes the numbering rows from the columns. collabels: adds custom column labels. title: adds a title for the table. refcat: creates an additional line with descriptions above some certain variables /groups of variables. obslast: puts the number of observations last. varwidth: specifies the width of the variable names displayed in the table. label: shows labels instead of variable names. addnotes: adds footnotes to the table. The same computation of the summary statistics, storage and tabulation process is repeated for the remaining panels. Panel B. Municipal Elections and Local Government eststo panelB: estpost tabstat nparties nparties_seats seats_total mocion_5_100 /// abs_majority PP_mayor PSOE_mayor IU_mayor CIU_mayor , stat(mean sd min max) /// columns(statistics) esttab panelB, cells((mean(fmt(%18.2fc)) sd(fmt(2)) min(fmt(1)) max(fmt(0)) )) nomtitles /// nonum collabels(&quot;Mean&quot; &quot;Std. dev.&quot; &quot;Min&quot; &quot;Max&quot;) /// refcat(nparties &quot;B. Municipal Elections and Local Government&quot;, nolabel) /// obslast varwidth(35) label /// addnotes(Notes: &quot;Panel B provides descriptives on electoral outcomes at the municipality-council level.&quot;) Panel C.1 - Local Government - Stable Mayor eststo panelC1: estpost tabstat abs_majority nparties_seats if mocion_5==0, /// stat(mean sd min max) columns(statistics) esttab panelC1, cells((mean(fmt(%18.2fc)) sd(fmt(2)) min(fmt(1)) max(fmt(0)) )) /// nomtitles nonum collabels(&quot;Mean&quot; &quot;Std. dev.&quot; &quot;Min&quot; &quot;Max&quot;) refcat(abs_majority /// &quot;C1. Local Government - Stable Mayor&quot;, nolabel) obslast varwidth(35) label /// addnotes(Notes: &quot;Panel C splits this sample into councils that approved at least one vote of no confidence during the term (C2), and those that did not(C1)&quot;) Panel C.2 - Local Government - Vote of No Confidence eststo panelC2: estpost tabstat abs_majority nparties_seats if mocion_5==1, /// stat(mean sd min max) columns(statistics) esttab panelC2, cells((mean(fmt(%18.2fc)) sd(fmt(2)) min(fmt(1)) max(fmt(0)) )) /// nomtitles nonum collabels(&quot;Mean&quot; &quot;Std. dev.&quot; &quot;Min&quot; &quot;Max&quot;) /// refcat(abs_majority &quot;C2. Local Government - Vote of No Confidence&quot;, nolabel) /// obslast varwidth(35) label /// addnotes(Notes: &quot;Panel C splits this sample into councils that approved at least one vote of no confidence during the term (C2), and those that did not (C1)&quot;) 3.2.5 Empirical Strategy After having carried out the descriptive statistics analysis, in this section we dive into the empirical strategy of the research paper. First, we describe the model specification, then we provide explanations on how to plot RDD graphs and, finally, we address how to perform the first stage. The paper employs the following main specification: \\[\\begin{equation} \\tag{1} Y_{it} = α_{1} + τ_{1}N_{it} + β_{1}V_{pit} + β_{2}V_{pit}D_{pit} + π_{it} \\end{equation}\\] Yit an indicator equal to 1 if the mayor of municipality i is unseated and replaced by a new mayor during term t – to the measure of fragmentation. This corresponds to the variable “mocion 5” in the dataset. Nit the number of parties with seats in the council. This corresponds to the variable “nparties seats” in the dataset. Vpit the running variable, representing the difference between the vote-share of each party p and 5% in each municipality i and for each term t. This corresponds to the variable “rv” in the dataset. The number of parties N is instrumented with an indicator D equal 1 for a party being above the 5% threshold. This corresponds to the variable “d” in the dataset. First stage estimation: \\[\\begin{equation} \\tag{2} N_{it} = α_{0} + γ_{1}D_{pit} + δ_{1}V_{pit} + δ_{2}V_{pit}D_{pit} + u_{pit} \\end{equation}\\] Figure 1 shows both the first stage and the reduced form graphs. These figures are relevant on RDD analysis, since they provide a visual representation of the existence of a discontinuity at the threshold. Figure 1: The effect of fragmentation on instability - first stage and reduced form (RD graph) In order to replicate these graphs, the mains results and the robustness checks, first we are going to keep only the observations needed. use &quot;$dt\\Dataset_CCR.dta&quot;, clear keep if Mainresults_sample == 1 Figure 1: Top Panel Since this is a fuzzy design, the first panel plots the running variable against the variable that it will instrument. In this case, the running variable against the number of party seats in council. In order to do an RDD plot, it is necessary to establish a bandwidth (to take into account only observations close to the threshold) and if there are many, sort the observations into bins to make them easier to plot. preserve local increment = 0.0025 // This is done to create the bins of the running variable, which // refer to intervals in which the data is grouped or divided. egen rv_bin = cut(rv), at(-0.025(`increment&#39;)0.025) // Create a new variable rv_bin that // categorizes the variable rv into 7 // bins based on the specified range // and increment // Generate x-axis points as the average within the bin: egen nparties_mean=mean(nparties_seats) if nparties_seats != ., by(rv_bin) replace rv_bin = rv_bin+`increment&#39;/2 // Center bins in the midpoint instead of left-end Plotting the First Stage Graph Now we create a two-way graph with linear fit confidence intervals (lfitci) and scatter points (scatter) twoway (lfitci nparties_seats rv if rv&lt;=0 &amp; rv&gt;=-0.025, lwidth(thick) /// fintensity(inten80) lcolor(gs6) alcolor(none) acolor(gs12*0.1) fcolor(gs14) /// alp(dash) ) (lfitci nparties_seats rv if rv&gt;0 &amp; rv&lt;=0.025 , lwidth(thick) /// fintensity(inten80) lcolor(gs6) alcolor(none) acolor(gs12*0.1) fcolor(gs14) /// alp(dash)) (scatter nparties_mean rv_bin, msymbol(O) mlcolor(gs11) /// mfcolor(gs15) msize(medlarge ) xlabel(-.025(.025).025, labsize(large))) /// (scatteri 3 0 4 0, recast(line) ) , ylabel(3(.5)4, labsize(large)) /// ytitle(N. of parties, size(large)) xtitle(Distance to threshold, /// size(large)) legend(off) graph export &quot;$dt/First_stage_025_linear.png&quot;, replace restore Explanation of twoway syntaxis and options: lwidth(thick): specifies the line width as thick. intensity(inten80): sets the intensity of the confidence interval to 80%. lcolor(gs6): sets the line color to grayscale code 6. alcolor(none): specifies no color for the area under the line. **acolor(gs12*0.1): sets the color of the confidence interval area to grayscale code 12 with 10% opacity. fcolor(gs14): sets the color of the fit line to grayscale code 14. alp(dash): specifies a dashed line for the fit. msymbol(O): specifies circular markers for the scatter plot. mlcolor(gs11): sets the line color of the markers to grayscale code 11. mfcolor(gs15): sets the fill color of the markers to grayscale code 15. msize(medlarge): sets the marker size to mediumlarge. xlabel(-.025(.025).025, labsize(large)): specifies custom x-axis labels. scatteri 3 0 4 0 specifies a vertical line segment between the points (0, 3) and (0, 4). recast(line): instructs Stata to recast the scatter plot as a line plot. graph export**: exports the graph in a specified folder and designated format. Figure 1: Bottom Panel For the second stage we repeat the same but for the reduced form, plotting directly the Probability of No confidence Votes against the the running variable (Distance to threshold). preserve local increment = 0.0025 egen rv_bin = cut(rv), at(-0.025(`increment&#39;)0.025) egen mocion_mean=mean(mocion_5) if mocion_5 != ., by(rv_bin) replace rv_bin = rv_bin+`increment&#39;/2 // Reduced form graph twoway /// (lfitci mocion_5 rv if rv&lt;=0 &amp; rv&gt;=-0.025, lwidth(thick) fintensity(inten80) /// lcolor(gs6) alcolor(none) acolor(gs12*0.1) fcolor(gs14) alp(dash) ) /// (lfitci mocion_5 rv if rv&gt;0 &amp; rv&lt;=0.025 , lwidth(thick) fintensity(inten80) /// lcolor(gs6) alcolor(none) acolor(gs12*0.1) fcolor(gs14) alp(dash)) /// (scatter mocion_mean rv_bin, msymbol(O) mlcolor(gs11) mfcolor(gs15) msize(medlarge ) /// xlabel(-.025(.025).025, labsize(large))) (scatteri 0 0 0.06 0, recast(line) ) /// , ylabel(0(.02).06, labsize(large)) ytitle(P(No-confidence vote), size(large)) /// xtitle(Distance to threshold, size(large)) legend(off) graph export &quot;$dt/RF_mocion_025_linear.png&quot;, replace restore 3.2.6 Main Results Now, we address how to perform the 2SLS estimation present the results on tables. Table 2 reports the estimates of the reduced-form and second-stage coefficients, respectively in Panel A and Panel B. Table 2. Panel A: RF and 2SLS estimates - P(no confidence vote) To run only this part, import the dataset again and keep only the observations from the Main Results sample: use &quot;$dt\\Dataset_CCR.dta&quot;, clear keep if Mainresults_sample == 1 Then, create globals for the regression to be able to easily add as controls the surface and population (in logs) of the municipality, include election year and council seat fixed effects, and include the running variable “rv” and its interaction with the indicator D “rv d”. global weights_on = 1 global controls &quot;log_pop log_surface&quot; global fixed_effects &quot;i.election_year i.seats_total&quot; global rv &quot;rv rv_d&quot; Additionally, if we want to run the regression with each observation weighted by the inverse of the number of parties running in the election, as the authors of the paper do, we store the global $weights_on = 1. If this is the case, the next command lines sorts the data by town and the election year and generates a new variable which counts the number of observations within each combination of id_town and election_year, and then creates the weights. if $weights_on == 1 { bys id_towns election_year: gen count = _N g weights = 1/nparties global weights_bw &quot;weights(weights)&quot; global weights_reg &quot;[aw = weights]&quot; } if $weights_on == 0 { global weights_bw &quot;&quot; global weights_reg &quot;&quot; } Panel A: Reduced-Form results Before running the regressions, it is important to establish a bandwidth, which will restrict the sample used based on the distance of the observations to the threshold. We use the command “rdbwselect” for bandwidth selection using the robust regression discontinuity design methods. The option c(0): indicates that the running variable has a discontinuity at zero; p(1): specifies the order of the polynomial used in the local polynomial regression; fuzzy(): indicates the fuzzy regression discontinuity design and specifies the variable to be used as the fuzzy instrument; vce(): specifies that the standard errors should be clustered at the level specified; masspoints(off): specifies that mass points are turned off in the estimation; covs($controls): indicates that additional covariates should be included in the regression. eststo clear rdbwselect mocion_5 rv, c(0) p(1) fuzzy(nparties_seats) vce(cluster id_towns) /// kernel(uni) $weights_bw masspoints(off) covs($controls) local CCT_bw_l = e(h_mserd) // stores the value of the estimated bandwidth Now, we run each regression of our outcome variable on the running variable, the indicator and their interaction, restricting the analysis to the observations where the absolute value of the running variable is less than the bandwidth that was previously computed and stored in “CCT bw l”. //Simple regression. eststo: reg mocion_5 $rv d $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; Additionally, “estadd scalar” is used to add a scalar to the estimation results, and it is assigned the value of the previously calculated and stored bandwidth. Then an scalar is also added with the value of the mean of the variable mocion 5. summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) The addition of controls and fixed effects results in the different columns of the main results, which are stored in a matrix using the “eststo” command. //For this column, we include controls, but not fixed effects. eststo: reg mocion_5 $rv d $controls $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) //For this column, we include fixed effects, but not controls. eststo: reg mocion_5 $rv d $fixed_effects $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, /// cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) //For this column, we include both controls and fixed effects. eststo: reg mocion_5 $rv d $controls $fixed_effects $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, /// cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) We create the first part of the table with the main results using “esttab” (see pag.7) esttab, se(3) nolabel b(3) sfmt(0) keep(d) nostar nonotes /// refcat(d &quot;A. Reduced-form results&quot;, nolabel) /// s(vmean band N, label(&quot;Mean of dep.var.&quot; &quot;Bandwidth&quot; &quot;Obs.&quot; ) /// fmt(3 3 0)) coeflabels(d &quot;Above threshold&quot;) varwidth(35) /// title(&quot;Table 2. Reduced-form and 2SLS Estimates - Fragmentation and Stability&quot;) /// mtitles(&quot;Mayor uns.&quot; &quot;Mayor uns.&quot; &quot;Mayors uns.&quot; &quot;Mayor uns.&quot;) Explanation of the esttab syntax: se(3): displays standard errors instead of t-statistics with 3 decimal points. nolabel: to not use labels instead of variables names. b: displays point estimates. sfmt(): sets format(s) for scalars. keep: keep individual coefficients. nostar: suppress stars in the table footer. nonotes: suppress notes in the table footer. s: specifies the statistics or summary measures you want to include in the output. Table 2. Panel B: 2SLS results - Fragmentation and Stability Notes: Reduced-form and 2SLS estimates of the effect of the number of parties on the probability of unseating the mayor (equation 2). Sample restricted to elections in which the largest party receives less than 40 percent of valid votes. The dependent variable is an indicator taking value 1 if the mayor was unseated by a vote of no confidence during the legislature. Controls and FE are included as indicated in each column. Controls: surface and population (in logs). FE: number of available seats and election year fixed effects. The optimal bandwidth is calculated using the CCT method. Standard errors are clustered at the municipality level. Now, we replicate the different columns of Panel B, this time using an instrumental variable. As in Panel A, the bandwidth selection is done first with the “rdbwselect” command: eststo clear rdbwselect mocion_5 rv, c(0) p(1) fuzzy(nparties_seats) vce(cluster id_towns) /// kernel(uni) $weights_bw masspoints(off) covs($controls) local CCT_bw_l = e(h_mserd) Then, we estimate the second stage of the main specification with the help of the command “ivreg”. The instrument “d” for number of party seats in council is then specified in parenthesis (nparties seats = d). The estimation is also conditional on the running variable’s absolute value being less than the bandwidth that was previously computed and stored in “CCT bw l”. //Simple regression. eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, /// cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) Additional to the scalars with the bandwidth and the mean of the dependant variable, we use “estadd local” to add a local macro labeled as FE and controls with the sign “N” or “Y” to the estimation results. This to add to the output table to indicate whether that estimation contained Fixed Effects and/or controls. quietly estadd local FE &quot;N&quot; quietly estadd local controls &quot;N&quot; //For this column, we include controls, but not fixed effects. eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $controls $weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) quietly estadd local FE &quot;N&quot; quietly estadd local controls &quot;Y&quot; //For this column, we include fixed effects, but not controls. eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $fixed_effects $weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) partial($fixed_effects) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) quietly estadd local FE &quot;Y&quot; quietly estadd local controls &quot;N&quot; //For this column, we include both controls and fixed effects. eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $controls /// $fixed_effects $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) /// partial($fixed_effects) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) quietly estadd local FE &quot;Y&quot; quietly estadd local controls &quot;Y&quot; To replicate the Panel B of Table 3 we use a similar code to the one presented previously to recreate Panel A: esttab, se(3) nolabel b(3) sfmt(0) keep(nparties_seats) nostar nonotes /// refcat(nparties_seats &quot;B. 2SLS results&quot;, nolabel) s(vmean band N FE controls, /// label(&quot;Mean of dep.var.&quot; &quot;Bandwidth&quot; &quot;Obs.&quot; &quot;Fixed Effects&quot; &quot;Controls&quot;) fmt(3 3 0)) /// coeflabels(nparties_seats &quot;N. Parties&quot;) varwidth(35) /// title(&quot;Table 2. Reduced-form and 2SLS Estimates - Fragmentation and Stability&quot;) /// mtitles(&quot;Mayor uns.&quot; &quot;Mayor uns.&quot; &quot;Mayors uns.&quot; &quot;Mayor uns.&quot;) /// addnotes(&quot;Notes: Reduced-form and 2SLS estimates of the effect of the number of parties on the probability of unseating the mayor (equation 2). Sample restricted to elections in which the largest party receives less than 40 percent of valid votes. The dependent variable is an indicator taking value 1 if the mayor was unseated by a vote of no confidence during the legislature. Controls and FE are included as indicated in each column. Controls: surface and population (in logs). FE: number of available seats and election year fixed effects. The optimal bandwidth is calculated using the CCT method.Standard errors are clustered at the municipality level.&quot;) 3.2.7 Robustness Checks In this last section, we explain how to perform two standard robustness checks for the RDD methodology among the five proposed by the paper, this to assess the sensitivity of the results to changes in model specifications or changes in the definition of the sample. By doing so, we ensure that the specific choices made during the analysis do not unduly influence the conclusions. As stated in Table 3, first, we select the municipalities with 17 or more seats in council to verify that the fragmentation effect persists when concentrating solely on the set of compliers, in which the sample is restricted to municipality-election pairs where the 5 percent threshold is likely to be enforceable. (i.e. municipalities with 17 or more seats in the council). Additionally, we present the “global quadratic polynomial” to be able to capture possible nonlinearities in the conditional expectation of the outcome, although it requires us to rely on more observations that are far from the threshold. Table 3. Robustness Checks To run only this part, previously import the dataset again, keep only the observations from the Main Results sample and run again the globals and weights presented in page 14 of this document. A. Total Seats over 17 In the first case, we only keep the sample in which the number of Council seats is greater than 17 to check the robustness. We store both regression results and IV regression results to be shown in different columns (see page 6 for the esttab and eststo explanations). eststo clear preserve keep if seats_total&gt;=17 rdbwselect mocion_5 rv, c(0) p(1) fuzzy(nparties_seats) vce(cluster id_towns) /// kernel(uni) $weights_bw covs($controls) local CCT_bw_l = e(h_mserd) local band1 = round(`CCT_bw_l&#39;,0.001) eststo: reg nparties_seats d $rv $fixed_effects $controls $weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd local band `band1&#39; eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $fixed_effects $controls$weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) restore esttab, se(3) nolabel b(3) sfmt(0) keep(d nparties_seats) nostar nonotes model(35) /// refcat(d &quot;A. Large Councils Only (#seats &gt;= 17)&quot;, nolabel) s(band N, /// label(&quot;Bandwidth&quot; &quot;Obs.&quot;) fmt(3 0)) /// coeflabels(nparties_seats &quot;N. Parties&quot; d &quot;Above threshold&quot;) varwidth(36) /// title(&quot;Table 5. ROBUSTNESS CHECKS&quot;) /// mtitles(&quot;First-Stage&quot; &quot;2SLS Estimate (N. Parties)&quot;) /// addnotes(&quot;Notes: Column 1 shows the first-stage estimate of our instrument when estimating equation 3. Column 2 reports the associated 2SLS estimate of the effect of number of parties on stability obtained by estimating equation 2. Each panel corresponds to a different robustness check: A) estimates obtained restricting the sample to municipalities with 17 or more seats in the council. Standard errors clustered at the municipality level. Bandwidths obtained using the CCT method in all panels.&quot;) Table 3. Robustness Checks E. Global Quadratic Polynomial This check considers a global quadratic polynomial, where the estimates are obtained using a large 5% bandwidth and a quadratic polynomial in the running variable. To do this, first we generate two different variables which are the square of ”rv” (rv 2) and the square of ”rv” interacting with ”d” (rv d 2). Additionally, we define a global to store these values and use them in the regressions. Finally, we define and store a local macro with a 5% bandwidth. eststo clear preserve local CCT_bw_l = 0.05 gen rv_2=rv^2 gen rv_2_d=(rv^2)*d global rv_2 &quot;rv rv_d rv_2 rv_2_d&quot; local band5 = round(`CCT_bw_l&#39;,0.001) eststo: reg nparties_seats d $rv_2 $fixed_effects $controls $weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd local band `band5&#39; eststo: ivreg2 mocion_5 $rv_2 (nparties_seats = d) $fixed_effects /// $controls$weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) restore esttab, se(3) nolabel b(3) sfmt(0) keep(d nparties_seats) nostar nonotes model(35) /// refcat(d &quot;E. Global Quadratic Polynomial&quot;, nolabel) s(band N, /// label(&quot;Bandwidth&quot; &quot;Obs.&quot;) fmt(3 0)) /// coeflabels(nparties_seats &quot;N. Parties&quot; d&quot;Above threshold&quot;) varwidth(36) /// title(&quot;Table 5. ROBUSTNESS CHECKS&quot;) /// mtitles(&quot;First-Stage&quot; &quot;2SLS Estimate (N. Parties)&quot;) /// addnotes(&quot;Notes: Column 1 shows the first-stage estimate of our instrument when estimating equation 3. Column 2 reports the associated 2SLS estimate of the effect of number of parties on stability obtained by estimating equation 2. Each panel corresponds to a different robustness check: E) estimates obtained using a large 5% bandwidth and a quadratic polynomial in the running variable. Standard errors clustered at the municipality level. Bandwidths obtained using the CCT method in all panels.&quot;) Authors: Carolina Arboleda Lenis, Elif C¸anga, Mariana Navarro Torres, Martina Pugliese, students in the Master program in Development Economics and Sustainable Development (2023-2024), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2023 "],["does-the-election-of-a-female-leader-clear-the-way-for-more-women-in-politics.html", "3.3 Does the Election of a Female Leader Clear the Way for More Women in Politics?”", " 3.3 Does the Election of a Female Leader Clear the Way for More Women in Politics?” Thushyanthan Baskaran and Zohal Hessami. 2018. “Does the Election of a Female Leader Clear the Way for More Women in Politics?” American Economic Journal: Economic Policy, 10 (3): 95–121. https://www.aeaweb.org/articles?id=10.1257/pol.20170045 [Download Do-File corresponding to the explanations below] | [Download Codebook corresponding to the explanations below] [Link to the full original replication package paper from OPEN ICPSR] Highlights Baskaran and Hessami (2018) examine whether female council candidates receive more preferential votes when a female mayor has been recently elected into office in Germany. The authors implement a Regression Discontinuity Design (RDD) based on close mixed-gender mayoral elections, leveraging the natural randomness of election outcomes near the margin to identify causal effects. The methodology follows standard sharp RDD practices in economic policy research, using local linear and quadratic regressions with optimal bandwidths, as recommended by Gelman and Imbens (2016). We provide a simplified approach to using the replication package, adressing the deprecated optimal bandwidth methods (CCT and IK) by implementing manual adjustments. The different Stata tricks you will learn: Creating custom Stata commands with ado-files Plotting RDD graphs Understanding and applying the partial() option in the ivreg2 command for robust estimation 3.3.1 Introduction In modern politics women remain underrepresented, even in contexts where progress towards gender equality has been made. In 2016, less than 23% of national parliamentarians worldwide were women. This underrepresentation is not only unfair, but it also has consequences. Evidence shows that women prioritize policies that align with women’s preferences, invest in children, and reduce corruption. This paper focuses on the German political landscape to examine whether female candidates gain voter share after a female mayor has been voted to power in local council elections. Using data from 109,017 candidates in four elections (2001-2016) across the German state of Hesse, the analysis focuses on close mixed-gender mayoral elections to investigate causal impact of having a female mayor on local council voter behavior. Key insights include: Rank improvement effect: Germany`s open-list electoral system allows voters to influence the ranking of candidates on party lists. Candidates can advance from their initial party-assigned rank based on voter support. The paper shows, that in municipalities with a female mayor, female candidates experience larger rank advancements than those in municipalities led by male mayors. Reduction in voter bias: Female leadership leads to a reduction in anti-female biases and reduction of stereotypes. The effect persists across party lines. Spillover to neighboring municipalities: Positive effects of female leadership on subsequent elections extend beyond the municipality the leader is elected. 3.3.2 Identification Strategy 3.3.2.1 Sharp RDD The authors use a Sharp Regression Discontinuity Design (RDD) based on close mixed-gender mayoral elections. This approach compares municipalities where a female candidate narrowly won against a male candidate to those where a male candidate narrowly won against a female candidate. Key Features: Assumption of Local Randomization: Close elections ensure that municipalities near the threshold differ only in the gender of the elected mayor, isolating the causal effect of mayoral gender on subsequent female council candidate outcomes. Running Variable: The margin of victory for the female mayoral candidate. Outcome Variable: Normalized rank improvement of female council candidates. The share of elected women in councils is sometime used as a secondary outcome variable. 3.3.2.2 Use of a Two-Stage Estimation The authors use the Two-Stage Least Squares Stata package (ivreg2) without an instrumental variable to run their regression because it allows to use the partial() option, which requires two stages. In the first stage, the effects of the running variable (margin of victory) and its interaction with the independent variable (margin of victory × female mayor) are “partialed out” by regressing the independent variable and the dependent variable on these controls. This step isolates the variation in both variables that is orthogonal to the controls. The main regression (second stage) is then performed on these residuals, ensuring that the estimated relationship between dependent and independent variable is not confounded by the effects of the control variables. The option does not perform instrumental variable (IV) regression but partials out the effects of the specified controls. The goal is to estimate the causal effect of the independent variable more cleanly, without computing coefficients for the control variables. 3.3.3 Good Practices This section highlights some advisable procedures before starting an empirical analysis in Stata. 3.3.3.1 Folder Creation and Directories Clean your Stata (globals with eret clear, data in memory with clear *, close any opened logfile with capture log close). Create the appropriate folders and globals to store your datasets and results. A global is a named storage location that can hold a value or a string of text. It can be use to group variables or, here, to create pathways in your computer. We use the command global to define it and the prefix $ to access it. capture log close clear * clear matrix eret clear global dir &quot;Your directory here&quot; global dirdata &quot;$dir/datasets&quot; global dirlogs &quot;$dir/logs&quot; global diroutputs &quot;$dir/outputs&quot; log using &quot;${dirlogs}/Replication.log&quot;, replace use &quot;$dirdata\\Dataset_DLS.dta&quot;, clear To use our Dofile, you should: Put your directory in the global dir. Create the following folders in this directory: datasets (where you should put the downloaded dataset “Dataset_DLS”), logs (for the logfile) and outputs. 3.3.3.2 Package Installation To carry out the replication of this paper, the following packages must be installed: ssc install unique // Command to calculate the unique values taken by a variable ssc install rdrobust // Provides tools and commands to execute a regression discontinuity design methodology. ssc install ivreg2 // Extends Stata&#39;s built-in instrumental variables (IV) estimation capabilities. ssc install estout // Provides additional formatting and exporting options for regression results. ssc install outreg2 // Provides additional options formatting regression results for output in tables. ssc install ranktest // Provides tools for rank-based tests. 3.3.3.3 The Dataset Use the clean dataset “Dataset_DLS” provided here: [Download Clean Dataset] Since we replicate the summary statistics, the main graph and the placebo robustness check, we created a dataset appending the two datasets needed from the Replication Package. “main_dataset.dta”: for the main graph and the summary statistics “dataset_with_lagged_rank_improvements.dta”: for the placebo The cleaning, labeling, and renaming processes are detailed in the associated do-file, available here: [Download Dofile for Clean Dataset] In the new original replication package, the main variables (from “main_dataset.dta”) have the same name as the variables used for the placebo test (from “dataset_with_lagged_rank_improvements.dta”). We renamed the variables used in the placebo with the suffix variable_RC (RC for Robustness Check). 3.3.3.4 Check the Structure of the Dataset The commands describe and summarize are always useful to get an understanding of your data. describe summarize 3.3.4 Summary Statistics 3.3.4.1 Table 1 - Summary Statistics for Candidate Characteristics This table provides summary statistics for candidates to council election’s characteristics, for all candidates in panel A and only for female candidates in panel B. It allows to compare the observable characteristics of the candidates for council elections between the whole population (both genders) and the sample for the RDD (female). The following statistics are given: number of observations, mean, standard error, minimum and maximimum. 3.3.4.2 Code for Summary Statistics 3.3.4.2.1 Table 1A : All Candidates In this panel, the statistics are computed for the whole council candidates sample (main dataset variables). This is done with the command summarize or sum for all the characteristics listed (normalized rank improvement, initial list rank… see Codebook_DLS.pdf). The estpost command allows to store the statistics in a matrix, it is part of the estout package we installed previously. Here, the esttab command, from the same package, is used in a simple way. It allows to put the stored results in a table. In the 6. Robustness Check section we will explain more complex options of the command. In this section we will explain the basic options used: using \"$diroutputs/Table1_PanelA.tex\": gives the directory and the name of the table. .txt will give us a classic text file and .tex will give us the Latex code for the table. replace: ensures that the file will be overwritten, if it already exists. fmt(%6.0f): describes the format of the numbers included in the table, which will be numbers with no decimal. estpost sum gewinn_norm listenplatz_norm age non_university_phd university phd /// architect businessmanwoman engineer lawyer civil_administration teacher /// employed selfemployed student retired housewifehusband // Table 1 panel A: esttab using &quot;$diroutputs/Table1_PanelA.tex&quot;, replace style(tab) /// cells(&quot;count(fmt(%6.0f)) mean(fmt(%6.3f)) sd(fmt(%6.3f)) min(fmt(%6.3f)) max(fmt(%6.3f))&quot;) /// collabels(none) label 3.3.4.2.2 Table 1B : Female Candidates In this panel, the statistics are computed only for the women council candidates. Hence, we have to restrain our dataset to this sample. We use the preserve command to create a copy of the dataset in the Stata memory, so that any changes or modifications will only affect this temporary copy. We then restore the dataset to its initial state with the command restore. We proceed similarly to panel A, computing the summary statistics for the same characteristic variables. preserve keep if female == 1 estpost sum gewinn_norm listenplatz_norm age non_university_phd university phd /// architect businessmanwoman engineer lawyer civil_administration teacher /// employed selfemployed student retired housewifehusband // Table 1 panel B: esttab using &quot;$diroutputs/Table1_PanelB.tex&quot;, replace style(tab) /// cells(&quot;count(fmt(%6.0f)) mean(fmt(%6.3f)) sd(fmt(%6.3f)) min(fmt(%6.3f)) max(fmt(%6.3f))&quot;) /// collabels(none) label restore 3.3.5 Main Results 3.3.5.1 Figure 2: Rank Improvement of Female Candidates - RDD Plot This Figure 2 represents a RDD plot. It is a very commun representation of the regression of the dependent variable (here the normalized rank improvement for women candidates in council election) on the running variable (here the margin of victory of the mayor) in a RDD setting. The x-axis represents the margin of victory for female candidates (closely lost the election when negative, below the cutoff and closely won the election when positive, abode the cutoff), while the y-axis shows the normalized rank improvement of female council candidates. The jump at the cutoff (margin of victory = 0) in the rank improvement of female candidates indicates that municipalities with female mayors witness significantly higher rank improvements for female council candidates, supporting the hypothesis that electing female leaders reduces voter bias against women. This RDD plot consists of four key elements: Bins: Represent averaged data points to visualize the raw relationship between the dependent variable and the running variable Regression Line (red line): Represents the fitted relationship between the dependent variable and the running variable Confidence Interval (gray shading): Shows the uncertainty range around the regression line Upper and Lower Limits (thin black lines): Indicate the boundaries of the fitted data for the plot (extremum of the confidence intervals) In the following section, we will break down the steps required to replicate this graph “manually”. In the original replication package, the authors included a very long and complicated ado-file that automates the graph creation. We preferred to simplify the process since the graph is only done once in the paper. 3.3.5.2 Code for RDD Plot - Main Results 3.3.5.2.1 Start the Code To start this part of code, we define the sample we are going to work on. To do so, we use the command preserve to allow us to return to our original dataset if needed. The dataset is then restricted to observations involving female council candidates (female == 1) where a female mayor was either narrowly elected or narrowly defeated, within a margin of 30 percentage points (abs(margin_1)&lt;30). The command tempvar allows us to define the variables we are going to generate in this code as temporary variables. They will be deleted once the RDD plot code is run. The variables have to be used between apostrophes to be recognized. preserve keep if female==1 keep if abs(margin_1)&lt;30 tempvar bin mean x0 s0 se0 x1 s1 se1 ul0 ll0 ul1 ll1 3.3.5.2.2 Bins In a RDD plot, bins represent grouped averages of the dependent variable (normalized rank improvement) within discrete intervals of the running variable (margin of victory). They allow to have a clear view of the relationship of the two variables around the threshold. The command mod() rounds down the running variable margin_1 to the nearest multiple of 3, so it creates one bin every three units of the variable (hence 10 bins per side of the cutoff). The command egen [...], by(bin) computes the average of the normalized rank improvement in each bin. gen `bin&#39; = margin_1- mod(margin_1,3)+3/2 egen `mean&#39; = mean(gewinn_norm), by(`bin&#39;) 3.3.5.2.3 Regression Line and Standard Errors The command lpoly is used for local polynomial smoothing. It estimates the relationship between the dependent variable and the running variable, by performing regressions locally by grid point (see below). It smooths the relationship locally using weighted regressions. Separate regressions are performed for both sides of the RDD plot: negative margin of victory (the female mayor candidate lost) and positive margin of victory (the female mayor candidate won). We will break down the options included in the command to understand what it is used for: bw(20.10): specifies the bandwidth used for smoothing, how far observations are included around each grid point. In the original code, the bandwidth was chosen using the CCT method. Since it is not available in Stata anymore we set manually the bandwidth on Table 2 in the paper. deg(1): fits a local lineal polynomial (degree 1). n(100): evaluates the local regression at 100 gris points. gen(): stores regression results in specified variables: x0/x1: the 100 grid points where the regression is evaluated for each side of the threshold s0/s1: the smoothed predicted values at each grid point ci: computes the confidence intervals for the smoothed values. se(se0/se1): stores the standard errors of the smoothed values (s0/s1) into the se0/se1 variables. kernel(triangle): specifies a triangle kernel to assign higher weights to observations closer to the grid point. The variables for the control group (below the threshold) are designed by the suffix 0 and the variables for the treatment group (above the threshold) are designed by the suffix 1. lpoly gewinn_norm margin_1 if margin_1&lt;0 , bw(20.10) deg(1) n(100) gen(`x0&#39; `s0&#39;) ci se(`se0&#39;) kernel(triangle) lpoly gewinn_norm margin_1 if margin_1&gt;=0 , bw(20.10) deg(1) n(100) gen(`x1&#39; `s1&#39;) ci se(`se1&#39;) kernel(triangle) 3.3.5.2.4 Confidence Intervals and Upper/Lower Limits To compute the 95% confidence intervals for both groups we use a loop allowing to treat first the control group (v=0) and then the treatment group (v=1). Inside the loop the following steps are taken: Computation of the Upper Confidence Limits (ul0 and ul1): Calculated as the smoothed predicted values (s0 and s1 respectively) plus 1.96 times their standard error (se0 and se1). Computation of the Lower Confidence Limits (ll0 and ll1): Calculated as the smoothed predicted values (s0 and s1) minus 1.96 times their standard error (se0 and se1). The intervals between the final variables (ul0 and ll0, ul1 and ll1) represent the confidence intervals for each group. forvalues v=0/1 { gen `ul`v&#39;&#39; = `s`v&#39;&#39; + 1.96*`se`v&#39;&#39; gen `ll`v&#39;&#39; = `s`v&#39;&#39; - 1.96*`se`v&#39;&#39; } 3.3.5.2.5 Generate the RDD Plot The twoway command in Stata creates two-dimensional (X-Y) plots, it allows to combine multiple plot types (scatter, line, area…) in a single graph. We use this to combine all the elements together: rarea: two grey shade areas as confidence intervals between the upper and lower confidence limits: one for the control and one for the treatment group (ul0/ul1, ll0/ll1), precising x0/x1 as grid points where predictions and their confidence intervals are evaluated scatter: plots the bins at their x-axis values and the corresponding means of the dependent variable on the y-axis. line: six lines: three per side of the threshold One connects the smoothed predicted values (s0/s1) for each grid point (x0/x1) in red. Two connect the upper and lower limit values (ul0/ul1 and ll0/ll1) for each grid point (x0/x1) in black. Options for the graph presentation: legend(off): no legend is required ytitle()/xtitle(): title of the y-axis and x-axis ylabel()/xlabel(): labeling of the y-axis and x-axis xline(0): draws a line at the cutoff where the margin of victory is equal to 0. The code for the Figure ends with the export of the graph in the output directory and the command restore to return to the original dataset. twoway /// (rarea `ul0&#39; `ll0&#39; `x0&#39;, bcolor(gs14)) /// (rarea `ul1&#39; `ll1&#39; `x1&#39;, bcolor(gs14)) /// (scatter `mean&#39; `bin&#39;, msymbol(circle) msize(large) mcolor(black)) /// (line `s0&#39; `x0&#39;, lwidth(thick) lcolor(red)) /// (line `ul0&#39; `x0&#39;, lwidth(thin) lcolor(gray)) /// (line `ll0&#39; `x0&#39;, lwidth(thin) lcolor(gray)) /// (line `s1&#39; `x1&#39;, lwidth(thick) lcolor(red)) /// (line `ul1&#39; `x1&#39;, lwidth(thin) lcolor(gray)) /// (line `ll1&#39; `x1&#39;, lwidth(thin) lcolor(gray)), /// legend(off) /// ytitle(&quot;Rank improvement of women&quot;) /// xtitle(&quot;Female mayoral candidate margin of victory (%)&quot;) /// ylabel(-5(2.5)5) /// xlabel(-30(10)30) /// xline(0) graph export &quot;$dirtables/figure2.png&quot;, replace restore 3.3.6 Robustness Check 3.3.6.1 Table A8: Rank Improvement of Female Candidates in Previous Council Elections The authors include a placebo test as one of their robustness checks to help confirm the validity of their results. It consists in intentionally applying the treatment variable (female mayor) to an unrelated scenario or variable where no causal impact is expected. The purpose is to verify whether the results remain significant in these contexts, which would suggest issues with the identification strategy. In this case, the authors applied the treatment variable (female mayor), to the lagged dependent variable: the normalized rank improvement in previous council elections. Since these elections occurred before the mayor’s term in question, the presence of a female mayor should logically have no effect. ![](Resources/tableA8.png) We included this placebo test in our replication because it is a **widely recognized robustness test** and it allows us to explain how the authors used **ado-files** to compute weights and how we adapted these computations to account for the changes in the updated version of Stata, which deprecated certain options used in the original replication package. Additionally, it provides us with the opportnity to explain the authors&#39; strategy for the regressions. 3.3.6.2 Ado-file An ado-file is a script that can be written in another dofile but called in the main one. It contains commands or procedures defined by the user and allows to extend Stata’s functionality by executing custom-written programs as if they were built-in commands. It is written in a kind of loop that is done over each time the command is called for when it has been run before. In the original package, the authors wrote three ado-files: - *bandwidth_and_weights.ado* - *post_ttest.ado* - *rdd_plot.ado* We do not focus on the *post_ttest.ado* ado-file, and we have already simplified the *rdd_plot.ado* ado-file above In the ado-file ***bandwidth_and_weights.ado***, the authors create the **`bandwidth_and_weight`** command which computes the optimal bandwidth using the **`rdrobust`** command. It allows specifying options like the bandwidth selection method (CCT or IK), kernel weight type, and polynomial degree. It then generates three bandwidth values: the optimal bandwidth (**`bw_opt`**), its half and its double for when variants of the CCT method are specified (CCT/2 or CCT*2) (cf. Baskaran and Hessami, 2018). For each bandwidth, the program calculates weights using a formula based on the scaled running variable, ensuring weights are applied only within the bandwidth. The issue with this ado-file is that, it **relied on older bandwidth selection methods, CCT (Calonico, Cattaneo, and Titiunik) and IK (Imbens and Kalyanaraman)**. CCT provided bias-corrected estimates with robust confidence intervals, while IK minimized the mean squared error but lacks bias correction. These methods were widely used but are now deprecated in newer versions of Stata due to the development of more advanced and efficient alternatives, like MSERD (Mean Squared Error Regression Discontinuity), which provide more reliable results. We have tried to call the command specifying for the new method MSERD in **`rdrobust`** but the results differed significantly from the original estimation. For future research using regression discontinuity designs, the **`rdrobust`** command with the MSERD method is a reliable choice. However, for exact replication of this study, the older methods (CCT and IK) or their manual adjustments are necessary to ensure comparability. To address this, we modified the process by including the **ado-file weight computations** directly into our main do-file and we **manually inserted the optimal bandwidth values**, obtained in Table A8, into the specification code to maintain consistency with the original study, deleting the optimized bandwidth computation. The computed weights are kernel weights designed to give more importance to observations closer to the threshold. These weights are influenced by the selected bandwidth, which determines the range within which observations are given non-zero weight. **Below is the ado-file for computing these weights, followed by a breakdown of its components:** ``` {.Stata language=&quot;Stata&quot; numbers=&quot;none&quot;} capture program drop weights program weights syntax [if] [in], var(varlist) bw(real) capture: drop temp1 ind temp2 weight gen temp1 = `var&#39;/ `bw&#39; gen ind = abs(temp1) &lt;= 1 gen temp2 = 1 - (abs(temp1)) gen weight = temp2 * ind end ##### How to Start an Ado-File: The first step in defining a user-written command is to **drop the program** if it has been run before. Then the ado-file starts with the command **`program`**. - **`program`**: creates a program. Here, its name will be **`weights`**. - **`syntax`**: defines the syntax of the new **`weight`** program: - **`[if]`**: to filter observations based on conditions (e.g., if female == 1). - **`[in]`**: to restrict the analysis to specific rows (e.g., in 1/100). - **`var(varlist)`**: to specify the running variables (here the margin of victory) on which we want to compute weights to test different bandwidth. - **`bw(real)`**: to input of the bandwidth we specified manually in the code. - **`capture: drop`**: drops the temporary variables **`temp1`**, **`ind`**, **`temp2`**, and **`weight`** without interrupting the script if they have not been generated in a previous use of the command. ``` {.Stata language=&quot;Stata&quot; numbers=&quot;none&quot;} capture program drop weights program weights syntax [if] [in], var(varlist) bw(real) capture: drop temp1 ind temp2 weight 3.3.6.2.1 Weights Computation within the Program The weight computation involves four steps: temp1: Normalizes the running variable based on the bandwidth called in the syntax of the program. ind: Generates a dummy for the observations within the kernel’s range (between -1 and 1). 3. temp2: Computes weights for each observation based on its distance from the cutoff. 4. weight: Computes non-zero weights for observations within the kernel’s range (when ind=1). It is our final variable. gen temp1 = `var&#39;/ `bw&#39; gen ind = abs(temp1) &lt;= 1 gen temp2 = 1 - (abs(temp1)) gen weight = temp2 * ind 3.3.6.2.2 Finishing an Ado-File: The end command stops the programs (and the ado-file). end 3.3.6.3 Code for Placebo Test - Robustness Check In this part of the code, the variables used are those with the suffix _RC (for Robustness Check) because they are the variables from the second dataset we appended with the main dataset. In this subset database, the dependent variable is the lagged normalized rank improvement for female council elections candidates. Here, we will break down the first specification code and the table code because the other four specifications are very similar except for the size of the bandwidth manually included. 3.3.6.3.1 First Specification 3.3.6.3.1.1 Set the Manual Bandwidth and Compute the Weights Define manually the bandwidth in a global $manual_bw_CCT. In each specification we named the global based on what the method used in the original replication package was (here, CCT). Use the command weights to create weights based on the value of the variable margin_1_RC and the bandwidth that we specified in the global. preserve global manual_bw_CCT = 15.71 weights, var(margin_1_RC) bw($manual_bw_CCT) 3.3.6.3.1.2 Perform the Regression with the Manual Bandwidth and the Weights As outlined in the Identification Strategy part, the authors use the command ivreg2 without an instrument because this command allows the partial() option, which requires two stages. Here is a breakdown of the option outlined in the ivreg2 command: if abs(margin_1_RC) &lt; $manual_bw_CCT: restricts the regression to the sample within the bandwidth manually defined. [pw = weight]: applies the weights computed in the command weights (structures the analysis around the cutoff by giving more weights to the observations around the cutoff (near 0)). r: specifies heteroskedasticity-robust standard errors. cluster(gkz_RC): clusters the standard errors at the municipality level. partial(margin_1_RC inter_1_RC): removes the influence of the specified control variables (margin_1_RC and inter_1_RC) from both the dependent variable (gewinn_norm_RC) and the independent variable of interest (female_mayor_RC). Regress each variable on the controls in a first stage, isolating the variation in the dependent and independent variables that is orthogonal to the controls. Main regression on these residuals, ensuring that the estimated relationship between female_mayor_RC and gewinn_norm_RC is not confounded by the effects of the control variables. The command est store m1 store the results of the regression in Stata memory under the name m1 (first specification.) ivreg2 gewinn_norm_RC female_mayor_RC margin_1_RC inter_1_RC if abs(margin_1_RC)&lt;$manual_bw_CCT [pw = weight], r cluster(gkz_RC) partial(margin_1_RC inter_1_RC) est store m1 3.3.6.3.1.3 Options and Macros for the Final Table For the final table to be homogeneous between the five specifications, the same macros are generated under each specification containing different information based on the specification. The information will be stored in Stata memory within the specification m1. The command estadd is used to store information about the estimation under the form of local macros: bw: the method used to calculate the optimal bandwidth (in this case, we specify that we have implemented a manual bandwidth that corresponds to the CCT method in the paper). degree: the degree used in the regression. bw_length: the bandwidth used for this specification. num_of_election: the number of unique elections (using the unique identifier gkz_jahr_RC) included in the sample for this specification. mean_depvar: the mean of the dependent variable (gewinn_norm_RC) for the sample used in the specification. sd_depvar: the standard deviation of the dependent variable for the sample. estadd local bw &quot;Manual CCT&quot; estadd local degree &quot;Linear&quot; estadd local bw_length $manual_bw_CCT unique gkz_jahr_RC if e(sample) estadd local num_of_elections `&quot;`r(sum)&#39;&quot;&#39; sum gewinn_norm_RC if e(sample) estadd scalar mean_depvar =r(mean) estadd scalar sd_depvar =r(sd) 3.3.6.3.2 Final Table - Integration of all specifications The command esttab displays the results of the five estimations stored previously along with the descriptive statistics and the added information stored in the different macros. We have previously brokem down some of the basic options of this command, so we will go over the new ones here. Options of the esttab command: cells(): defines what to display for each regression: Regression coefficients b with stars for statistically significance (star) formatted to 3 decimal places (fmt(%8.3f)) Standard error se between brackets (par) formatted to 3 decimal places (fmt(%6.3f)) collables(none) and mlabel(,none): deletes the labels and the model names for the columns. keep(): restricts the table to only include the coefficient of the female_mayor_RC variable. varlabels(): labelizes the variable. stats(): adds statistics which had been stored in local macros in the code above, except for the number of observations N and the number of clusters N_clust that are automatically computed during the regression. layout(): controls how the statistics are displayed. The @ allows to place each statistic in one row except for the \"@ (@)\" that places the standard deviation of the dependent variable in brackets next to its mean. fmt(): specifies the format of each statistic. When it begins with a ~ it is a text, when it begins with a number, it is a numeric value and the number it ends with is the number of decimals required. labels(): assigns custom labels to each statistic for the table. esttab m1 m2 m3 m4 m5 using &quot;$dirtables\\tableA8.tex&quot;, replace style(tab) order( ) mlabel(,none) /// cells(b(label(coef.) star fmt(%8.3f) ) se(label((z)) par fmt(%6.3f))) /// collabels(none) /// keep (female_mayor_RC) varlabels(female_mayor_RC &quot;Female Mayor&quot;) /// stats(bw bw_length degree N num_of_elections N_clust mean_depvar sd_depvar , layout( @ @ @ @ @ @ `&quot;&quot;@ (@)&quot;&quot;&#39; ) fmt( %~#s %9.2f %~# %9.0g %9.0g %9.0f %9.2f %9.2f ) /// labels(&quot;Bandwidth type&quot; &quot;Bandwidth size&quot; &quot;Polynomial&quot; &quot;N&quot; &quot;Elections&quot; &quot;Municipalities&quot; &quot;Mean (SD)&quot; )) /// starlevels(* 0.10 ** 0.05 *** 0.01) restore Authors: Quitterie Dumez, Juliana Ludemann, and Lennart Schreiber, students in the Master program in Development Economics and Sustainable Development (2023-2024), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2023 "],["differences-in-discontinuity-design.html", "Chapter 4 Differences in Discontinuity Design ", " Chapter 4 Differences in Discontinuity Design "],["equality-of-opportunity-and-human-capital-accumulation-motivational-effect-of-a-nationwide-scholarship-in-colombia.html", "4.1 Equality of opportunity and human capital accumulation: Motivational effect of a nationwide scholarship in Colombia", " 4.1 Equality of opportunity and human capital accumulation: Motivational effect of a nationwide scholarship in Colombia Rachid Laajaj, Andrés Moya, Fabio Sánchez, Equality of opportunity and human capital accumulation: Motivational effect of a nationwide scholarship in Colombia, Journal of Development Economics, Volume 154, 2022, 102754, ISSN 0304-3878, .https://doi.org/10.1016/j.jdeveco.2021.102754 Download Do-File corresponding to the explanations below Download Codebook Download Original Data (Data>Originals>SPP_ready.dta) [Link to the full original replication package paper from Open ICPSR] Highlights The research question of this paper is the following: How did the opportunity to receive a scholarship influence the performance of low-income students in the national exit and 9th grade exams, and their university enrollment rates? This replication exercise will deal with the quasi-experimental Regression Discontinuity Design (RDD) method, with an eligibility cutoff and a linear function (and its extension in the form of difference-in-discontinuities model). This methodology is applied in this context because the scholarship eligibility is based on an eligibility threshold, which allows the authors to compare similar students and thus, avoid the issue of selection on unobservable. The special feature of this study is the combination of the regression discontinuity design extension, with the quantile regression method. Throughout this replication exercise, we provide a detailed explanation of the original replication package. Through this exercise, you will learn many tricks on Stata: How to present your results in academic tables, by using the packages estout and outreg2 How to use bandwidths in a regression, here the optimal bandwidth from Calonico et al. (2014) using the rdrobust package How to run multiple loops within another loop How to use the grc1leg package to merge different graphs in only one figure, using an online package How to conduct regression discontinuity analysis using the package rddensity, and more specifically to test for manipulation (Cattaneo et al., 2018) 4.1.1 Before we start If you wish to replicate yourself, you will need to download the original dataset SPP_ready.dta from the original replication package. Then, follow these instructions: Set the directory where you have stored the dataset SPP_ready.dta and where you will store your datasets and results: cd &quot;put here your own working directory&quot; *Data Preparation use &quot;SPP_ready.dta&quot;, clear *keep only the variables we need for the replication keep ranking eligible_post eligible non_eligible post sisben sisben_eligible sisben_post sisben_eligible_post area_ciudades area_urbano icfes_padre_nivel1 icfes_padre_nivel2 icfes_padre_nivel3 icfes_madre_nivel1 icfes_madre_nivel2 icfes_madre_nivel3 edad sexo_sisben departamento_cod_* saber_rk_col area_sisben_dnp puntaje_sisben_dnp year *rename the variables rename area_ciudades cities rename area_urbano other_urban rename icfes_padre_nivel1 father_educ_prim rename icfes_padre_nivel2 father_educ_second rename icfes_padre_nivel3 father_educ_high rename icfes_madre_nivel1 mother_educ_prim rename icfes_madre_nivel2 mother_educ_second rename icfes_madre_nivel3 mother_educ_high rename edad age rename sexo_sisben sex rename area_sisben_dnp area_sisben rename puntaje_sisben_dnp score_sisben rename saber_rk_col saber_avg_school *change labels for the variable area_sisben label define new_labels 1 &quot;14 Main Cities&quot; 2 &quot;Urban Rest&quot; 3 &quot;Rural&quot; label values area_sisben new_labels *use a loop to rename several variables forval i = 1/33 { local oldvar &quot;departamento_cod_`i&#39;&quot; local newvar &quot;department_`i&#39;&quot; rename `oldvar&#39; `newvar&#39; } gen status_eligible= 1 if non_eligible == 0 &amp; post ==0 replace status_eligible= 2 if non_eligible == 1 &amp; post ==0 replace status_eligible = 3 if non_eligible == 0 &amp; post ==1 replace status_eligible = 4 if non_eligible ==1 &amp; post ==1 label define status_eligible_labels 1 &quot;Need-based Eligible 2013-2014&quot; 2 &quot;Non-Eligible 2013-2014&quot; 3 &quot;Need-based Eligible 2015&quot; 4 &quot;Non-Eligible 2015&quot; * Apply the labels to the categorical variable label values status_eligible non_eligible_post_labels drop non_eligible label variable score_sisben &quot;Score Sisbén DNP&quot; label variable area_sisben &quot;Area Sisbén DNP&quot; label variable father_educ_prim &quot;Father&#39;s primary education&quot; label variable father_educ_second &quot;Father&#39;s secondary education&quot; label variable father_educ_high &quot;Father&#39;s high school education&quot; label variable mother_educ_prim &quot;Mother&#39;s primary education&quot; label variable mother_educ_second &quot;Mother&#39;s secondary education&quot; label variable mother_educ_high &quot;Mother&#39;s high school education&quot; label variable sex &quot;Sex&quot; label variable cities &quot;14 Main Cities&quot; label variable other_urban &quot;Other cities&quot; label variable ranking &quot;Ranking Saber test&quot; label variable sisben &quot;Distance to the threshold&quot; label variable post &quot;Before or after SPP&quot; label variable eligible &quot;Eligibility&quot; label variable eligible_post &quot;Treated or not&quot; label variable status_eligible &quot;Student&#39;s status&quot; label variable saber_avg_school &quot;Saber average score of middle school&quot; save &quot;dataset_LMS.dta&quot;, replace // save the dataset created in a new file You will also have to install a few packages and set up a global listing the control variables used in the regressions. ** Necessary packages to run the code: ssc install estout, replace ssc install outreg2, replace ssc install rdrobust, replace net install grc1leg.pkg, replace ssc install rddensity, replace **Set a global for all the control variables used in regressions global controls cities other_urban father_educ_prim father_educ_second father_educ_high mother_educ_prim mother_educ_second mother_educ_high age sex department_1-department_32 saber_avg_school Now you can open the dataset: use &quot;dataset_LMS.dta&quot;, clear 4.1.2 Introduction In 2014, a scholarship named Ser Pilo Paga (SPP), meaning “Being a Good Student Pays Off”, was introduced in Colombia in order to allow meriting students coming from low- income households to go to high-quality universities by funding their entire undergraduate education and living expenses. The eligibility for this scholarship is based on two criteria. First, students must come from a household that scored below a certain cutoff on the Sisbén index, which is a socioeconomic index. Second, students must have scored above a given cutoff on the Saber 11, which is the national high school exit exam. Data on the Sisbén scores are available at the Department Nacional de Planeacion, and the information on the Saber 11 test scores is provided by the Instituto Colombiano para el Fomento de la Educación Superior. This paper analyzes the motivational effect of this scholarship on three dimensions: the students’ performances at the Saber 11, the enrollment rates in high-quality universities and the students’ performances at the Saber 9 to test for the ex-ante motivational effect. This replication paper exercise will be focused on the effect of the SPP on the students’ test scores at the Saber 11. The authors used a difference-in-discontinuities model, an extension of the Regression Discontinuity Design (RDD) approach. The latter consists in estimating a local average treatment effect around a given cutoff to compare individuals with similar characteristics and be able to attribute the difference in outcomes to the only effect of the treatment. The difference-in-discontinuities extension allows studying the differences around a discontinuity threshold as in a standard RDD, and at two different time periods. Here, the authors study the discontinuity around the need-based eligibility cutoff, comparing students just below and above the threshold, two and ten months after the introduction of SPP. Together with quantile regression, the authors are able to show a positive and significant effect of the SPP concentrated at the top of the distribution (from the 70th percentile), emphasizing the SPP’s contribution to a reduction in socioeconomic achievement gap between eligible and non-eligible students. Throughout a replication exercise of the article by Laajaj, Moya and Sánchez, the following aims at providing an explanation of the main insights of the Regression Discontinuity Design and Quantile Regression frameworks, as well as some methodological and code explanation to improve your Stata skills. 4.1.3 Main strategies explanation: Regression Discontinuity Design and Quantile Regressions 4.1.3.1 Regression Discontinuity Design The Regression Discontinuity Design is a method that leverages a natural experiment, utilizing a threshold or cutoff point to determine who receives treatment. The core idea is to compare the outcomes of individuals who receive treatment to those who do not by examining both sides of this threshold. The running variable X assigns observations to treatment given the cutoff value C. Here, the RDD takes the form of a standard eligibility cutoff with a linear function. Though, in other RDD settings, you might encounter spatial RDD with a border as threshold (Black, 1999) or Regression Kink Design, with a discontinuity showing a kink in the treatment variable (Card et al., 2015). The key identifying assumption here is that individuals situated just above or below the threshold share similarity in all aspects except for the treatment they are subjected to. This assumption enables researchers to attribute disparities in results solely to the treatment, sidestepping the issue of selection bias. To extrapolate the discontinuity, one needs to set up a window called bandwidths, containing the treated and control units. There are two main designs within RDD: the sharp design and the fuzzy design. In the sharp design, the treatment assignment is a deterministic function of the running variable X. Units are either treated or not, depending on whether they fall on one side or the other of the threshold. In the fuzzy design, the treatment assignment is not strictly determined by the threshold, there can be instances of imperfect compliance. The probability of receiving treatment might jump at the threshold. Some units may not receive treatment even if they were expected to, and vice versa. To identify the treatment effect at the discontinuity, one needs to specify the potential outcomes model as a flexible function of the running variable. The specification of the functions is empirically important, as it needs to be flexible enough to get the functional form of the relationship between the outcome variable and the running variable. In our paper, the authors are using a standard eligibility cutoff, in a sharp design, estimating the change in ranking of the national high school exit exam (Saber 11) after the introduction of a merit- and need-based scholarship in Columbia (SPP). The treatment is a specific cutoff on the Sisbén (the socioeconomic index used by the government to target subsidies and social programs): students coming from low-income households under the cutoff are eligible and those above are not eligible. The authors are using a linear function for the functional form of the relationship between Sisbén and the ranking. The Sisbén cutoff is based on the households’ geographic location: 57,21 for the 14 main cities 56,32 for other urban areas 40,75 for rural areas The running variable here is the distance between the student’s Sisbén score and the need-based eligibility cutoff. Students taken into account in the study are assumed to be similar, the only difference being the threshold and thus the possibility of receiving the scholarship or not ex-post. To isolate the confounding effect of other social programs established prior to SPP sharing the same eligibility cutoffs, the authors use the difference-in-discontinuities extension of the RDD. This method allows the authors to study, around the need-based eligibility cutoff, the motivational effect of SPP by comparing two time periods. The model was introduced by &lt;a href=” https://doi.org/10.1257/app.20150076 target=“_blank”&gt;Grembi et al. (2016). The difference-in-discontinuities estimate compares two cohorts: the 2015 cohort, composed of students who passed the Saber 11 exam in 2015, 10 months after the implementation of SPP, and the 2014 cohort that comprises students who passed the Saber 11 exam only two months after the implementation of SPP. The 2014 cohort is taken as a control group: since SPP was launched for the first time not long before their Saber 11 exam, their motivation and results cannot be influenced by the program itself. However, the program can be considered as a credible signal for 2015 students, hence the comparison in the motivational effect of SPP between eligible and non-eligible students (on both sides of the threshold), two months after SPP implementation (2014 students, post=0) and ten months after (2015 cohort, post=1). 4.1.3.2 Quantile Regression In addition to their RDD specification, the authors are testing for heterogeneous effects of the scholarship program on students with different test scores in Saber 11 using quantile regressions. While an OLS model is minimizing the sum of squared residuals, the quantile regression model minimizes the objective loss function (sum of absolute weighted deviations). Contrary to OLS, there is no explicit solution for this minimization. Thus, it should be done numerically via the simplex method (a method using iterations). This explains why, each time you run a quantile regression code, the Stata software will display several successive iterations in the “Results” window before displaying the final regression result (see image below). They regress the test scores (variable “ranking”) on the scholarship eligibility for students who passed the Saber 11 exam after the introduction of SPP (variables “eligible” and “eligible_post”) and the running variable (variable “sisben”) at the 25th , 50th, 75th and 90th percentiles. This method has the advantage of studying the effect of a treatment on the entire distribution of interest, not only on its conditional mean. In this paper, this method is crucial because the authors find no motivational average effect of the SPP on the Saber 11 test scores, while they find a positive effect starting from the 75th percentile. These results can be explained by the level of the merit-based cutoff. Indeed, as this ranking cutoff is away from the median, students at the bottom of the distribution will feel discouraged and give up as it would require too much effort to try reaching it. 4.1.4 Descriptive statistics - Replication of Table 1 and Figure 4 4.1.4.1 Main insights of descriptive statistics It is essential to run some descriptive statistics at every stage of a research project. Descriptive statistics play a crucial role in an econometric regression framework by providing a comprehensive summary of the data under consideration. By depicting the key insights and trends of the variables of interest, they serve to demonstrate the motivation behind the analysis, and provide justification for the chosen empirical strategy. Good descriptive statistics should also highlight the main results, ensuring the alignment of the research in the intended direction. Descriptive statistics also serve as an initial step in exploring the dataset, verifying the coherence of values and identifying any potential missing data issues. The command summarize in Stata shows you directly the mean, standard deviation, minimum, and maximum of the desired variables. However, descriptive statistics can also take the form of a graph depicting the correlation between two variables of interest, a figure, a map or even non-causal regressions. Nevertheless, whatever the form it adopts, the primary focus should remain on data visualization: descriptive statistics must be clear, and allow us to catch the main information very quickly. 4.1.4.2 Replication of Table 1 - Average rank of eligible and non-eligible students in the Saber 11 Replication of the means for average and quantiles scores using tabstat and the package estout The paper presents different sets of descriptive statistics. The focus is on replicating Table 1, which displays the average rank of eligible and non-eligible students in the Saber 11 test two months after the implementation of SPP (2013-2014 cohort) and ten months after (2015 cohort). It also details the rank of students at the 25th, 50th, 75th, and 90th percentiles within each group. The replication of Table 1 will take place in two steps: first, the replication of rows (1), (2), (4) and (5) of parts A and B of the table and then row (7) from part C. At the end of this part 1 of the replication exercise, you should have the following .txt output of Table 1, parts (A) and (B). The first part of Table 1 (rows (1), (2), (4) and (5) of parts A and B) displays the means for averages scores and for each quantile (25, 50, 75, 90). To replicate it, you will use the tabstat command with the “status_eligible” variable. With the tabstat command, you can select the desired statistics of the “ranking” variable with statistics, in this case the number of observations (N), the mean, the quantiles p25, p50, p75 and p90 (q and p90). To do so, you will first need to save it with the estpost command, with the name of your choice (here: A). The estpost, eststo and esttab commands are available in the estout package. ssc install estout eststo A: estpost tabstat ranking, by(status_eligible) statistics(N mean q p90) columns(statistics) tabstat displays summary statistics (here the number of observations, the mean and the 25th, 50th, 75th and the 90th quantiles). Then, you can export it in the desired format with esttab (here .txt) with the desired features: fmt(1) gives the desired number of decimal places, label gives the name of the labels in the output table, varwidth(30) gives the width of the column containing the variable names. esttab A using &quot;means.txt&quot;, cells(&quot;mean(fmt(1)) p25(fmt(1)) p50(fmt(1)) p75(fmt(1)) p90(fmt(1))&quot;) title(&quot;Table 1. Saber 11 rank of eligible and non-eligible students before and after the motivational effect&quot;) noobs nonumber nonote nostar label replace varwidth(30) Replication of the descriptive diff-in-diff in average and by quantile, using outreg2 To replicate the last part of table 1, row (7) of part C, which is the descriptive diff-in-diff in average then by percentile, you will do it in two steps: Step 1: Run a basic diff-in-diff regression of ranking on the variables “eligible_post”, “eligible” and “post” and export it in excel format reg ranking eligible_post eligible post, vce (robust) Once the regression is estimated, we can use the command outreg2 to report the regression outputs from Stata into Excel, Word, Latex or any other format. It gives you the type of presentation found in academic papers. To use it, you have to install the package outreg2 first. ssc install outreg2 outreg2 using &quot;descr_DiD_.xls&quot;, label /// ctitle(&quot;Average Rank&quot;) replace less (1) Step 2: Run the same regression but in quantiles, using a loop for each quantile, and then export it in an excel format (.xls) and append it to the basic diff-in-diff table you got in Step 1. foreach j in .25 .50 .75 .90 { qreg ranking eligible_post eligible post, q(`j&#39;) vce (robust) outreg2 using &quot;descr_DiD_.xls&quot;, /// ctitle(`j&#39;) append less (1) } At the end of this part 2) of the replication exercise, you should have the following .xls output of Table 1, part (C). The following results can be drawn from the table: First, non-eligible students have in general a better ranking than eligible, revealing the importance of targeting this subpopulation. (Part A and B of table 1). Then, there is a reduction of the gap between eligible and non-eligible students after the implementation of SPP (by calculating the difference between non-eligible’s rank and eligible’s rank). This suggests a potential positive motivational effect of the program on students’ results. With a simple difference-in-differences regression, the authors compute the significance of the gap reduction (part C of table 1). The gap reduction is much more important from the 75th percentile of the distribution, suggesting that even with an effect at the average, the impact of SPP is only significant for students ranked at the top of the distribution. Two main conclusions can be drawn from this table: It provides support for estimating a causal effect of SPP on motivation, by comparing eligible and non-eligible students. It highlights the interest of a quantile regression to study the potential heterogeneous treatment effect at different points of the distribution of ranking. 4.1.4.3 Replication of Figure 4 - Graphical representation of the Regression-in-Discontinuities estimators on average effect, and at each quantile Moreover, RDD is a special case where graphical representations as descriptive statistics are indispensable. Prior to any RDD estimation, it is imperative to conduct a graphical analysis to justify the credibility of the chosen strategy. The graphical representation should demonstrate a notable jump in the outcome and treatment assignment at the threshold, attributable to the treatment. For this purpose, one can plot the relationship between (i) treatment assignment and the running variable, between (ii) the outcome and running variable, (iii) between control variables and running variable, and (iv) the density of the running variable. The latter provides justification for the absence of manipulation of the running variable and is often done as a robustness check (see replication of Figure A2 below for more details). In what follows, you will replicate Figure 4, which plots the relationship between the running variable (Sisbén score, centered at the cutoff, variable “sisben”) and the outcome variable (ranking change, variable “ranking”). It provides a graphical representation of the RDD estimators on average effect, and at the 25th, 50th, 75th, 90th percentile. It shows evidence of the presence of a discontinuity at the threshold, and offers insights for a significant effect from the 75th percentile only. The code is quite imposing and can be discouraging. Let’s not get ourselves intimidated and break the code step by step. Step 1 - Data preparation: defining some scalars and creating bins The scalar x command stores a single numeric value into a scalar named x. Scalars are useful to hold numerical outcomes, constants, or intermediate results of computations without creating new variables. The command scalar drop _all deletes any scalar Stata has in memory, and can be used prior to any new computation. You will first use scalars to set the width of bins, as well as the value of the CERSUM optimal bandwidth (by Calonico et al. 2014) to 3.35. The optimal bandwidth restricts the regression to the subsample of students whose distance between the Sisbén score and the need-based eligibility cutoff is inferior to 3.35. Focus on bandwidths Bandwidths are employed to define a specific data window around the cutoff in order to compare eligible students to non-eligible ones. The authors apply the CERSUM (Coverage Error-Rate Optimal Bandwidths) optimal bandwidth suggested by Calonico et al. (2014). Coverage error is a type of non-sampling error that occurs when there is not a one-to-one correspondence between the target population and the sampling frame from which a sample is drawn. This can lead to violations of the Regression Discontinuity assumptions, and bias estimates calculated using survey data. CERSUM bandwidths are chosen to minimize the probability that the confidence interval for the regression discontinuities treatment effect does not contain the true effect. It thus allows for bias-corrected robust confidence intervals for the average treatment effect at the cutoff. In Stata, the computation of these CERSUM optimal bandwidths can be done using the package rdrobust introduced by Calonico et al. (2014). For more details about bandwidth calculation, you can look at the code in Section 4.1.5. In the context of a RDD graph, bins refer to intervals along the running variable in which observations are grouped for visualization or analysis. Each bin represents a range of values around the threshold. Here, bins of two different sizes are defined. scalar drop _all scalar bw_dif_cersum = 3.3502511 scalar small_binsize = .01 scalar large_binsize = .3 set seed 1984 The set seed command is used to set the seed for the random number generator, ensuring that Stata produces the same sequence of random numbers (here 1984) every time the program is run. Use the keep if command to keep only observations for which the absolute value of the running variable (distance between student’s Sisbén score and the need-based eligibility cutoff) is inferior to 3.35 (scalar bw_dif_cersum), that is keep only the need-based eligible. keep if abs(sisben)&lt;bw_dif_cersum Create a variable “pre” using the command gen, that is the opposite of the variable post. It is equal to 1 for students who passed the Saber 11 exam in 2015 before and didn’t benefit from the SPP. gen pre = (post == 0) You can then generate bins that will appear in the graph. In an RDD graph, observations are often grouped into bins based on their values on the assignment variable. The graph displays the average ranking in the Saber 11 for each bin. gen large_bins = round(sisben + large_binsize/2, large_binsize) - large_binsize/2 if abs(sisben)&lt;bw_dif_cersum gen small_bins = round(sisben,small_binsize) // Each small bin is set to 0.01 around the sisben score for each individual. Step 2 - Prepare the OLS (average) estimation depicted in the graph You will prepare in this section the main regression to be represented in the OLS graph, and the percentiles. Firstly, regress (using the command reg dependant independents, robust) the ranking in Saber score (outcome) according to control variables and passing the Saber exam after the implementation of SPP. You will then have to save the residuals of this regression, creating a variable resid_ols. The predict command always refers to the last regression Stata has in memory. You have to specify after the comma what you want to predict. For clearer and less heavy regressions, don’t forget to define a global with all your controls of interest. This avoids having to rewrite the full list of covariates in every regression. global controls cities other_urban father_educ_prim father_educ_second father_educ_high mother_educ_prim mother_educ_second mother_educ_high age sex department_1-department_32 saber_avg_school /*if not done yet*/ reg ranking ${controls} post, robust predict resid_ols, residuals The next step is to compute local percentiles. This is done in 2 stages: Stage 1: Compute means of the outcome variable and residuals Using a loop, compute means for the variables ranking and resid_ols, within subsets defined by the variables small_bins and large_bins, for two time periods (pre and post). It creates new variables for each combination of time period, variable, and subset. For examples: ranking_ols_pre is the mean of ranking within each small bin, before the introduction of SPP. ranking_ols_r_pre is the mean of the residuals (of the regression of ranking on post) within each small bin, before the introduction of SPP. ranking_ols_r_largeb_post is the mean of the residuals within each bin, before the introduction of SPP. foreach t in pre post { bysort small_bins: egen ranking_ols_`t&#39;= mean(ranking) if `t&#39;==1 bysort small_bins: egen ranking_ols_r_`t&#39;= mean(resid_ols) if `t&#39;==1 bysort large_bins: egen ranking_ols_r_largeb_`t&#39;= mean(resid_ols) if`t&#39;==1 } A loop repeats the same lines of code for different values of a specified variable, here for each value of pre and post. It allows you to save time, by avoiding copying and pasting the exact same code for different variables. Don’t forget to open the bracket at the end of the first line after specifying the variable to repeat the loop on, to return to line to write your code, and to return to another line to close the bracket. The command bysort tells Stata to apply an operation (here creating means) separately for each value taken by the variable small_bins. Please note that to compute means, you have to use the command egen and not gen. Stage 2: Adjust for the constant, so that the average is the same as the data You will calculate means for the variables computed in stage 1 (ranking_ols_r_pre, ranking_ols_r_post, ranking_ols_r_largeb_pre, ranking_ols_r_largeb_post), and then adjust the values of those variables based on the differences between the mean of the original variable (ranking) and the calculated means on residuals. You can notice here that you can include several loops within the first one: foreach t in pre post { foreach x in ranking_ols_r_`t&#39; ranking_ols_r_largeb_`t&#39; { quietly sum`x&#39;if `t&#39; == 1 //This quietly calculates the mean of each variable within each group for the specified time period. scalar m_`x&#39; = r(mean) //Store the previous mean in a scalar (m_`x&#39;) quietly sumranking if `t&#39; == 1 // This calculates the mean of ranking within each group for the specified time period. scalar ols_ranking = r(mean) // Store the previous mean in a scalar (ols_ranking) scalar dify = ols_ranking - m_`x&#39;// This calculates the difference between the mean of the original ranking variable and the mean of the residuals variables. replace `x&#39; = `x&#39; + dify if `t&#39; == 1 // Replace the values of the variables by their adjusted values based on the calculated difference } } Step 3 - Prepare the quantile estimations depicted in the graph With a loop , you can run the same code as in the OLS estimation, on each quantile (foreach q in 25 50 75 90) ; that is regress the ranking in Saber score according to controls and passing the Saber exam after the implementation of SPP ; calculation of local percentiles ; and adjust for the constant. You have to be careful though, when running quantile regression, that Stata is able to compute enough iterations. In quantile regression, the estimation procedure follows a numerical minimization procedure through the simplex method: the algorithm repeats computations to find parameter estimates minimizing the objective loss function. To make sure Stata will compute enough iterations, we suggest you to set the maximum number of iterations to 1000 with the command set maxiter 1000. Step 4 - Collapse data to keep only one observation per bin It is frequent using quantiles that multiple observations take the same value. Instead of counting them as one observation, each of the observations is assigned a weight. For example, if you have three observations taking the same value, counting them as one observation and assigning them a weight of 3 reflects the fact that these three observations contribute to the percentile calculation. One will be used for counting and then weight based on the number of observations used to calculate the percentile. First of all, for predictions, you need to replace each control by its average value: foreach x in cities other_urban father_educ_prim father_educ_second father_educ_high mother_educ_prim mother_educ_second mother_educ_high age sex saber_rk_col department_1 department_2 department_3 department_4 department_5 department_6 department_7 department_8 department_9 department_10 department_11 department_12 department_13 department_14 department_15 department_16 department_17 department_18 department_19 department_20 department_21 department_22 department_23 department_24 department_25 department_26 department_27 department_28 department_29 department_30 department_31 department_32 { quietly sum`x&#39;//This quietly calculates the mean of each control scalar aux = r(mean) //Store the previous results in the scalar &quot;aux&quot;. quietly replace`x&#39; = aux //Replace each control variable by its mean value. } Then you will create binary indicator variables (“one_pre” and “one_post”) that take the value 1 for observations corresponding to the specified time periods (pre or post, that is if t==1) and 0 for all other observations. foreach t in pre post { gen one_`t&#39; = 1 if `t&#39; == 1 } Finally, collapse by bins, in order to keep only one observation per bin, which makes you avoid artificial significance through repetition of observations: collapse ranking_*sisben large_bins eligible_post eligible post sisben_eligible sisben_post sisben_eligible_post ${controls} (count) one_pre one_post, by(small_bins) You should end up with only 675 observations, one for each unique value of small bins. For each of the residuals variables, you then have to generate a variable equal to the difference between their pre and post value. We do so in the OLS average and in the quantile estimations using a loop. This is the code for the OLS loop: foreach x in ranking_ols_r_ ranking_ols_r_largeb_ { gen `x&#39;dif = `x&#39;post - `x&#39;pre } Don’t forget to save the database you just created in a new file. It is important to never delete the original database. You rather want to save your modifications in a separate file. save&quot;bybeans_pre_post_dif.dta&quot;, replace Step 5 - Construct the OLS average graph and its confidence intervals We are getting closer to creating the graph! Once again, let’s decompose into several stages. By the end of these stages, you should have the following .png output of Figure 4: Stage 1: Main regression to display in the graph Use the original dataset to run this regression. reg ranking eligible_post eligible post sisben sisben_eligible sisben_post sisben_eligible_post ${controls} if abs(sisben)&lt;bw_dif_cersum, ro Stage 2: Data preparation for the confidence intervals Here, use the second dataset you created (the one processed by bins). Interacting a variable with post restricts the estimation on the sample of students who passed the Saber 11 exam in 2015, 10 months after the implementation of SPP. The regression in discontinuities design compares treated and untreated students (on both sides of the threshold) before (post=0, 2014 cohort) and after the implementation of SPP (post=1, 2015 cohort). However, in order to represent the discontinuity effect graphically and construct confidence intervals, you have to get rid of this temporal dimension. The setting shifts closely to a standard RDD representation that solely depicts the relation between the running variable determining the eligibility to SPP and the output variable (ranking), independently of the year students passed their Saber 11 exam and benefited from SPP. To do so, the authors suppress the interaction with the post variable. They replace each variable that is interacted with post, by the corresponding simple non-interacted variable. For instance, they attribute the values of the eligible variable to the eligible_post variable. The variable of interest (initially eligible_post - that is whether students are eligible to the scholarship and pass the Saber 11 exam in 2015) is translated to simply being eligible to the scholarship. It now estimates the effect of being eligible to SPP on the ranking in the Saber 11 exam, independently from whether students benefited from SPP two or ten months before writing the Saber 11 exam. replace eligible_post = eligible replace sisben_post = sisben replace sisben_eligible_post = sisben_eligible post, eligible, sisben, and sisben_eligible are now present twice in the dataset, once under their original name, and a second time under their post interaction name (that you just modified the value). You now want to set the values of the original variables to 0 to avoid multicollinearity in the regression. foreach x in post eligible sisben sisben_eligible { replace`x&#39; = 0 } Stage 3: Predicted values of the main regression Using the predict command, you will calculate linear predicted values from the regression above (creating a new variable yh_ols); as well as standard errors of the prediction xb (saving them in a new variable y_stdp_ols) predict yh_ols, xb predict y_stdp_ols, stdp Stage 4: Adjustment for the constant You want here to adjust the values yh_ols variable, so that the average is the same as the q'th percentile. To do so, you need to add to each observation of the *yh_ols* variable the calculated average difference between mean of *ranking_ols_r_dif* and mean of *yh_ols.* Note that with many commands, you can also use thequietly` command when you don’t want Stata to show the results. quietly sum yh_ols // Calculate mean for the variable yh_ols scalar m_yh_ols = r(mean) // Store the previous results in a scaler (m_yh_ols) quietly sum ranking_ols_r_dif // Calculate mean for the variable ranking_ols_r_dif scalar ols_ranking_dif = r(mean) // Store the previous results in a scaler (ols_ranking_dif) scalar dify = ols_ranking_dif - m_yh_ols // Calculate the difference between the mean of ranking_ols_r_dif and the mean of yh_ols and store it in the scalar (dify) replace yh_ols = yh_ols + dify // Adding the calculated difference to each observation Stage 5: Compute confidence intervals Create here two new variables for the lower and upper bound of the confidence intervals, taking missing values for each observation, and then attribute values to the CI bounds, to estimate confidence intervals at the 95% level. For each small and large bin, the value of the higher bound of the confidence interval is equal to: (the coefficient estimates - 1.96 * the standard error estimated). gen ci_h_ols=. gen ci_l_ols=. replace ci_h_ols = yh_ols+1.96*y_stdp_ols replace ci_l_ols=yh_ols-1.96*y_stdp_ols Stage 6: Create the graph! One particularity of this RDD design is the linear fit on each side of the threshold. Within this framework, you want to attribute the values of the variable small_bins to sisben. replace sisben = small_bins Using the twoway command, you will create a scatter plot, displaying the relationship between ranking in the Saber 11 and the Sisbén score at the cutoff. In the command scatter (and within the same brackets) you can specify options after a comma to define the symbol to employ and its color (msymbol and mcolor). You can also add lines in your scatter plot, using the command line, and specify options after a comma to define the style, pattern and color of the line (pstyle, lpattern and lcolor). For instance, here, we add a line plot of ci_l_ols against small_bins for cases where sisben is inferior to 0. The line has point-style (p3 p3), a dashed line pattern, is sorted, and has green color. Once you have indicated all elements to draw in the twoway graph, add a comma and write the options of the graph: its title, y and x-axis titles, legend settings, and design options (style and color of the graph region, background color of the graph). twoway(scatter ranking_ols_r_largeb_dif large_binsif large_bins == small_bins, msymbol(O) mcolor(gray)) (line yh_ols small_bins if sisben &lt;0, pstyle(p) sort lcolor(blue)) (line ci_l_ols small_bins if sisben &lt;0, pstyle(p3 p3) lpattern(dash) sort lcolor(green)) (line ci_h_ols small_bins if sisben &lt;0, pstyle(p3 p3) lpattern(dash) sort lcolor(green)) (line yh_ols small_bins if sisben &gt;0, pstyle (p) sort lcolor(blue)) (line ci_l_ols small_bins if sisben &gt;0, pstyle (p3 p3) lpattern(dash) sort lcolor(green)) (line ci_h_ols small_bins if sisben &gt;0, pstyle (p3 p3) lpattern(dash) sort lcolor(green)), ytitle(&quot;Ranking change in average&quot; &quot; &quot;) xtitle(&quot;Eligible Not Eligible&quot; &quot; &quot; &quot;{it:Sisbén} score (centered at cutoff)&quot;) legend(label(1 &quot;Change by bin&quot;) label(2 &quot;Dif in RD linear prediction&quot;) label(3 &quot;95% CI of linear prediction&quot;) order(2 1 3)) title(Average Effect) graphregion(style(none) color(gs16)) bgcolor(white) xline(0, lcolor(red)) name(DifRD_ols, replace) Step 7: Export the graph into the format of your choice, here .png. You just replicated one of the elements of Figure 4! graph export &quot;DIf_RD_Fig_Lin_ols.png&quot;, replace Step 6 - Construct the quantile graphs and their confidence intervals We redo the same procedure as in the OLS estimation (steps 1 to 7), with a loop to get one graph for each quantile (download our do-file in section Highlights for details). At this stage of the replication exercise, you should have four .png outputs of Figure 4, one for each quantile (as the output below for the 25th percentile). Step 7 - Merge the five graphs into one figure using the grc1leg package To be able to use the grc1leg command, you probably have to install the package. However, the package is not provided by the Sata package library. You have to import it directly from its author, Vince Wiggins (2010). net install grc1leg, from (http://www.stata.com/users/vwiggins) You will first merge the three first graphs on a single-row graph named row1, then the two remaining ones on another single-row graph named row2. We finally merge row1 and row2 in a single one column graph. grc1leg DifRD_ols DifRD_25 DifRD_50, rows(1) name(row_1, replace) graphregion(color(white)) grc1leg DifRD_75 DifRD_90, rows(1) name(row_2, replace) graphregion(color(white)) grc1leg row_1 row_2, cols(1) graphregion(color(white)) graph export&quot;figure_4.png&quot;, replace At the end of this part of this replication exercise, you should have the following .png output of Figure 4: 4.1.5 Main results - Replication of Table 3 In their main results, the model they estimate is the following (based on Grembi et al, 2016): \\[\\begin{equation} \\tag{1} Ranking_{it} = β_{0} + β_{1} eligible\\_post_{it} + β_{2} eligible_{i} + β_{3} sisben_{i} + β_{4} sisben\\_eligible_{i} + β_{5} sisben\\_post_{it} + β_{6} sisben\\_eligible\\_post_{it} + X_{it} α + ϵ_{it} \\end{equation}\\] where \\(Ranking_{it}\\) is the dependent variable, the rank (from 0 to 100) on the Saber 11 exam of student \\(i\\) who passed the test in year \\(t\\), \\(β_{1}\\) is the difference-in-discontinuities estimate (our coefficient of interest, the change in discontinuity in test scores before and after the introduction of SPP), and \\(sisben_{i}\\) is the running variable. Table 3 summarizes the main results of the paper from the above equation. The first column reports the results estimated with the regression-in-discontinuities specification (mean effect) while the columns 3 to 5 report the results obtained with quantile regressions. At the end of this part of this replication exercise, you should have the following .xls output of Table 3: Step 1 - Bandwidths calculation Before running this regression, it is necessary to define a bandwidth around the cutoff to compare eligible students to non-eligible ones. Focus on bandwidths - The authors apply the CERSUM (Coverage Error-Rate Optimal Bandwidths) optimal bandwidth suggested by Calonico et al. (2014). Coverage error is a type of non-sampling error that occurs when there is not a one-to-one correspondence between the target population and the sampling frame from which a sample is drawn. This can lead to violations of the Regression Discontinuity assumptions, and bias estimates calculated using survey data. CERSUM bandwidths are chosen to minimize the probability that the confidence interval for the regression discontinuities treatment effect does not contain the true effect. It thus allows for bias-corrected robust confidence intervals for the average treatment effect at the cutoff. In our case, the optimal bandwidth restricts the regression to the subsample of students whose distance between the Sisbén score and the need-based eligibility cutoff is inferior to 3.35. In Stata, the computation of these CERSUM optimal bandwidths can be done using the package rdrobust introduced by Calonico et al.(2014). First you will have to install the package rdrobust. ssc install rdrobust Then, to get the CERSUM optimal bandwidths, you will: Calculate it for pre-period (year 2013 and 2014, post=0) with the regression discontinuity analysis command rdrobust, using the uniform kernel function: *pre cersum: rdrobust ranking sisben if post == 0, kernel(uniform) bwselect(cersum) Create a scalar variable and assign it to the value of the estimated bandwidth stored in e(h_l) scalar bw_pre_cersum = e(h_l) Do the same for the post-period (post=1) *post cersum: rdrobust ranking sisben if post == 1, kernel(uniform) bwselect(cersum) scalar bw_post_cersum = e(h_l) Create a scalar variable and assign it to the mean of both above calculated bandwidths. scalar bw_dif_cersum = (bw_pre_cersum +bw_post_cersum)/2 To see the resulting bandwidths estimation, use scalar list _all Once you have done this computation a first time and obtained the optimal bandwidth value (3.35 in our case), you can simply store this value in a scalar for simplicity. This avoids you redoing the complete computation every time you want to use it. scalar bw_dif_cersum = 3.3502511 Then, set a global for the control variables required in this regression. global controls cities other_urban father_educ_prim father_educ_second father_educ_high mother_educ_prim mother_educ_second mother_educ_high age sex department_1-department_32 saber_rk_col Step 2 - Regression at the eligibility cutoff and saving the results with outreg2 command You will regress, at the eligibility cutoff, the ranking in the Saber 11 (variable ranking) on variables for whether students are eligible to the scholarship and pass the Saber 11 exam in 2015 (variable eligible_post), on the running variable sisben, as well as on control variables. The coefficient of interest (variable eligible_post) gives the change in discontinuity in test scores before and after the introduction of SPP. To do so, you will use the command reg and add the condition which states that the absolute value of the variable sisben must be inferior to the optimal bandwidth, as explained above. reg ranking eligible_post eligible post sisben sisben_eligible sisben_post sisben_eligible_post ${controls} if abs(sisben)&lt;bw_dif_cersum, robust Once the regression is estimated, use the command outreg2 to report the regression outputs from Stata into Excel, Word, Latex or any other format. It gives you the type of presentation found in academic papers. To use it, you have to install the package outreg2 first. ssc install outreg2 outreg2 using &quot;DIF_RD_main.xls&quot;, addtext(Quantiles,&quot;OLS Dif RD&quot;) ctitle(&quot;LATE&quot;) less(1) keep(eligible_post) nocons replace The option less() specifies how many less significant digits to be displayed for the auxiliary (non-coefficient) statistics. Step 3 - Quantile regressions with qreg command For the quantile regressions, use the command qreg to run the regression, as before, the dependent ranking on variables of interest, the running variable, and on control variables. Though, in this case, the regression is performed with a loop foreach to repeat it for each quantile. Once again, you will have to specify the condition of eligibility and use a robust estimator of variance vce(r). To display the results for each quantile regression in an academic table, you can once again use the outreg2 command in the loop. However, before running the regression, you will have to set the maximum number of iterations to 1,000. Indeed, in quantile regression, the estimation procedure follows a numerical minimization procedure through the simplex method: the algorithm repeats computations (here set at 1000 repetitions) to find parameter estimates minimizing the loss objective function. set maxiter 1000 foreach q in 25 50 75 90 { qreg ranking eligible_post eligible post sisben sisben_eligible sisben_post sisben_eligible_post ${controls}if abs(sisben)&lt;bw_dif_cersum , q(`q&#39;) vce(robust) outreg2 using &quot;DIF_RD_main.xls&quot;, addtext(Quantiles,&quot;`q&#39; Dif RD&quot;) ctitle(&quot;`q&#39;&quot;) less(1) keep(eligible_post) nocons append } How to read Table 3? Table 3 above shows that the introduction of the scholarship Ser Pilo Paga has non-significant effect on the Saber 11 test scores at the mean, as well as at the 25th and 50th percentiles. However, looking at heterogeneity in the ranking, there are positive and statistically significant effects for the 75th and 90th percentiles, respectively. In other words, a significant gap reduction in test scores between eligible and non-eligible students can be observed at the top of the distribution of the ranking in test scores. The effect of the scholarship on the Saber 11 ranking is around 1.5 rank for eligible students. These findings are in line with the Figure 4 replicated just above. 4.1.6 Robustness check - Replication of Figure A2 An important and interesting robustness check to conduct in RDD is to study the density of the running variable. This allows checking whether there are too many observations on one side of the cut-off compared to the other side. In other words, you want to verify whether individuals have manipulated the running variable in order to benefit from the program provided by the policy in place. McCrary (2008) was the first to introduce the idea of manipulation testing in the context of regression discontinuity (RD) designs. In our case, the authors want to demonstrate that the Sisbén score has not been manipulated around the eligibility cutoff. To this end, they will test for discontinuity in observation densities around the cutoff. A cluster below the cutoff would suggest that households have manipulated their Sisbén score in order to qualify for the program, creating a selection effect and posing a serious problem in terms of unobservable differences between students on either side of the cutoff. This would make for a biased comparison between the two groups. To test for that, you will replicate Figure A2 below. It should show no significant discontinuity in the Sisbén score density, when examining the three types of geographical areas taken into account by the Colombian statistics office: the 14 main cities, the other urban areas and the rural areas (each with a different cutoff). 4.1.6.1 Test for manipulation around the eligibility cutoff using the rddensity package To test for manipulation around the eligibility cutoff, the authors are using the local polynomial density estimation technique introduced by Cattaneo et al. (2017). To start, you will first have to find the p-value using the Cattaneo et al.’s method. For that, you will need to install the package rddensity. If the p-value is lower than 0.05, it suggests that there has been some manipulation. ssc install rddensity Since the year of interest for manipulation is 2015, set keep if year == 2015. Then, run a regression discontinuity analysis, specifying a first-order polynomial (p(1)) around the cutoff point in the sisben variable for each of the sisben area. For example, to take into account the 14 largest Colombian cities, you have to set the variable area_sisben=1. rddensity sisben if area_sisben==1, p(1) You should see that these results are robust for the three types of geographical areas taken into account by the Colombian statistics office: the 14 main cities, the other urban areas and the rural areas (each with a different cutoff). p-value for 14 main cities: 0.3183 p-value for other urban areas: 0.7747 p-value for rural areas: 0.1285 Indeed, all the three p-values are higher than 0.05. Compared to the McCrary density test (2008) usually used so far, the technique of Cattaneo et al. (2017) avoids pre-binning the data and is constructed intuitively based on easy-to-interpret kernel functions. 4.1.6.2 Replication of the chart for the 14 major cities, then for urban and rural using twoway To plot the discontinuity, use the command twoway, which is a basic command for graphs fitting Y on X and use the histogram function for the variable score_sisben. At this step, you may: - Choose the color of your histogram (here black (fcolor(black)) with gray outlines (bcolor(gray))), - Place a red line to place the cutoff of the 14 main Colombian cities using the command scatteri, - Add the p-value you got in the part A for each type of area (here for the 14 main cities 0.3183) at the lop left of the graph, - Give the respective names Sisbén - Socioeconomic Index and Density to the X and Y axes and finally, - Export the graph as .png file format ```{.Stata language=“Stata” numbers=“none”} Replicate first graph of Figure A1 twoway(histogram score_sisben if area_sisben==1, fcolor(black) bcolor(gray)) /// (scatteri 0 56.32 0.025 56.32 (9), c(l) m(i) color(red)), /// plots a single red line at coordinates (0.025, 56.32) text(0.025 57.21 “14 Cities”, place(e) size(medium)) /// adds the text for the line text(0.025 0 “P-Val=.3183”, place(e) size(medium)) /// adds the p-value at the top left of the graph legend(off) xtitle(“{it:Sisbén} - Socioeconomic Index”) ylabel(0(.01).025) /// no legend, name of the var on the x- and y-axis ytitle(“Density”) graphregion(style(none) color(gs16)) name(“histogram_14cities”, replace) // title graph export “histogram_14cities.png”, replace // Exports the graph as a PNG file named “histogram_14cities1.png” to the directory specified #### Combination and exportation of the three graphs together ```{.Stata language=&quot;Stata&quot; numbers=&quot;none&quot;} graph combine histogram_14cities histogram_urban histogram_rural, /// rows(3) graphregion(style(none) color(gs16)) /// imargin(medsmall) xcommon ycommon graph export&quot;figure_A2.png&quot;, replace Authors: Angélique FANTOU, Agathe LOYER, Emma VERHILLE, students in the Master program in Development Economics and Sustainable Development (2023-2024), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2023 "],["differences-in-differences.html", "Chapter 5 Differences-in-Differences ", " Chapter 5 Differences-in-Differences "],["the-long-term-effects-of-measles-vaccination-on-earnings-and-employment.html", "5.1 The Long-Term Effects of Measles Vaccination on Earnings and Employment", " 5.1 The Long-Term Effects of Measles Vaccination on Earnings and Employment Atwood, Alicia. 2022. “The Long-Term Effects of Measles Vaccination on Earnings and Employment.” American Economic Journal: Economic Policy, 14 (2): 34–60. https://doi.org/10.1257/pol.20190509 Note: We used three datasets for our replication, as the author did since merging them was impossible. They are created and cleaned by the master do-file “0_master_do_cleaning.do”, which calls the three cleaning do-files for each dataset (“1_do_cleaning_inc_rate_ES.do”, “2_do_cleaning_results.do”, “3_do_cleaning_placebo_test.do”) [Download 0_master_do_cleaning.do] [Download 1_do_cleaning_inc_rate_ES.do] [Download 2_do_cleaning_main_results.do] [Download 3_do_cleaning_placebo_test.do] We then use these three datasets (“inc_rate_ES.dta”, “main_dataset_acs_200017.dta”, “placebo_dataset_acs196070.dta”) in the “4_dofile_results_LASCPE.do” to compute the tables and figure : [Download Do-File 4_dofile_results_LASCPE.do] This is the codebook: [Download Codebook] And the link to the replication package: [Link to the full original replication package paper from Open ICPSR] Highlights The article investigates the long-term effects of the measles vaccine on earnings and employment in the United States. Specifically, it examines how childhood exposure to the vaccine impacts adult labor market outcomes such as income, poverty, and employment rates. It employs a staggered difference-in-differences (DiD) identification strategy. This approach exploits variation in pre-vaccination measles incidence rates across states and the staggered introduction of the vaccine, determined by birth cohorts and state-level differences in exposure timing. The standard difference-in-differences method is commonly used in public health economics, as it captures the long-term benefits of improved health, such as better economic outcomes or reduction in the burden of disease. This staggered DiD approach incorporate continuous treatment intensity, as pioneered by Bleakley (2007). It exploits geographic variation in pre-vaccination conditions, improving causal inference and capturing differential impacts across states and cohorts, in order to offer a more nuanced analysis of the vaccine’s long-term economic impacts. Our added value relies mainly in the detailed explanation of the code for the original replication package, the creation of the code for Table 2 (as the authors do not provide it), and a discussion of new two-way fixed-effect estimators. Stata tricks you will learn through this replication: How to generate an event-study graph with scatter plot. How to use a foreach command in order to loop over a list of dependent variables and store the corresponding results. How to create comprehensive results table with esttab. 5.1.1 Introduction 5.1.1.1 Paper Summary In this paper, the author uses a staggered Difference-in-Differences (DiD) approach with continuous treatment to estimate the long-term impacts of measles vaccination on adult income and employment in the United States. The introduction of the measles vaccine in 1963 serves as a plausibly exogenous shock to childhood health. Leveraging geographic and temporal variations, the study measures exposure intensity through pre-vaccination measles incidence rates in an individual’s birth state. Atwood (2022)’s paper reveals significant long-term economic benefits of measles vaccination on adult labor market outcomes, showing its crucial impact on individual and aggregate well-being: Adults fully exposed to the vaccine, born in U.S. states with average pre-vaccine measles incidence rates, experience an increase in their annual income of $447 or 1.1% above the pre-vaccine cohort average. This income effect is even larger for those with non-zero incomes. Beyond income, the vaccine reduces the likelihood of living in poverty by 0.5 percentage points. Employment likelihood also increases modestly by 0.29 percentage points, representing a 0.3% increase from the pre-vaccine cohort mean employment rate. Notably, these gains can be attributed to productivity improvements rather than increased working hours, as average weekly hours slightly decline by 0.19 hours, a small but statistically significant result. On a national scale, these benefits translate into an estimated $76.4 billion annual increase in personal income in the U.S. (in 2019 USD), equivalent to 0.4% of total personal income. Overall, these findings emphasize the economic potential of early childhood health interventions, i.e. vaccination, in reducing socio-economic inequalities. Robustness checks further validate the findings, demonstrating consistency across alternative sample restrictions and income segmentations. 5.1.1.2 Data Data on Measles incidence rate. The data include annual state-level incidence rates for the population under 18, using (i) population estimates from Current Population Reports (CPS 1952–1968) and (ii) state-level case counts of particular diseases in the US published in the CDC’s Morbidity and Mortality Weekly Reports Annual Supplement (MMWR) from 1952 to 1975. These records cover measles, mumps, rubella, pertussis, and chicken pox, allowing analysis of disease trends pre- and post-vaccine availability. The measles vaccine was introduced in 1963. This dataset enables comparisons of health outcomes over time, highlighting the transformative effects of vaccination programs. Data on Labor market outcomes. To measure labor market and income outcomes of individuals that were exposed to the disease and to the vaccine, the author uses the ACS (American Community Survey) database between 2000 and 2017, for individuals aged from 25 to 60 at the time of the survey. The sample is restricted to individuals born in the US, and for who it is possible to retrieve their state of birth. This allows the author to measure the exposure to measles and to the vaccine of individuals when they were children, avoiding issues related to selective migration. 5.1.1.3 Methodology The Difference-in-Differences (DiD) method is a widely used econometric tool for estimating the effect of a treatment, such as a policy or program, by comparing outcome changes over time between a treatment group and a control group. In its classic form, DiD compares outcomes before and after a single intervention point for both groups. However, when treatment does not occur simultaneously across all units but is introduced at different times, a staggered Difference-in-Differences framework is employed. This framework accounts for the sequential rollout of interventions, allowing for more flexible modeling and increasing the number of possible comparisons. The staggered DiD method exploits the varying timing of treatment adoption by comparing each treated group to others that have not yet received the treatment. This approach improves the reliability of causal estimates by incorporating temporal and spatial variation. Like the standard DiD method, it relies on the parallel trends assumption: outcomes for treated and untreated groups are assumed to follow the same trajectory in the absence of treatment. In the staggered case, this assumption applies to each group that receives treatment over time relative to others yet untreated. In this paper, the author employs the staggered DiD framework to study the long-term effects of the introduction of the measles vaccine in 1963. This analysis draws on Bleakley (2007)’s methodology, which builds on the staggered approach by incorporating continuous treatment intensity and exploiting geographic variation in pre-intervention conditions to improve causal inference. Specifically, the study uses pre-vaccination measles incidence rates across states to measure treatment intensity and combines them with a staggered implementation of the vaccine across birth cohorts and states. By interacting state-level pre-vaccine measles incidence with birth-year-specific vaccine exposure, the author captures both the geographic and temporal dimensions of treatment variation. This continuous treatment intensity measure enhances the analysis, allowing for a detailed exploration of the vaccine’s long-term economic impacts. Bleakley (2007)’s approach is supposed to provide robust causal estimates by addressing confounders such as mean reversion and pre-existing trends. 5.1.1.4 Empirical Strategy Aiming to isolate the causal impact of improved childhood health on adult economic outcomes, the empirical model draws upon staggered vaccine exposure, due to birth cohorts and state-level variations in pre-vaccine disease rates: \\[ Y_{isct} = \\beta \\left(M_{1952-1963}^{pre} \\times \\text{Exposure to Vaccine}_{sc}\\right) + \\delta_s + \\gamma_c + \\alpha_t + \\theta X_{isct} + \\epsilon_{isct} \\] focuses on measuring long-term effects of vaccine exposure on labor market outcomes, such as earnings or employment, for individual \\(i\\) born in state \\(s\\), from cohort \\(c\\), and observed at year \\(t\\). The key term \\(\\left(M_{1952-1963, pre} \\times \\text{Exposure to Vaccine}_{sc}\\right)\\) captures the interaction between the pre-vaccine measles incidence rate \\(M_{1952-1963}^{pre}\\) and the duration of an individual’s exposure to the vaccine. The \\(\\beta\\) coefficient estimates the differential impact of vaccine exposure across states with varying pre-vaccine measles incidence. Fixed effects \\(\\delta_s, \\gamma_c, \\alpha_t\\) account for state, cohort, and survey-year-level unobserved characteristics, while \\(X_{isct}\\) controls for individual-level characteristics (such as gender and race). The coefficient of interest \\(\\beta\\) relies on two sources of variation: differential exposure of cohorts to the timing of the vaccine’s introduction differential measles’ incidence rates across states. Thus, \\(\\beta\\) gives the reduced-form estimate for the differences in labor market outcomes based on measles incidence rates before vaccine. 5.1.2 Good Practices Before Starting 5.1.2.1 Packages To run this analysis you only need to install the estout package. ssc install estout, replace 5.1.2.2 Folder Creation Before starting the replication of this study, first create a structured folder system to organize your files and code efficiently. Set up a main project folder and create 5 sub-folders within this one: “raw_data”: Store here the datasets that are in the “raw_data” folder that you downloaded from the full original replication package. “data_replication”. This is where we will store the cleaned datasets: “inc_rate_ES.dta” for the event-study graph, “main_dataset_acs_200017.dta” for the main results, and “placebo_dataset_acs_196070.dta” for the robustness check. “do_files_replication”: you can store there the cleaning do-files and the do-file used in the rest of this replication (“4_dofile_results_LASCPE.do”). “logs_replication”: this folder will store the log of the results. A log file is a record of everything that happens while you are using Stata, both what is written in the do-file and what Stata produces in response. “output_replication”: this folder will contain the figure and tables you’ll create. Within this folder, create one sub folder named “tables” and another one named “figures” In each do-file, the setting of globals (macro names that will help you store the paths to the folders as strings) is the following: global home &quot;~/replication_folder&quot; //this is where you put the path to your main project folder global tables &quot;$home/output_replication/tables&quot; global figures &quot;$home/output_replication/figures&quot; global do_files &quot;$home/do_files_replication&quot; global data &quot;$home/data_replication&quot; global raw &quot;$home/raw_data&quot; global logs &quot;$home/logs_replication&quot; 5.1.2.3 Cleaning the data In order to clean the data and to obtain the three datasets necessary to produce each one of the outputs, you first need to run the master do-file “0_master_do_cleaning.do”. This do-file is going to call each of the 3 cleaning do-files, in the right order, and is going to create three datasets in your “data_replication” folder. Do not forget this step before trying to do the rest of the analysis! 5.1.3 Descriptive Statistics - Event Study To identify the causal effect of vaccination on labor market outcomes and to validate the staggered DiD assumptions, the author first uses an event study analysis framework: \\[ Y_{ts} = M_{1952-1963}^{pre} \\left[ \\sum_{y=-6}^{-2} \\alpha_y \\mathbf{1}\\{t - t^* = y\\} + \\sum_{y=0}^{11} \\lambda_y \\mathbf{1}\\{t - t^* = y\\} \\right] + \\delta_s + \\gamma_t + \\theta X_{ts} + \\epsilon_{ts} \\] The above equation models the relationship between a dependent variable \\(Y_{ts}\\) (e.g., the incidence of a disease) and the interaction of pre-vaccine measles rates \\(M_{1952-1963}^{pre}\\) with event-time dummies. The terms \\(\\mathbf{1}{t - t^* = y}\\) capture event-time effects relative to the introduction of the measles vaccine, where \\(y\\) indexes the years before or after the vaccine became available. The coefficients \\(\\alpha_y\\) represent pre-treatment trends, which are crucial for testing the parallel trends assumption of staggered DiD, while \\(\\lambda_y\\) estimate the impact of the intervention (vaccine availability) in post-treatment periods. The model includes state fixed effects \\(\\delta_s\\), year fixed effects \\(\\gamma_t\\), and covariates \\(X_{ts}\\). Figure 3: Event Study—Effect of Measles Vaccine on Measles Incidence Figure 3 of the paper demonstrates the effect of the introduction of the vaccine on the incidence rate of measles. We first load the primary dataset, “inc_rate_ES.dta”, which contains incidence rates for measles before and after the vaccine was introduced. Clearing existing data ensures no residual data interferes with our analysis: use &quot;$data/inc_rate_ES.dta&quot;, clear To analyze the impact of pre-vaccine infection rates over time, we create interaction terms between yearly indicators (_Texp_i, i indicating the year) and the 12-year average measles rate per state (avg_12yr_measles_rate). Specifically, we generate interaction terms for years 1 through 18, which capture pre-vaccine exposure rates. Note that year 6 is intentionally omitted as the reference year, anchoring the analysis and allowing for a comparative interpretation of effects across other years relative to this baseline. local year &quot;1 2 3 4 5 7 8 9 10 11 12 13 14 15 16 17 18&quot; foreach i of local year { gen exp_Mpre_`i&#39; = _Texp_`i&#39; * avg_12yr_measles_rate } Next, we run a regression analysis with measles incidence (Measles) as the dependent variable. The independent variables include the generated interaction terms (exp_M*), along with additional controls (state dummies, population size, exposure dummies). Then, the command cluster(statefip) ensures robust standard errors by accounting for correlation within states over time. This regression provides estimates of measles incidence both before and after vaccine introduction, adjusted for initial infection levels. Finally, we use the regsave command to store the regression results, including confidence intervals and p-values. These results will be used later to facilitate the visualization of the event study findings through a graph. reg Measles exp_M* _Is* population _T* avg_12yr_measles_rate, cluster(statefip) robust regsave, ci pval For clarity in graphing, we modify the data to explicitly include a zero value for the omitted year (year 6) as the reference. Dropping unnecessary coefficients and setting all omitted values to zero ensure a consistent baseline for comparing effects in other years. drop in 18/87 set obs 18 replace var = &quot;exp_Mpre_6&quot; in 18 replace coef = 0 in 18 replace stderr = 0 in 18 replace N = 1108 in 18 replace ci_lower = 0 in 18 replace ci_upper = 0 in 18 To establish a temporal structure for the interaction terms, we assign each interaction term a specific time point, relative to the vaccine’s introduction. Negative values indicate years before the vaccine, zero marks the year of vaccine introduction, and positive values represent years after. This time structure is essential for interpreting trends visually. gen exp = . foreach i in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { local value = `=`i&#39;-7&#39; replace exp = `value&#39; if var == &quot;exp_Mpre_`i&#39;&quot; } sort exp The scatter command generates an event study graph plotting the coefficient estimates coef and confidence intervals ci* over time. Different line types and widths help differentiate the main estimates and confidence intervals, while yline(0) and xline(-1) mark reference points at zero and one year pre-vaccine, respectively. The x-axis shows years relative to vaccine availability, helping us interpret the effect timeline clearly. scatter coef ci* exp if exp&gt;-6 &amp; exp&lt;11, c(l l l) cmissing(y n n) /// msym(i i i) lcolor(gray gray gray) lpatter(solid dash dash) lwidth(thick medthick medthick) /// yline(0, lcolor(black)) xline(-1, lcolor(black)) /// subtitle(&quot;`sub&#39; rate by year (per 100,000)&quot;, size(small) j(left) pos(11)) ylabel( , nogrid angle(horizontal) labsize(small)) /// xtitle(&quot;Years relative to measles vaccine availability&quot;, size(small)) xlabel(-5(5)10, labsize(small)) /// legend(off) /// graphregion(color(white)) graph export &quot;$figures/Figure3_event_study.png&quot;, replace How to read Figure 3 ? Figure 3 demonstrates that the common trend assumption is valid, as there are no statistically significant differences in measles incidence rates among states during the pre-vaccine period. After the introduction of the measles vaccine in 1963, marked by the vertical line in the figure, there is a sharp decline in measles incidence rates. This decrease is stabilizing approximately four years after the introduction of the measles vaccine, with a coefficient of \\(-1\\). This coefficient indicates that for each unit of the pre-vaccine incidence rate, there was a one-for-one reduction in post-vaccine incidence, highlighting the vaccine’s effectiveness, especially in states with higher initial incidence rates. 5.1.4 Main Results - TWFE In order to obtain the main results of this paper, we will use the cleaned dataset of the American Community Survey (2000-2017), merged with measles rates data. To estimate the positive long-run effects of the measles vaccine on adults’ various outcomes (income, employment, poverty), the author uses a specification that allows for differential exposure to the measles vaccine, by interacting the measles rate pre-vaccine infection rate with a measure of the exposure to the vaccine, across states and cohorts of birth. Table 2: Effects of Measles Vaccine on Adult Labor Market Outcomes First, since our specification will include interaction terms, we drop interaction terms with empty cells. To explain this, let’s take a random example: if the variables “age” and “state of birth” are interacted, and you have no observations for two of your states, then the corresponding interaction outputs will be empty. Telling Stata to drop those terms will allow the code to run faster and will prevent errors. set emptycells drop Now, you are going to replicate the results of the table of main results, for every labor market and income outcomes. This code will allow you to learn two techniques: - Using loops to run regressions, to avoid a repetitive process. Using scalars in the estout package, to provide the complete table from Stata, with very little modification needed in LaTeX (or your preferred output format) to have a “publication style” table. eststo clear foreach dep in cpi_incwage cpi_incwage_no0 ln_cpi_income poverty100 employed hrs_worked { local controls i.bpl i.birthyr i.ageblackfemale i.bpl_black i.bpl_female i.bpl_black_female black female * Run main specification and store results reg `dep&#39; M12_exp_rate `controls&#39; i.year, robust cluster(bplcohort) eststo col`dep&#39; *Add outcome means&#39; line quietly summarize `dep&#39; if exposure == 0 local dep_mean = r(mean) estadd scalar dep_mean = `dep_mean&#39; * Store the coefficient of M12_exp_rate scalar beta_`dep&#39; = _b[M12_exp_rate] * Perform calculation with the stored coefficient scalar result_`dep&#39; = beta_`dep&#39; * (unweight_avg_12_measles_rate / 100000) * 16 * Add scalars to the eststo model estadd scalar unweight_avg_12_measles_rate = unweight_avg_12_measles_rate estadd scalar calculated_result = result_`dep&#39; } You are going to create a loop over the different dependent variables to run the main regression. This process is gonna take the following steps: Start by creating a loop over all the dependent variables that are used in Table 2, and then, you can start working within the loop. Since the controls are the same for each model, you can create a local with all the variables used as controls in this main specification. Now, you can run a simple regression using the variable of interests, the local with controls, census year fixed effects and robust standard errors clustered at the state of birth by cohort level. Each iteration of the loop will then save the result of the regression for a different dependent variable with the command eststo, which we will then use to create the table. You are now going to add rows 4 to 6 of Table 2, which are summary statistics and additional measures of the measles vaccine effect. First, you are going to calculate for each dependent variable its average for the prevaccine cohorts (i.e. those who were not exposed to the vaccine). Each mean is going to be stored in a scalar with the “estadd” function, which you will then display in the final table of results. To obtain row 6 of the table of main results, the author says that she performed “back of the envelope calculations” to calculate the impact for each dependent variable of being born in a state with unweighted average prevaccine measles incidence rate (across all states) and full exposure to the vaccine. To do this, you need to perform this simple calculation in Stata: using the coefficient of interest (associated to the variable \\(measles\\times exposure\\)) of each outcome variable, you need to multiply it by the unweighted average measles infection rate per 100,000 individuals, and then multiply this by 16 (number of years for full exposure). Using estadd enables you to add to each iteration of the loop (and to each eststo col*) the scalars needed to replicate the rows 4 to 6 of the main results table: everything that you added with the estadd function is going to be stored along with the regression results for each dependent variable, under a different name. After closing the loop, you can finally use the esttab function to replicate (almost) identically the main results table. First, you are going to use each stored results under the prefix col*, and save them in a LaTeX format table in your output folder. The only coefficient you need to keep is the one associated with the variable of interest, and you can use the labels that you created before to display them in the table. To change the number of decimals for the coefficient and the standard errors, use b() and se(). In order to create rows 2 to 6, you can specify which “stats” you want in your table: here we want the R-squared, the number of observations and the different scalars that we stored earlier. fmt enables you to change the number of decimals you want to show, and you can modify the labels of each one of those stats. The code %9.0fc adds commas between thousands for numbers above 1,000 which is practical for readability. esttab col* using &quot;$tables/Table2_Main_results.tex&quot;, keep(M12_exp_rate) b(4) se(4) /// label stats(r2 N dep_mean unweight_avg_12_measles_rate calculated_result, fmt(4 %9.0fc 4 0 4) /// labels(&quot;R$^2$&quot; &quot;Observations&quot; &quot;Outcome mean for prevaccine cohorts&quot; &quot;Av. 12-y measles prevaccine incidence rate&quot; &quot;Outcome with av. measles incidence rate and full exposure&quot;)) replace The results show a positive and significant effect of the vaccine on labor market outcomes across all specifications and measures of earnings. Indeed, the coefficient of row 6 in Column 3 can be interpreted as a 1.7 percent increase in income for an individual being born in a state with average measles incidence rate and full exposure to the vaccine. 5.1.5 Robustness Check - Placebo Test 5.1.5.1 What is a Placebo Test ? A placebo test is a method used to check if the results of a study are valid by applying the same analysis to a situation where no effect is expected. If no significant effect is found in the placebo test, it supports the reliability of the main results. However, if a significant effect appears, it suggests the findings might be influenced by errors, unaccounted factors, or random chance. This helps ensure the study’s findings are specific to the actual treatment or intervention. 5.1.5.2 In this case The author conducts a test for contemporaneous effects by using adults aged 26 to 60 at the time of the 1960 and 1970 census. Since measles is a childhood illness, these individuals, who were already adults in 1963, would not have been eligible for the vaccine as they likely had measles in childhood. Therefore, the measles vaccine should have no impact on their immediate labor market outcomes. To examine this, the Main Results equation is adjusted by interacting \\(M^{pre}_{1952-1963}\\) with an indicator variable if the observation occurs after the measles vaccine has been introduced. The results show no statistically significant differences in employment, labor force participation, or years of education between adults in high-exposure and low-exposure states. These findings support the idea that the measles vaccine has short-term health impacts on the population it prevents from contracting measles, which in turn can lead to long-run impacts for those vaccinated. Table 5: Contemporaneous Outcomes Check, Adult Labor Market Outcomes 5.1.5.3 Code explanation To perform this placebo test, you will use the dataset containing contemporaneous labor market outcomes (placebo_dataset_acs196070.dta). The author examines the outcomes of adults during the vaccine introduction period to validate their TWFE methodology and assess the plausibility of the common trend assumption. use &quot;$data/placebo_dataset_acs196070.dta&quot;, clear set emptycells drop eststo clear As in the Main Results section, begin by dropping interaction terms with empty cells. Then, clear the previously stored estimators to avoid conflicts when extracting the results table. The code structure will mirror that of the main results, employing a loop to run regressions for each dependent variable: employment status, labor force participation, and years of education. The independent variable in these regressions is an interaction term between the pre-vaccine measles rate (1952–1963) and an indicator for whether the observation occurs after the introduction of the measles vaccine. Within the loop, add a local macro to control for key variables, including sex, race, age, state, family income, and rural residency. The author uses a similar set of controls as in the main results to replicate the specification for a group of individuals who are unlikely to experience the vaccine’s positive effects. To compute the estimated impacts of the regression coefficients, multiply each coefficient by the average measles incidence rate (0.00964). Note that the full exposure time (16 years) is not applied here, as the outcomes were measured before the full exposure period. A small discrepancy in point estimates for the calculated impact between our replication and the paper arises because the author expresses results in percentage points. foreach dep in employed labforcepart edu_years2 { local controls i.year rural ruralpost female femalepost blackpost blackfemale blackfemalepost i.age i.race i.statefip famincome famincome2 famincome3 reg `dep&#39; M_post_rate_scale `controls&#39; , cluster(statefip) eststo col`dep&#39; * Store the coefficient of M_post_rate_scale scalar beta_`dep&#39; = _b[M_post_rate_scale] * Perform calculation with the stored coefficient scalar result_`dep&#39; = beta_`dep&#39; * (unweight_avg_12_measles_rate / 100000) * Add scalars to the eststo model estadd scalar calculated_result = result_`dep&#39; } Use the esttab command again to generate a self-contained table in LaTeX format. You can use the same options as in the main results section. esttab col* using &quot;$tables/Table5_placebo.tex&quot;, keep(M_post_rate_scale) b(4) se(4) /// label stats(r2 N calculated_result, fmt(4 %9.0fc 4) /// labels(&quot;R$^2$&quot; &quot;Observations&quot;&quot;Outcome with av. measles incidence rate post-vaccine availability&quot;)) replace The results align with the author’s assumptions: the coefficients are statistically insignificant, and the calculated impacts are close to zero across all specifications. This supports the hypothesis that the measles vaccine had immediate effects on measles incidence and long-term labor market impacts for adults exposed to the vaccine as children. 5.1.6 Conclusion In this study section, we have walked through the replication of “The Long-Term Effects of Measles Vaccination on Earnings and Employment” by Atwood (2022). We started by exploring the study’s research question and methodology, reviewing the staggered Difference-in-Differences (DiD) framework, and tackling advanced topics like continuous treatment intensity, event studies, and a placebo robustness check. Through this section, you can gain experience with important econometric tools and Stata coding skills, as used in academic research and policy evaluation. 5.1.6.1 Extension on new TWFE estimators A limit of this paper is that it uses a two-way fixed effects estimator, with a staggered continuous treatment timing, as described in the Methodology section. However, as underlined by the Goodman-Bacon (2021). decomposition, the TWFE estimator is a weighted average of all \\(2\\times2\\) difference-in-differences estimators, and does not provide the causal effect of treatment on the outcome variable. A solution to this caveat would be to use new TWFE estimators and compare the results with the specification used by the author. However, the Stata package necessary to implement new “clean” estimators does not yet exist for a continuous treatment (here, the measles incidence interacted with the length of exposure to the vaccine). Indeed, as showed by Callaway et al. (2024), the causal treatment effect can be identified on a generalized common trend assumption, in the way as previous estimators for binary treatments. However, it is not possible to interpret differences in the parameters across the different values of the continuous treatment (here, different measles incidence rates) as unbiased, due to selection on unobservables. They provide an alternative estimation strategy that do not suffer from these TWFE drawbacks, but the paper has yet no replication package to implement their new procedure. For a more complete review of the (fast moving) literature on TWFE methods, we suggest you to look at Roth et al. (2023). 5.1.7 References Atwood, Alicia. 2022. “The Long-Term Effects of Measles Vaccination on Earnings and Employment.” American Economic Journal: Economic Policy, 14 (2): 34–60. DOI: 10.1257/pol.20190509 Bleakley, H. (2007). Disease and Development: Evidence from Hookworm Eradication in the American South. The Quarterly Journal of Economics, 122(1), 73–117. http://www.jstor.org/stable/25098838 Callaway, B., Goodman-Bacon, A., &amp; Sant’Anna, P. H. (2024). Difference-in-differences with a continuous treatment (No. w32117). National Bureau of Economic Research. Goodman-Bacon, A. (2021). Difference-in-differences with variation in treatment timing. Journal of Econometrics, 225(2), 254-277. https://doi.org/10.1016/j.jeconom.2021.03.014 Roth, J., Sant’Anna, P. H., Bilinski, A., &amp; Poe, J. (2023). What’s trending in difference-in-differences? A synthesis of the recent econometrics literature. Journal of Econometrics, 235(2), 2218-2244. https://doi.org/10.1016/j.jeconom.2023.03.008 Authors Elsa Poupelin, Camille Sirera, Anna Luntovska, students in the Master program in Development Economics (2024-2025), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date December 2024 "],["expected-returns-to-crime-and-crime-location.html", "5.2 Expected Returns to Crime and Crime Location", " 5.2 Expected Returns to Crime and Crime Location Braakmann, N., Chevalier, A., &amp; Wilson, T. (2024). Expected returns to crime and crime location, AER, October 2024. https://www.aeaweb.org/articles?id=10.1257/app.20220585 [Download Do-File corresponding to the explanations below] [Download Codebook] [Link to the full original replication package paper from Open ICPSR] Highlights The research question of this paper is the following: Did the external increase of gold price affect the behavior of criminals in the sense that they increase the number of jewelry crimes and their targets, by selecting groups that stereotypically have more gold objects at home? The paper employs a Difference-in-Differences (DiD) approach to examine the impact of gold price fluctuations as an exogenous shock on burglary locations. As a treatment group, it uses neighborhoods with a high proportion of South Asian households that they identify using census data. The DiD estimator interacts the monthly gold price with an indicator for a South Asian neighborhood, while controlling for various low-level geographical fixed effects, as well as time effects and neighborhood specific time-trends. In the recent literature, their methodology falls somewhere between the traditional 2x2 DiD and the Generalized DiD. This is because , while they compare each group’s outcome before and after the treatment, they also incorporate multiple treatment and control groups (neighborhoods). This paper contributes to the DiD literature by defining the treated group as local outliers. Additionally, it incorporates quadratic trends in the main regression to account for time-varying unobservable variables. They also contribute by conducting a robustness check using placebo tests with 1,000 replications of the main regression and randomized treatment assignments. Here, compared to the original replication package, we provide a smaller dataset that retains only the variables used, enabling a numerically simpler replication. Additionally, we offer a more detailed explanation of the original package. Finally, we have reduced the number of randomizations in the placebo test code from 1,000 to 25, significantly reducing run time: while the original code took 24 hours, this version requires only 60 to 90 minutes. This code will teach some important functions and tricks: How to use a high-dimensional fixed effects regression How to perform a placebo test and plot it ! Give you practice in generating statistics and regressions, and then aggregating them into clear and comparable tables Give you practice in using the preserve command, which can be very useful when using Stata ! 5.2.1 Introduction and Motivation This paper discuss to what extent changes to the expected returns of criminal activity affect criminals’ choice about the nature of their crimes and their targets (the location of crime). For such, they need an exogenous variation on gold prices and a population sample with enough variation in their variable of interest. Hence, regarding the first condition, the authors showed that there was enough increase in gold prices in the period of interest. Then, they look at the effect of such an increase in gold prices on the number of jewellery crimes, according to the regions where criminals had more chance of success. To tackle this question, the authors rely on ethnic variations in household preferences to store wealth in the form of gold jewellery: they exploit the long-standing cultural practices of families of South Asian descent to have a high proclivity toward storing wealth in the form of gold jewellery, specially of the highest quality (gold 22-carat). Then, they estimate whether the exogenous variation in the price of gold affects the geographical pattern of crime in England and Wales, as burglars target households with an expected greater preference of gold. That is, burglars target regions where there is a higher concentration of the South Asian community. This second step of the article faces two challenges: Individuals from South Asian descent represent just over 5% of the population in England and Wales. South Asian communities tend to be regionally concentrated in the UK, in a way that the national variation is not well suited to the purpose since it would imply that burglars travel large distances in order to target neighborhoods with a high South Asian share. As a consequence, the identification strategy used was to choose another segregation level: the local authorities. They compare neighborhoods within local authorities, which allows higher variation of the share of the population originating from South Asia between the units compared. In other words, they exploit this within-local authority variation in the share of South Asian households to estimate how fluctuations in the price of gold affect the distribution of burglaries within a local authority. 5.2.2 Data In practice, to exploit this within-local authority variation in the share of South Asian households, the authors collected data on individual crime occurrences between 2011 and 2019 for all police forces in England and Wales, and aggregate these to form a monthly panel at the level of small geographical neighborhoods (lower-layer super output areas, which gives 35,000 LSOAs). The neighborhoods (LSOA) are classified as relatively high density South Asian neighborhoods when their share of South Asians is an outlier in their local authority (LA). To define this relatively high density, they use the rule: if their share of South Asian population is in excess of the 75th percentile plus 1.5 times the interquartile range for that local authority (which is a standard outlier definition). 5.2.3 Good practices before starting If you wish to replicate this article yourself, you can both use the original LSOAFinal.dta from the original replication package, or use the prepared data available in this project Dataset_BCW.dta. To build it, we kept the variables that we will be using here, rename them, and generate intermediary variables. You will have to set your directory where you saved the data to access it. You will also have to create the appropriate folders and globals to store your datasets and results. A global is a named storage location that can hold a value or a string of text. We use the command global to define it and the prefix \\$ to access it. * Set the directory, where you have stored the original dataset and where you will store your results. To do so create a file named &quot;data&quot; in the file that you have defined as home *global home &quot;C:\\Users\\XXX&quot; *cd &quot;C:\\Users\\XXX\\...\\data&quot;&quot; * Set a global, to export all outputs we are going to make. To do so, create two files, &quot;graphstata&quot; and &quot;texstata&quot; in the file that you have defined as home/directory above. global graphout &quot;$home/graphstata&quot; global texout &quot;$home/texstata&quot; * Importing data use &quot;$home/data/Dataset_BCW.dta&quot;, clear To carry out the replication of this paper, the following packages must be installed: ssc install estout ssc install reghdfe ssc install ftools 5.2.4 Descriptive Statistics After setting the Stata environment, we can begin to explore the data characteristics through the computing of descriptive statistics to achieve the replication of Table 1. For this article, we will generate the stats for 3 groups. The first subsample is what they consider the treatment group, or neighborhoods with a high share of South Asian people, their group of interest. To identify them, we will add the condition treat2 == 1 in our command, as this is the dummy variable that will force our code to only generate statistics for the units that obey this condition. The other groups are the control group (treat2 == 0) and the overall population. 5.2.4.1 Step 1 : Finding means and standard deviations For that, we will need 2 main commands: eststo: it stores estimation results in a model list for later use (e.g., to create comparison tables). This allows you to organize and summarize results across multiple groups. Following the eststo command, you can add a name, so it’s easy to reference it later. Always remember to est clear before running this command to avoid using previously stored results. summarize: creates main statistics about the given group such as mean, standard deviation, maximum, and minimum values. To find means only for our group of interest, we add the condition treat2 == 1 to determine the treatment group, as previously explained. We will only replicate column 3 of our table in this code. If you are interested in replicating all 3 columns, you only need to change the condition totreat2 == 0 for the control group, and remove all conditions for the total population statistics. Don’t forget to rename the estimation after the eststo command. estadd: adds custom annotations to stored regression results, like model specifications or contextual details, to enrich descriptions for easier comparison or presentation in tables. We created a local variable named tt to save the number of months for which we compute information (108 months). We also added a scalar variable named unique_lsoacode, which gives the number of neighborhoods that fulfill the condition treat2 == 1. preserve: this function saves the existing dataset used and allow us to make temporary changes on it, such as calculating variables or creating graphics. Always remember to restore after finishing the temporary changes Here, we use it to calculate the number of neighborhoods for the given condition: the contract function keeps only the unique values of variable we chose; by creating a local variable equal to _N, we are defining a temporary variable that represents the count of unique neighborhoods for the specified group. //* Table 1 - Summary statistics */ // Column 3: Treatment Group est clear eststo treated: estpost summarize BIPshare treat2 Blackshare DEshare urban totpop unemploy burglary robbery vehiclecrime violence totexclburg if treat2 == 1 estadd local tt &quot;108&quot; * Count unique lsoacode for control group preserve keep if treat2 == 1 contract lsoacode local neigh_treated = _N // Store the number of unique neighborhoods in a local macro restore * Add the unique count to the eststo object estadd scalar unique_lsoacode = `neigh_treated’ ::: {style=“text-align: center;”} 5.2.4.2 Step 2 : Export our results in a tex-format table We can visualize the results of the estimation with esttab command, which takes stored results from one or more estimation commands and formats them into a table. You can use it in multiple ways, as it allow us to choose what coefficient we want, with different decimals points and notations, and exporting in different forms. Here, we choose to do export in the .tex format, which goes very well with Latex files and gives us nice and clean tables. You can see all the specifications we choose in the following. Furthermore, we highlight the using $texout/Table_1.tex command, which sets to save the generated table in the path we set before, in the correct form file. esttab total1 control treated using &quot;${root}\\output\\Table_1test_norandom.tex&quot;, /// cells(&quot;mean(fmt(3))&quot; &quot;sd(par)&quot;) substitute({l} {p{11cm}}) /// stat(tt unique_lsoacode N, fmt(%9.0fc %9.0fc %9.0fc) labels(&quot;Time periods (months)&quot; &quot;Neighbourhoods&quot; &quot;Obs&quot;)) nonum /// collabels(none) /// mti(Total Control &quot;Outlier in LA&quot;) /// title(&quot;Summary Statistics by Group&quot;) /// addnote(&quot;\\textit{Notes: The table displays summary statistics at the neighbourhood level (LSOA). The top panel is based on Census 2011 data, quarterly unemployment rate is provided by NOMIS. Monthly Crime data are aggregated at the LSOA level by the authors. Note, that there is no data available for the Greater Manchester Police Force between June and December 2019.}&quot;) /// label replace nonumbers nocons nodepvar nostar compress 5.2.5 Main strategies explanation: Difference-in-Differences After having carried out the descriptive statistics analysis, in this section we dive into the empirical strategy of the research paper. The main analysis of this article relies on a standard Difference-in-Differences design, where we have one unique treatment (international gold princes increase) that happened at the same time, for all treatment groups (neighborhoods with a relatively high proportion of South Asian households compared to the rest of the local authority) and their respective control group (neighborhoods with a relatively low proportion of South Asian households, in the same local authority). \\[ y_{ict} = \\beta_0 + \\beta_1 SA_{ic} + \\beta_2 GP_t + \\beta_3 (SA_{ic} \\ast GP_t) + \\alpha_c + \\gamma TREND_{it} + \\epsilon_{ict} \\] The output variable is the prevalence of burglaries in neighborhoods (LSOA) i, nested within local authorities c, at time t. The main explanatory variable is the average monthly gold price, \\(GP_t\\). This variable is interacted with the treatment variable \\(SA_{ic}\\), which takes the value of 1 when its condition is fulfilled. The coefficient of interest is the one associated with the interaction (\\(\\beta_3\\)). To ease the interpretation, the measure of crime and the gold price are expressed in inverse hyperbolic sine, so that this coefficient can be interpreted as an elasticity. \\(\\alpha_c\\) represents local authority fixed effects. Usually, it is also important to include time-varying controls at the local level, accessing for other possible variables that can explain the output. The authors differentiate themselves by using neighborhood-specific time trends \\(TREND_{it}\\), which encapsulate how the outcome changes over time within each specific neighborhood. This approach allows them to account for these unique long-term patterns in each neighborhood. 5.2.6 Main Results The following code replicates Table 3 of the paper, which highlights the main results. On this page, we provide the code only for the 4th column estimation, which gives us one of the preferred estimations. If you’re interested in replicating the entire table, you can do so by using the provided do-file or by simply modifying this code, adding or removing variables and fixed effects. You will first run the regressions and store the estimates, then you will export your results 5.2.6.1 Step 1 : Run the regressions We will again use eststo, which we provide further explications above. Some important commands used here: quietly: suppresses the display of output during the execution of the regression. reghdfe: runs a high-dimensional fixed effects regression. This is a robust method for handling large datasets with multiple levels of fixed effects. hypburg: Our dependant variable, measures of the prevalence of various types of crime (principally burglary) for the respective neighborhood in a given month. BIParea: a binary variable indicating whether a certain condition related to the BIP is met. It corresponds to \\(SA_{ic}\\). hypgoldprice: a continuous variable representing hypothetical gold prices, which could influence the outcome. It corresponds to \\(GP_t\\). i.month: includes month categorical variable (values from 1 to 12) to control temporal variations. Indeed, it is likely that there are more burglaries during the summer (and differ across units). We also include the command absorb in reghdfe: specifies the fixed effects to “absorb”. They are: i.pfa_num: Local Authority Fixed effects, corresponds to \\(\\alpha_c\\). lsoaid#c.time: Interaction of the location ID (lsoaid) with a linear time variable (time), controlling for time-specific trends at the location level. lsoaid#c.time2: Interaction of lsoaid with a quadratic time term (time2), allowing for nonlinear time trends specific to each location. We also clusters the standard errors at the location ID (lsoaid) level with cluster(lsoaid). This accounts for within-location correlation of errors over time, providing more reliable inference. Finally, we used estadd again, adding custom annotation such as weather if the model have Fixed effect or no, the unit of the quadratic trend, and the number of months /* Table 3 - Impact of Gold Price on Burglaries */ // Column 4 clear all use &quot;$home/data/Dataset_BCW.dta&quot;,clear est clear eststo: quietly reghdfe hypburg BIParea hypgoldprice interact i.month , absorb(lsoaid#c.time lsoaid#c.time2) cluster(lsoaid) estadd local fe &quot;None&quot; estadd local time &quot;LSOA&quot; estadd local tt &quot;108&quot; 5.2.6.2 Step 2 : Export our results in a tex-format table esttab using &quot;$texout/Table_3_Main.tex&quot;, replace b(3) se(3) nonote compress nomti substitute({l} {p{16cm}}) stat(fe time tt N_clust N, fmt(%9.0f %9.0f %9.0f %9.0f %9.0f) labels(&quot;Fixed Effect&quot; &quot;Quadratic Time Trend&quot; &quot;Time Periods&quot; &quot;LSOAs&quot; &quot;Obs&quot;)) /// keep(BIParea hypgoldprice interact) order(BIParea hypgoldprice interact) /// coef( BIParea &quot;High South Asian neighbourhood&quot; hypgoldprice &quot;Gold Price&quot; interact &quot;High South Asian neighbourhood x Gold Price&quot;) addnote(&quot;Notes: The table displays estimates of the impact on burglaries estimated using Eq.1. Regressions also control for seasonality via monthly dummies. Standard errors adjusted for clustering at the LSOA level in parentheses. */**/*** denote statistical significance on the 10\\%, 5\\% and 1\\% level respectively.&quot;) title(&quot;Table 3: Impact of Gold Price on Burglaries&quot;) Table 2 - Regression results 5.2.7 Robustness Checks In this last section, we explain how to perform a standard robustness check. This is a common and highly recommended practice, as it strengthens the robustness of the main results. The authors tested alternative definitions of treated neighborhoods and analyzed displacement effects on other crime types or nearby neighborhoods. They also conducted placebo randomization tests, ensuring that these effects are not driven by arbitrary threshold definitions. This is the practice we will replicate in the following code. Figure 1 - Placebo test The placebo randomization tests whether the results are robust to the potentially arbitrary definition of treated neighborhoods (outliers of BIPshare). Unlike in classic DiD designs, where treatment is exogenously defined (e.g., by policy), here the researchers define treatment using a statistical threshold, allowing for randomization. By randomly assigning the treated status 1,000 times and recalculating the estimated effects, they compare these placebo results to the actual coefficients. If the real results differ significantly from the random ones, it validates that the findings are not driven by the threshold choice (or randomness) but by the actual relationship being studied. The figure shows the distribution of placebo regression coefficients, with a line for the authors’ coefficient, highlighting its significant difference, proving that the coefficient found is not randomly driven. The following code will replicate this figure. 5.2.7.1 Step 1 : Adjusting data for the Placebo test clear all use &quot;$home/data/Dataset_BCW.dta&quot;, clear est clear Before starting the randomization, we need to ensure the same distribution is obtained each time, with set seed. This is essential for result traceability. set seed 12345 With set matsize, we expand the maximum size for matrices, essential for handling large data or storing results during iterative procedures. set matsize 5000 We will keep only the relevant variables to speed up computation time. We used compress to optimize variable types to reduce memory usage. We save this reduced dataset as a new file for subsequent use. With keep, we filter the dataset to include only observations for the first time period. This is because BIPshare is assumed constant over time, so only the initial period is needed for randomization. keep laid lsoaid time time2 BIPshare hypgoldprice BIPhypgoldpriceOUTLA month hypburg BIPuqLA BIPiqrLA BIParea interact compress save a, replace keep if time==1 5.2.7.2 Step 2 : Create the random distribution To reduce calculation time, we can reduce the number of distributions to 25 instead of 1000, the statistical accuracy of the results is decreased, but we can explain that this is to enable faster calculations. 5.2.7.2.1 Randomization forvalues i=1(1)25: Executes a loop to generate random distributions between 0 and 1 for each LSOA. bys laid (lsoaid) : Divides the data into groups based on the variable laid Within each group, the data is sorted by lsoaid. g rand i' = runiform(0,1): Generates uniformly distributed random numbers between 0 and 1 within each LA. Simulates random assignment of treated neighborhoods for placebo tests. forvalues i=1(1)25{ bys laid (lsoaid): g rand`i&#39;=runiform(0,1) } 5.2.7.2.2 Sorting Sorts the LSOAs by the random numbers (rand i') within each LA. Assigns a sequential order (random i') to LSOAs based on the sorted random numbers. Creates a new randomized BIPshare variable for each iteration. forvalues i=1(1)25{ bys laid (rand`i&#39;): g random`i&#39;=_n bys laid (lsoaid): g BIP`i&#39;rand=BIPshare[random`i&#39;] } 5.2.7.2.3 Hypothesis tested: Now, we test hypothesis to check the results of the Placebo test. Null hypothesis (H0): The relationship between BIP and hypburg is random. If H0 is true, the coefficients obtained with the real data should not differ from those obtained with the random data. Alternative hypothesis (H1): The relationship between BIP and hypburg is significant and non-random. keep laid lsoaid BIP*rand save &quot;$home/data/tempLSOA&quot;, replace clear We reload the original dataset to prepare for merging with the randomized data. use a We merge the main dataset with the generated random distributions and delete the temporary dataset merge m:1 laid lsoaid using &quot;$home/data/tempLSOA&quot;, nogen erase &quot;$home/data/tempLSOA.dta&quot; 5.2.7.3 Step 3 : Regress the new distribution and store the results Now, we gonna re-do the regressions explain in section 1.5, for column 4 of previous Table 2. reghdfe hypburg BIParea hypgoldprice interact i.month , absorb(i.laid lsoaid#c.time lsoaid#c.time2) We store the coefficients and standard errors from regressions, to set as comparison in future image. mat B=[_b[interact], _se[interact]] Why the following code is so long to run? This loop may take approximately 24 hours to run with 1,000 randomizations! With “only” 25 randomizations, the code processes each of the 36,000 LSOAs by generating 25 random BIP distributions (forvalues i=1(1)25) and running one regression for each random distribution. This means the following code will execute approximately 900,000 regressions in total (for 25 randomizations). Run the code and enjoy a well-deserved break! In the following loop, with gen, we create a new variable (BIPhypgoldout i') to identify outlier BIPshare values from the random distribution. Outliers are defined as values greater than the upper quartile (BIPuqLA) plus 1.5 times the interquartile range (BIPiqrLA). These outliers are then multiplied by hypgoldprice. Converts the outlier variable (BIPhypgoldout i) into a binary indicator. This defines a “treated area” based on the randomized BIP distribution. Then we run multiple regressions against the variable hypburg We store the results with mat. Appends the coefficient and standard error of the BIPhypgoldouti variable from the current regression to the matrix B. Finally, with drop we delete the variables specific to the current iteration to free memory and avoid conflicts in the next loop iteration forvalues i=1(1)25{ gen BIPhypgoldout`i&#39;=(BIP`i&#39;rand&gt;=(BIPuqLA+(1.5*BIPiqrLA)))*hypgoldprice gen BIPareaout`i&#39;=BIPhypgoldout`i&#39;&gt;0 reghdfe hypburg BIPareaout`i&#39; hypgoldprice BIPhypgoldout`i&#39; i.month , absorb(i.laid lsoaid#c.time lsoaid#c.time2) mat B=[B\\_b[BIPhypgoldout`i&#39;], _se[BIPhypgoldout`i&#39;]] drop BIPhypgoldout`i&#39; BIPareaout`i&#39; } 5.2.7.4 Step 4 : Export as a graphic 5.2.7.4.1 Organizing data that we will plot Now, we create a new Excel file for each matrix m (stored coefficients and standard errors), with putexcel. The second line of the code writes thematrix m to the first cell (A1) of the corresponding Excel file. foreach m in B { putexcel set &quot;$texout/randdist`m&#39;&quot;, replace putexcel A1= matrix(`m&#39;) } We import the combined Excel file with all the stored matrices into Stata for further analysis. import excel &quot;$texout/randdistB&quot;, clear We create a new variable num as a sequential identifier for each row of the imported dataset. gen num=_n We calculate summary statistics for the variable A in the first row of the dataset. su A if num==1 We store the mean of A (from the first row) in the local macro BIP. local BIP=r(mean) 5.2.7.4.2 Creating density plot We will first count the total number of observations in the dataset. Then, with local, we store the number of rows minus one (excluding the first row) in the local macro obs. Finally, we delete the first row from the dataset with drop if. count local obs=(r(N) - 1) drop if num==1 We summarize the remaining observations of A to understand its distribution. Then, run a regression on the variable A. su A reg A We again conduct a hypothesis test to compare the constant term (_cons) from the regression with the stored mean value (BIP). test _cons=`BIP&#39; We generate a density estimate to visualize the distribution of estimated coefficients. kdensity A, generate(beta density) We create a line plot showing the kernel density of coefficients (density) against beta. Adds a vertical line at the mean coefficient (BIP) to visually compare the real result with the random coefficients. two line density beta, xtitle(Beta) xlabel(-0.06(0.02)0.06) ylabel(0(5)25) xline(`BIP&#39;) graphregion(color(white)) ytitle(Density) Finally, we export the density plot to the local already created. And then delete the dataset used, to finish with a clean environment! graph export &quot;$graphout/Figure_2.pdf&quot;, replace as(pdf) clear erase a.dta Authors Victoria Bell, Rebecca Gramiscelli-Hasparyk, and Louis Marie, students in the Master program in Development Economics (2024-2025), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date December 2024. "],["the-effects-of-a-large-scale-mental-health-reform-evidence-from-brazil.html", "5.3 The Effects of a Large-Scale Mental Health Reform: Evidence from Brazil", " 5.3 The Effects of a Large-Scale Mental Health Reform: Evidence from Brazil Dias, Mateus, and Luiz Felipe Fontes, “The Effects of a Large-Scale Mental Health Reform: Evidence from Brazil”, American Economic Journal: Economic Policy, 2024, Volume 16, 257–89. https://www.aeaweb.org/articles?id=10.1257/pol.20220246 [Download Replication Do-File] [Download Codebook] Link to the original replication package: https://www.openicpsr.org/openicpsr/project/195122/version/V1/view 5.3.1 Highlights This paper evaluates how the introduction of mental health centers (CAPS) impacted the Brazilian health system, the patient’s health and how it impacted crimes. Since the CAPS were constructed from 2003 to 2016, the authors use a staggered difference-in-differences design, conducting event studies for every outcome to make sure that there were no pre-trends pre-reform. Our approach aims to give a detailed version of how to conduct a staggered difference-in-differences in Stata. Above all, the following Stata tricks will be reviewed: How to create a table for the descriptive statistics using a loop and a matrix How to use the package did_multiplegt to create a Program in Stata and run it repeatedly to estimate an effect for multiple groups and multiple time periods. How to create a clean scatter plot to give a visual representation of a correlation 5.3.2 Introduction When randomized experiments are not feasible, the difference in differences strategy is a very good substitute to estimate a causal effect, especially when some groups or regions are exposed to an intervention or policy, and some others are not. This paper focuses on estimating the impact of a psychiatric reform of mental health centers in certain municipalities in Brazil. The paper observes the exposure to this psychiatric reform on outcomes related to public mental health such as the density of mental health professionals and psychiatric beds, outpatient production, public spending on mental health, and hospital admissions and deaths by cause. The reform introduces Psychosocial Care Centers (abbreviated as CAPS, for Centros de Atençao Psicossocial) as a community based substitute for inpatient care, as a way of increasing outpatient care in psychiatric health. In order to estimate its effect, the paper exploits the staggered implementation of CAPS across different municipalities in Brazil within a difference-in-differences framework, since it began in 2002 up until 2016. Nevertheless, not enough CAPS centers opened in 2002, the authors chose to use 2003 as the starting year of the study. This empirical framework is based on an underlying assumption: the parallel trends assumption. The latter states that in the absence of treatment, the evolution of the outcome variable follows the same trend for the treatment and control groups (municipalities). Concretely, if the reform had not been implemented, the differences in outcomes related to public mental health between each municipality would have remained constant overtime. In addition, our framework takes into account the staggered factor and adjusts its Two Way Fixed Effect estimator in order to eliminate any bias in case of heterogeneous treatment effect. Hence, they follow the method proposed by De Chaisemartin and D’Haultfoeuille (2020). Lastly, the standard errors are clustered at the municipality-level. The novelty of this paper is giving proof that government policies can positively influence mental health while replacing traditional psychiatric institutionalization by community-based centers. The CAPS reform is correlated with a decrease in the number of hospitalizations, notably for long-stay admissions and among patients suffering from schizophrenia. Also, the paper is embedded in the literature on the links between crime and mental health. Furthermore, they find that violent crimes, such as homicides, increased, which is consistent with the Penrose hypothesis according to them (inverse correlation between the number of psychiatric in-patients and the prison population). 5.3.3 Good practices 5.3.3.1 Setup Download and save the original dataset “final_dataset.dta” from https://www.openicpsr.org/openicpsr/project/195122/version/V1/view Each do file should begin with the clear all command. This command removes all data, programs and saved results from memory. It basically resets Stata’s environment in order to avoid errors caused by previous Stata sessions. It is usually followed by the command set more off to ensure that the script runs without interruption. clear all set more off The next step to start a do file is to specify the folder/directory you are working in by using the command cd. Watch out! For the access paths on Windows, / are replaced by \\\\. Set directory: cd &quot;C:\\Users\\OneDrive\\Documents\\SORBONNE\\COURS\\M2\\tdstata\\Replication&quot; Once the path has been created, you can open the dataset with the command use \"dataset\" followed by the option clearto clear the current dataset from memory before loading the new one. Open the dataset: use &quot;final_dataset.dta&quot;, clear global replication &quot;/Users/julianatorres/Desktop/M2 EDD/STATA SEMINAR/REPLICATION PROJECT/data_and_dofiles&quot; global results &quot;${replication}/results_replication&quot; global data_final &quot;${replication}/dataset_replication&quot; global data_original &quot;${replication}/dataset_original&quot; global adofile &quot;${replication}/stata-programs&quot; adopath ++ &quot;${adofile}&quot; With the command GLOBALS, we set up working directories that will indicate our working environment. We set up 5 globals: global replication: this global will indicate our working environment where all the other globals will be located (the main folder) global results: to store our outputs (tables, graphs, figures) global data_final : indicates where our new dataset is stored global data_original: indicates where the original dataset is stored global adofile : indicates where we have stored any adofiles Adofiles: Adofiles contain user written programs that have written commands that are not automatically built in Stata. When you use a program, you are able to define and run a chunk of code repeatedly if needed. Adofile is just the text file that contains a stata program. In order for stata to search for the adofiles needed to run our code, we set up our Stata adopath. The command adopath++ add an existing directory to the beginning of the search path stored in the global macro “adofile”. This add the directory specified in the macro “adofile” to the Stata adopath. 5.3.3.2 Necessary packages ssc install did_multiplegt ssc install Ftools ssc install Moremata ssc install Reghdfe ssc install Outreg2 ssc install coefplot ssc install grstyle 5.3.4 Descriptive Statistics One main concern for a difference-differences strategy is the pre-trends before the CAPS implementation. Table A2 gives us the mean outcomes for the hospitalizations, mortality and several health outcomes, comparing municipalities that have been treated (the ones that implemented a CAPS for the first time in the period 2003 - 2016) to the ones that have not. The baseline year is 2002. The matrix allows us to store the summary statistics, giving the variables (outcomes) in the rows and the results in columns. The local states where the results go in the matrix and the loop iterates the results for every variable. On average, before the implementation of any CAPS, the Brazilian municipality had similar characteristics (socioeconomic characteristics measured by sex, age and income) and similar healthcare systems levels (number of hospitalizations, mortality, outpatient care and mental health facilities). This is reassuring, given that no pre-trends divergence were detected with these statistics. use &quot;${data_final}/dataset_v1&quot;, clear Create new variables: generate (abbreviated to gen or g): used to create new variables. Condition if caps == 1: the new variable caps_implemented will store the year when the CAPS program was implemented in a given observation (in our case, in a municipality). Missing values: A common problem when creating new variables is the presence of missing values, noted (.) for numeric values and ““ for string values. Indeed, in caps_implemented, observations where caps =!1 will logically have missing values for the year (you can’t know the year of implementation if the CAPS hasn’t been implemented yet!). The problem is that missing values (.) are treated by default as the smallest possible numeric value. So we wouldn’t be able to calculate the minimum implementation year of CAPS (ie. the lowest value of caps_implemented). How to deal with missing values: recode: replaces these (.) with the numeric value 9999. This popular strategy ensures that we don’t take into account missing values (corresponding to non-implementation years in our new variable), since year-9999 is far too large to affect the minimum year. Create complex variables: egen introduces a mathematical or logical function - here the min() function - to create more sophisticated variables and can be combined with the by prefix. bysort (bys) groups the data by the variable id_muni (representing each municipality). We then calculate the minimum value of caps_implemented for each municipality (since each one can receive the treatment several times). As a result, the new variable year_caps_implemented stores the first year when CAPS was implemented in each municipality. Since non-implementation years were recoded as 9999, they will not affect the minimum calculation. Once our interest variable is created and the output checked (ex commands: browse, tabulate, describe), we can permanently delete (drop) caps_implemented which was only temporary and is no longer needed. Always keep your dataset clean! g caps_implemented = year if caps==1 recode caps_implemented (.=9999) bys id_muni: egen year_caps_implemented = min(caps_implemented) drop caps_implemented tab year_caps_implemented rowtotal: computes the sum of values across multiple variables for each row. egen pop_m =rowtotal(pop_m_10to19 pop_m_20to29 pop_m_30to39 pop_m_40to49 pop_m_50to59 pop_m_60to69 pop_m_70to79) egen pop_f =rowtotal(pop_f_10to19 pop_f_20to29 pop_f_30to39 pop_f_40to49 pop_f_50to59 pop_f_60to69 pop_f_70to79) egen pop_tot_10to79=rowtotal(pop_m pop_f) We now create variables for the total population per age range, in order to calculate their share in total population: g fracpop_m =pop_m/pop_tot_10to79 egen pop_tot_10to19 = rowtotal (pop_f_10to19 pop_m_10to19) egen pop_tot_40to49 = rowtotal (pop_f_40to49 pop_m_40to49) egen pop_tot_70to79 = rowtotal (pop_f_70to79 pop_m_70to79) g fracpop10to19 = pop_tot_10to19/pop_tot_10to79 g fracpop40to49 = pop_tot_40to49/pop_tot_10to79 g fracpop70to79 = pop_tot_70to79/pop_tot_10to79 5.3.4.1 Matrix creation matrix: We create a matrix and name it Q It includes 41 rows and 4 columns. So far, we only set its structure: values are considered as missing (.) matrix Q=J(41,4,.) 5.3.4.2 Descriptive statistics computation Macros and Loops: A local macro is like a shoe box where you can store variables, by category for instance. You can name this box so that you know what kind of variables are inside. This helps to keep your code as simple as clean! Command local A loop is a structure which repeats a set of commands multiple times, typically for different variables, values, or conditions. It is often combined with a macro where the variables are stored. It’s a really useful tool for repetitive tasks, thus simplifying your code! Command foreach By playing with those commands, we can build our summary statistics table by looping the variables in the respective macros. Macro i formats the rows of the table, and the loop helps to go from row to row in the same column. Macro v groups the variables by type, and the loop helps to operate the sum of these variables by observation according to a condition if. This allows us to calculate the means and store them in their respective columns. The macro includes the following categories of variables: (i) hospital admissions, (ii) mortality, (iii) ambulatory care, (iv) providers. We used a similar approach for the baseline covariates: Pop size, fraction of pop by gender and age, bolsa familia transfers, and gdp per capita. The conditions (if) for each column are the following: column 2: values in baseline year (2002): ALL the municipalities column 3: excludes values in 2002 and the missing values: TREATED municipalities column 4: the missing values excluding 2002: UNTREATED municipalities Computation and Input: Hospital admissions: local i 2 foreach v in ha_by_md ha_schizo2 ha_mood ha_stress ha_substance ha_demencia ha_retardation ha_md { local i `i&#39;+1 sum`v&#39; if year==2002 matrix Q[`i&#39;,2]=`r(mean)&#39; sum `v&#39; if year_caps_implemented!=9999 &amp; year_caps_implemented!=2002 &amp; year==2002 matrix Q[`i&#39;,3]=`r(mean)&#39; sum`v&#39; if year_caps_implemented==9999 &amp; year==2002 matrix Q[`i&#39;,4]=`r(mean)&#39; } Mortality: local i `i&#39;+2 foreach v in suicide overdose death_alcohol death_mental_health homicides { local i`i&#39;+1 sum `v&#39; if year==2002 matrix Q[`i&#39;,2]=`r(mean)&#39; sum`v&#39; if year_caps_implemented!=9999 &amp; year_caps_implemented!=2002 &amp; year==2002 matrix Q[`i&#39;,3]=`r(mean)&#39; sum `v&#39; if year_caps_implemented==9999 &amp; year==2002 matrix Q[`i&#39;,4]=`r(mean)&#39; } Ambulatory care: local i `i&#39;+2 // procedures preserve keep if year_caps_implemented&gt;2009 foreach v in op_by_psychiatrist op_by_psycho op_by_socialwork op_by_therap { local i `i&#39;+1 sum`v&#39; if year==2008 matrix Q[`i&#39;,2]=`r(mean)&#39; sum `v&#39; if year_caps_implemented!=9999 &amp; year_caps_implemented!=2002 &amp; year==2008 matrix Q[`i&#39;,3]=`r(mean)&#39; sum`v&#39; if year_caps_implemented==9999 &amp; year==2008 matrix Q[`i&#39;,4]=`r(mean)&#39; } restore // drugs foreach v in dispense_antipsychotics { local i `i&#39;+1 sum`v&#39; if year==2002 matrix Q[`i&#39;,2]=`r(mean)&#39; sum `v&#39; if year_caps_implemented!=9999&amp;year_caps_implemented!=2002&amp;year==2002 matrix Q[`i&#39;,3]=`r(mean)&#39; sum`v&#39; if year_caps_implemented==9999&amp;year==2002 matrix Q[`i&#39;,4]=`r(mean)&#39; } Providers: local i `i&#39;+2 foreach v in psychiatrist_tot psychologist_tot socialworker_tot therapist_tot psychiatric_beds { local i`i&#39;+1 sum `v&#39; if year==2006 matrix Q[`i&#39;,2]=`r(mean)&#39; sum`v&#39; if year_caps_implemented!=9999&amp;year_caps_implemented!=2002&amp;year==2006 matrix Q[`i&#39;,3]=`r(mean)&#39; sum `v&#39; if year_caps_implemented==9999&amp;year==2006 matrix Q[`i&#39;,4]=`r(mean)&#39; } Some baseline covariates: local i `i&#39;+2 // N local i `i&#39;+1 count if year==2010&amp;year_caps_implemented!=2002 matrix Q[`i&#39;,2]=`r(N)&#39; count if year==2010 &amp; year_caps_implemented!=9999 &amp; year_caps_implemented!=2002 matrix Q[`i&#39;,3]=`r(N)&#39; count if year==2010 &amp; year_caps_implemented==9999 &amp; year_caps_implemented!=2002 matrix Q[`i&#39;,4]=`r(N)&#39; // Pop size, fraction of pop by gender and age, bolsa familia transfers, and gdp per capita foreach v in pop_ fracpop_m fracpop10to19 fracpop40to49 fracpop70to79 log_pbf_pc loggdppc { local i `i&#39;+1 sum`v&#39; matrix Q[`i&#39;,2]=`r(mean)&#39; sum `v&#39; if year_caps_implemented!=9999 &amp; year_caps_implemented!=2002 matrix Q[`i&#39;,3]=`r(mean)&#39; sum`v&#39; if year_caps_implemented==9999 matrix Q[`i&#39;,4]=`r(mean)&#39; } 5.3.4.3 Replication of the Table A2 We finalize the preparation of the table by creating a dataset based on the matrix Q, formatting the rows and columns with descriptive labels, and exporting the resulting table to an Excel file. Remove any existing dataset from memory Create a new dataset with 41 observations (rows), corresponding to the rows in the matrix Q Convert the matrix into a dataset: by converting the matrix columns into variables (ie. dataset columns), with the option n(main) to name the new variables main1, … main4 Please note that you need to convert the variable main1 (numeric by default) into a string variable since we will assign it text as column 1 of the matrix contains the names of the variables corresponding to the summary stats Also, be careful to replace missing values (.) (numeric) in main1 with an empty string (““) After those settings, we can assign descriptive labels to each row of main1 Finally, we export the descriptive statistics table in excel clear set obs 41 svmat Q, n(main) keep main tostring main1, replace replace main1 = &quot;&quot; if main1==&quot;.&quot; replace main1 = &quot;Hospitalizations (per 10,000 pop.)&quot; in 2 replace main1 = &quot;Mental and behavioral disorders&quot; in 3 replace main1 = &quot;Schizophrenia&quot; in 4 replace main1 = &quot;Depression/bipolarity&quot; in 5 replace main1 = &quot;Stress-related disorders&quot; in 6 replace main1 = &quot;Psychoactive substance abuse&quot; in 7 replace main1 = &quot;Mental retardation&quot; in 8 replace main1 = &quot;Dementia&quot; in 9 replace main1 = &quot;Others&quot; in 10 replace main1 = &quot;Mortality (per 10,000 pop.)&quot; in 12 replace main1 = &quot;Mental disorders&quot; in 16 replace main1 = &quot;Homicide&quot; in 17 replace main1 = &quot;Suicide&quot; in 13 replace main1 = &quot;Overdose &quot; in 14 replace main1 = &quot;Alcoholic and chronic liver disease &quot; in 15 replace main1 = &quot;Outpatient Care (per 10,000 pop.)&quot; in 19 replace main1 = &quot;By psychiatrists (2008)&quot; in 20 replace main1 = &quot;By psychologists (2008)&quot; in 21 replace main1 = &quot;By social workers (2008)&quot; in 22 replace main1 = &quot;By occupational therapists (2008)&quot; in 23 replace main1 = &quot;Antipsychotic drugs&quot; in 24 replace main1 = &quot;Mental Health Facilities (per 10,000 pop.)&quot; in 26 replace main1 = &quot;Psychiatrists (2006)&quot; in 27 replace main1 = &quot;Psychologists (2006)&quot; in 28 replace main1 = &quot;Social workers (2006)&quot; in 29 replace main1 = &quot;Occupational therapists (2006)&quot; in 30 replace main1 = &quot;Psychiatric beds (2006)&quot; in 31 replace main1 = &quot;Municipality Characteristics&quot; in 33 replace main1 = &quot;Number of municipalities&quot; in 34 replace main1 = &quot;Population&quot; in 35 replace main1 = &quot;Men&quot; in 36 replace main1 = &quot;Age 10--19&quot; in 37 replace main1 = &quot;Age 40--49&quot; in 38 replace main1 = &quot;Age 70--79&quot; in 39 replace main1 = &quot;PBF per capita&quot; in 40 replace main1 = &quot;GDP per capita&quot; in 41 export excel using &quot;C:\\\\Users\\\\adamg\\\\OneDrive\\\\Documents\\\\SORBONNE\\\\COURS\\\\M2\\\\\\\\td stata\\\\Replication&quot;, replace 5.3.5 Empirical strategy: formalization of strategy and defining estimator The paper builds estimators adapted to treatment effect heterogeneity. Authors formalize the construction of the strategy by defining a causal parameter of interest in order to estimate the average treatment effects of the staggered implementation of a CAPS center across municipalities starting at year 2003. The empirical strategy’s main focus is to estimate B(k) the average treatment effect across all switchers (treatment groups municipalities) k periods after each switcher received the treatment. When we refer to “receiving the treatment” it means when switchers implemented a CAPS for the first time. Authors first define different cells (m,t) that design a municipality m at year t. Dm,t denotes our treatment. This dummy variable takes the value of 1 if the municipality m gained a CAPS for the first time at year t and 0 otherwise. This means that for any year before t, this variable should be 0. Hence, for any consecutive years t-1 and t, Dm,t-1 = 0 and Dm,t = 1. Authors then define different sets S(k) and S(t,k) and C(t,k): S(k) denotes the set of treated municipalities (switchers) observed k periods after their treatment year t (t+k ≤ 2016). There are different sets of S(k) depending on the value of k S(t,k) denotes the set of municipalities that received a CAPS center for the first time at the same year t and observed k periods after treatment. There are different sets of S(t,k) depending on the combination of (t,k). C(t,k) denotes the set of municipalities that are not treated at year t+k. Under the DiD framework and its identifying assumption, the outcome of the non-treated municipalities (non-switchers) can be used as a counterfactual when observing the evolution of the switchers. Since the paper focuses on the effect of CAPS implementation observed at year t+k, authors establish that: For all municipalities part of S(t,k), Dm,t-1 = 0 and Dm,t = … Dm,t+k = 1. For all municipalities part of C(t,k) Dm,t-1 = 0 and Dm,t = … Dm,t+k = 0. Consequently, for a fixed event-time k ≥ 0 authors build a 2x2 DID estimator for the cohort of municipalities that implemented a CAPS at t for the first time: Assuming that C(t,k) and S(t,k) would have evolved in parallel in absence of treatment, this estimator aims to estimate the average treatment effect for municipalities that implemented a CAPS at period t, observed k periods later. DID(t,k) denotes the difference in evolution of mean outcome between two sets of groups: the set of municipalities that received the treatment at period t (t ≤ 2016 - k) and the set of municipalities that remained untreated until t+k. By establishing DID(t,k), authors now define the estimator of B(k), which is a weighted average of all the DID(t,k) estimators: The weight set for each DID(t,k) estimator depends on the size of each treated cohort. 5.3.6 Main results The CAPS reform had a major impact on the supply of professionals, findings show an increase of 0.84 professionals per 10,000 people. The major contribution of the CAPS reform is on outpatient care. All else being equal, there is an average increase of 264.2 procedures per 10,000 people per year. All else being equal, there is an average decrease of long stay hospital stays per 10,000 people per year. No significant effect is found on mortality rates. (cf. event studies) Also, all else being equal, there is an average decrease in long-stay hospital admission. Finally, in terms of homicide, the reform can be correlated to an increase of 0.149 deaths per 10,000 people on average. figure5_panelB 5.3.7 Code replication This code is used to reproduce all main outcomes in forms of figures and tables. We will only reproduce the impact of the CAPS reform on the number of long stay admissions in hospitals that is presented in the form of an event study. To replicate this outcome, we will need to estimate the dynamic treatment effects, the average treatment effect and multiple placebo tests to see the treatment effect before the CAPS reform was implemented and define the baseline value ong long stay hospital admissions for the baseline year which is 2002. In order to estimate the effect, we will use the command program. A program in stata is a subroutine that allows you to run a chunk of code all at once and repeatedly. We use this because in order to estimate all of the elements mentioned above, the authors needed to run all of those estimations for multiple groups (switchers) and multiple time periods for each of the primary and secondary outcome. Authors used the the commands from the user written program did_multiplegt redacted by Chaise et d’Hautefeuille (2022). This package sets up a list of commands that generalizes the standard DID estimator with two groups, two periods and a binary treatment to situations with many groups, many periods and a potentially non-binary treatment. For each pair of consecutive time periods t-1 and t and for each value of the treatment d, the command computes a DID estimator comparing the outcome evolution among the switchers, the groups whose treatment changes from d to some other value between t-1 and t, to the same evolution among control groups whose treatment is equal to d both in t-1 and t. The did_multiplegt (DIDM) estimator is then equal to the average of those DIDs across all pairs of consecutive time periods and across all values of treatment. Under a parallel trends assumption, DIDM is an unbiased and consistent estimator of the average treatment effect among switchers, at the time period when they switch. (source: did_multiplegt adofile) When a program is ran, it’ll run the chunk of code that is inside the program 5.3.7.1 Set up program cap program drop did program did, rclass args outcome group time treatment cluster dynamic placebo bootstrapreps textx texty ylow ydelta yup year Command caps program drop did : this is to drop any previous program named did that was installed or that was previously run Commands program did, rclass : this designates the start of the program did that was redacted by the authors using the commands made available in the package did_multiplegt. The rclass command is used so that results are stored temporarily in r(), a storage area for “return results”. This usually stores results in matrices, macros, or scalar so that they can be accessed immediately after running the code. Command args : This command is used to set up the arguments that will later on be referred to in the code inside the program did. This command defines a list of local macros that are referenced when the did program is ran. These macros capture inputs passed to the did program. In our case, these arguments have multiple tasks such as : establishing the program’s configuration like the variables that are needed for the estimation (treatment, group, time, outcome), the number of dynamic and placebo periods, how many bootstrap repetitions and also for defining the the settings of our matrices and our graphs. 5.3.7.2 Set up Matrix that will store results //-------------------------------------------------------------------- // SET UP OF THE MATRIX THAT WILL STORE THE RESULTS //-------------------------------------------------------------------- * number of rows of matrix that will store results local row= `placebo&#39;+`dynamic&#39;+1 * number of columns (event-study, upper bound of CI, lower bound of CI, se, N, N of switchers) loc column 6 * label local labelrow forvalues i=-`placebo&#39;/`dynamic&#39;{ local labelrow: display &quot;`labelrow&#39; `i&#39;&quot; } * Sequence of placebos starting at 1 and going up local placeboup forvalues i=1/`placebo&#39;{ local placeboup: display &quot;`placeboup&#39; `i&#39;&quot; } * Sequence of placebos starting at 1 and going down local placebodown forvalues i=`placebo&#39;(-1)1{ local placebodown: display &quot;`placebodown&#39; `i&#39;&quot; } * *Defining Controls global X loggdppc log_pbf_pc pop_m_10to19 pop_m_20to29 pop_m_30to39 pop_m_40to49 pop_m_50to59 pop_m_60to69 pop_m_70to79 pop_f_10to19 pop_f_20to29 pop_f_30to39 pop_f_40to49 pop_f_50to59 pop_f_60to69 pop_f_70to79 mhpf02trend estab02trend popruraltrend analfabs_2000trend sharepobres2000trend theil2000trend lnsaudepctrend poptotaltrend temperaturetrend municipality_areatrend distance_to_capitaltrend altitude_100trend rainfalltrend lnpbfpctrend lnpibpctrend For example for the command local row= placebo'+dynamic'+1 it alludes to the macros placebo' and`dynamic' that when the code will be run, these macros will be replaced by the values stored in them. It will display a row for each placebo period and a row for each dynamic (future) period, plus 1 additional row. Now we use a combination of local macros and loops in order to define the setting of the matrix that will store the results. We create macros that are initially empty, and we loop in the values “i” that are defined by the eachvalue command so that the loop will generate the numbers from these variables. For example, for the local labelrow, we use a loop that iterates all values going from -placebo to dynamic. If -placebo = -3 and dynamic = 3 then local labelrow will store values -3, -2, -1 0 ,1 ,2, 3. The display command integrate the values of i into the local “Labelrow”, separated by a space. This is done for the local placeboup that gives you the sequence of placebos from 1 and up and the local placebodw that gives the sequence of placebos from 1 and down. Command global : Here the command global defines a GLOBAL MACRO. This is a macro that can be accessed anywhere in the Stata. It is different than the local macros since they are only accessible within the scope where they are defined (a program for example). We create then a global “X” in order to store all of the control variables that we will use in our regression. This way we don’t have to write them all over again but we can just refer to our global in order to take them into account. 5.3.7.3 Defining our DiD estimator : package did_multiplegt *** In this part we set up the DiD estimator using the commands from package did_multiplegt from CHAISE &amp; D&#39;HAUTEFEUILLE (AER, 2022). * Dta use &quot;${data_final}/dataset_v1&quot;, clear * keeping what is relevant keep if year&gt;=`year&#39; keep `treatment&#39; `group&#39; `time&#39; id_state `cluster&#39; `outcome&#39; ${X} * generating the year in which the treatment starts g `treatment&#39;_implemented=year if `treatment&#39;==1 bys id_muni: egen year_`treatment&#39;_implemented=min(`treatment&#39;_implemented) g eventtime`treatment&#39;=year- year_`treatment&#39;_implemented drop year_`treatment&#39;_implemented `treatment&#39;_implemented We open our dataset dataset_v1 Now we build our DiD estimator using the commands from the did_multiplegt program from Chaise et D’hautefeuille. We use the command keep if to only keep in our estimator the arguments and variables that are needed for our results Command g used to generate a variable which designates the year the treatment was implemented Command BYS: this designates that we want the year the treatment was implemented for each municipality (id_muni). We calculate a new variable with EGEN which shows the first year the treatment was implemented for each municipality by using the previously created variable treatment'_implemented and calculating the minimum value of treatment'\\_implemented for each municipality. We then generate a new variable using year_treatment'_implemented which captures the amount of time the treatment has been active  this is defined by doing the difference between the year of the study and the year the treatment was implemented for the first time. With the command g (generate) we create the variable eventtime_treatment. Finally we drop all previously created variables except for eventtime treatment. * estimator did_multiplegt `outcome&#39; `group&#39; `time&#39; `treatment&#39;, trends_nonparam(id_state) controls(${X}) placebo(`placebo&#39;) dynamic(`dynamic&#39;) breps(`bootstrapreps&#39;) cluster(`cluster&#39;) covariances average_effect(simple) We set up our estimator where we set up our main parameters by referencing the local and global macros that we defined before. This estimator includes controls for covariates, placebo tests, accounts for clustered standard errors and estimates the average treatment effect and dynamic treatment effects. We also take into account covariances between different groups and periods in order to test the robustness of the estimation as well as taking into account for group specific trends at the state level (id_state). di &quot;cheguei aki?&quot; * This is a checkpoint to see that there aren&#39;t any bugs or errors command di : “di” stands for debugging. This command is used in order to make sure that there are no bugs or errors in your code. This is a kind of check point to show that up until here, no bugs or errors show. 5.3.7.4 Construction of matrices to store final results ****In this part we build the matrix storing results // Empty matrix to be filled matrix Q=J(`row&#39;,`column&#39;,.) // Label rows matrix rown Q = `labelrow&#39; //Add effects, ci, se, n, n of switchers forvalues i=0/`dynamic&#39;{ local m &quot;`i&#39;+`placebo&#39;+1&quot; local row: display `m&#39; matrix Q[`row&#39;,1]=`e(effect_`i&#39;)&#39; matrix Q[`row&#39;,2]=`e(effect_`i&#39;)&#39;+`e(se_effect_`i&#39;)&#39;*1.96 matrix Q[`row&#39;,3]=`e(effect_`i&#39;)&#39;-`e(se_effect_`i&#39;)&#39;*1.96 matrix Q[`row&#39;,4]=`e(se_effect_`i&#39;)&#39; matrix Q[`row&#39;,5]=`e(N_effect_`i&#39;)&#39; matrix Q[`row&#39;,6]=`e(N_switchers_effect_`i&#39;)&#39; } We define a Matrix Q that stores values for estimating average treatment effects, placebo tests and dynamic treatment effects. We also build variance-covariance matrices for the placebo tests and dynamic effects in order to assess the statistical inference of the results as well as their robustness. We use a loop with the forvalues to repeat the set of commands for time periods going from 0 to the value of the macro dynamic. For each time period i (for each iteration), we store the estimated treatment effect in column 1, the upper and lower bound of the 95% confidence interval of this coefficient in column 2 and 3, colmun 4 stores the clustered standard errors, column 5 stores the number of observations for time period i and column 6 presents the number of switchers going from untreated to treated in time period i. //Add placebos, ci, se, n local up `placeboup&#39; //xxixx local down `placebodown&#39; //xxixx local n : word count `up&#39; forvalues i = 1/`n&#39; { local u : word `i&#39; of `up&#39; local d : word `i&#39; of `down&#39; matrix Q[`u&#39;,1]=`e(placebo_`d&#39;)&#39; matrix Q[`u&#39;,2]=`e(placebo_`d&#39;)&#39;+`e(se_placebo_`d&#39;)&#39;*1.96 matrix Q[`u&#39;,3]=`e(placebo_`d&#39;)&#39;-`e(se_placebo_`d&#39;)&#39;*1.96 matrix Q[`u&#39;,4]=`e(se_placebo_`d&#39;)&#39; matrix Q[`u&#39;,5]=`e(N_placebo_`d&#39;)&#39; } We then add to our Matrix Q the placebo tests. We set up macros to define the number of placebo periods and then we use a loop to estimate the treatment effect, 95% confidence interval, standard errors and number of observations for each of the placebo periods. * Build Variance-Covariance matrix for placebos matrix COVQp = J(`placebo&#39;,`placebo&#39;,.) matrix COVQp[1,1]=`e(se_placebo_1)&#39;^2 forvalues i = 2/`placebo&#39; { matrix COVQp[`i&#39;,`i&#39;]=`e(se_placebo_`i&#39;)&#39;^2 forvalues j=1/`=`i&#39;-1&#39; { matrix COVQp[`i&#39;,`j&#39;]=`e(cov_placebo_`j&#39;`i&#39;)&#39; matrix COVQp[`j&#39;,`i&#39;]=`e(cov_placebo_`j&#39;`i&#39;)&#39; } } * Vector of placebo coef matrix Qp = J(`placebo&#39;,1,.) forvalues i=1/`placebo&#39;{ matrix Qp[`i&#39;,1]=`e(placebo_`i&#39;)&#39; } * F test: all placebos=0 matrix COVQp_inv=invsym(COVQp) matrix Qp_t=Qp&#39; matrix U_g=Qp_t*COVQp_inv*Qp scalar statistic_joint=U_g[1,1]/`placebo&#39; scalar p_joint=1-chi2(`placebo&#39;,U_g[1,1]) *Build Variance-Covariance matrix for dynamic effects matrix COVQd = J(`dynamic&#39;,`dynamic&#39;,.) matrix COVQd[1,1]=`e(se_effect_1)&#39;^2 forvalues i = 2/`dynamic&#39; { matrix COVQd[`i&#39;,`i&#39;]=`e(se_effect_`i&#39;)&#39;^2 forvalues j=1/`=`i&#39;-1&#39; { matrix COVQd[`i&#39;,`j&#39;]=`e(cov_effects_`j&#39;`i&#39;)&#39; matrix COVQd[`j&#39;,`i&#39;]=`e(cov_effects_`j&#39;`i&#39;)&#39; } } Finally we use forvalue loop command and local macros to create a variance-covariance matrix for the placebo tests, we estimate an F-test using the variance-covariance to make sure that all of the placebo coefficients come up to 0 and finally, we create another variance-covariance matrix to assess the precision of our estimated dynamic effects as well as the statistical inference of our estimates. 5.3.7.5 Calculation of average treatment effect * Avg effects // long run dynamic scalar controls_effect = `e(effect_average)&#39; // point estimate scalar controls_effect_se = `e(se_effect_average)&#39; // se // short run dynamic scalar controls_effect_short=(1/(3))*(e(effect_0)+e(effect_1)+e(effect_2)) // point estimate scalar controls_effect_short_se=sqrt((1/(3^2))*(COVQd[1,1]+COVQd[2,2]+COVQd[3,3]+2*COVQd[1,2]+2*COVQd[1,3]+2*COVQd[2,3])) // se * Avg placebo effects scalar controls_placebo = 0 forvalues i=1/`placebo&#39; { scalar controls_placebo=controls_placebo+(`e(placebo_`i&#39;)&#39;) // summing all placebos } scalar controls_placebo=controls_placebo/`placebo&#39; // point estimate mata : st_numscalar(&quot;controls_placebo_se&quot;, (1/(`placebo&#39;^2))*sum(st_matrix(&quot;COVQp&quot;))) scalar controls_placebo_se=sqrt(controls_placebo_se) // se We now store the different estimations and values in scalars. The command scalarcreates a variable that stores numeric values. They are commonly used to hold results of estimations in programs and do-files. We create scalars to store the long run average treatment effect and its standard errors and short run average treatment effect and its standard errors. Furthermore, we also define scalars for the average placebo effects which is done by summing all of the coefficients and dividing my the number of placebo periods. Finally, we calculate the standard errors for the placebo average treatment effect. 5.3.7.6 Construction of tables and event study In this part, we store the results in a table format. Even though we do not replicate a table, it is necessary to go through this part in order to plug in the results in the table into the even study. The event study is a visual support. This is how it is done by authors so we decided to follow their same methodology. * event-study svmat Q, n(controls) rename controls1 controls_b // point estimate rename controls2 controls_ciup // upper CI rename controls3 controls_cidw // lower CI rename controls4 controls_se // SE rename controls5 controls_n // N rename controls6 controls_nswitch // N of swithcers * long run avg dynamic gen controls_effect = scalar(controls_effect) in 1 // point estimate gen controls_effect_se = scalar(controls_effect_se) in 1 // SE * short run avg dynamic gen controls_effect_short = scalar(controls_effect_short) in 1 // point estimate gen controls_effect_short_se = scalar(controls_effect_short_se) in 1 // SE * avg placebo gen controls_placebo = scalar(controls_placebo) in 1 // point estimate gen controls_placebo_se = scalar(controls_placebo_se) in 1 // SE * baseline mean summ `outcome&#39; if eventtime`treatment&#39; &gt;= -`placebo&#39; &amp; eventtime`treatment&#39; &lt;= -1 g mean_baseline = `r(mean)&#39; in 1 // mean g se_mean_baseline = `r(sd)&#39; in 1 // sd * f test: all placebos =0 g f_test = statistic_joint in 1 // statistic g pvalue_f_test= p_joint in 1 // p value * saving dataset with the results keep controls* mean* se_mean_baseline f_test pvalue_f_test keep if controls_b!=. save &quot;${tables}/t`treatment&#39;`outcome&#39;r.dta&quot;, replace We generate new variables from the results stored in the Matrix Q. More specifically, we generate variables for the estimate of the treatment effect, the upper and lower bounds of the confidence interval, the standard errors of the estimation, the number of observations and the number of switchers. We then generate variables to store the long run average dynamic effect, the short run average treatment effect and the placebo treatment effect results. Finally, we keep all variables that are relevant to our estimation, specially all of the ones that start with `controls_b`and export the dataset. //------------------------------------------------------------------------- // GRAPH //------------------------------------------------------------------------- *** here we export our results in a event study graph local b: display %4.3f controls_effect local se: display %4.3f controls_effect_se local bpl: display %4.3f controls_placebo local sepl: display %4.3f controls_placebo_se local meany: display %4.2f mean_baseline local pftest: display %4.2f pvalue_f_test coefplot (matrix(Q[,1]) , ci((Q[,2] Q[,3])) pstyle(p1) msymbol(O)) /// , yline(0, lcolor(red)) xline( `=`placebo&#39;+1&#39;, lcolor(gs7) lpattern(dash)) vertical ytitle(&quot;Treatment Effect&quot;) xtitle(&quot;Time Since Treatment&quot;) ciopts(recast(rcap)) /// text(`textx&#39; `texty&#39; &quot;Average Treatment Effect: `b&#39; (`se&#39;)&quot; &quot;Average Placebo Effect: `bpl&#39; (`sepl&#39;)&quot; &quot;Baseline: `meany&#39;&quot; &quot; &quot; &quot;Joint significance of placebo effects: `pftest&#39;&quot; , place(e) just(left) size(small)) ylabel(`ylow&#39;(`ydelta&#39;)`yup&#39;) graph export &quot;${figures}\\f`treatment&#39;`outcome&#39;r.pdf&quot;, as(pdf) replace graph save Graph &quot;${results}/f`treatment&#39;`outcome&#39;r.gph&quot;, replace end Finally we format our variables to be able to create event study visuals. We then set up local macros to store the previously created variables and we use the package coefplotto plot our graphs. command end: this command designates that the program is over. 5.3.7.7 DiD estimation Now when we will run our DiD regression, it will run all the lines of code stored in the program for each outcome. We only ran this for one outcome but it is meant to be done for all 5 types of outcomes in order to generate all tables and event study graphs. foreach youtcome in longstay_ha_by_md { di &quot;`youtcome&#39;&quot; qui did `youtcome&#39; id_muni year ca id_muni 5 5 100 -1 1 -2 0.5 1 2002 } We finally run our DiD estimator for all groups and all time periods. Our estimator refers to each of the arguments that we define at the beggining of our program args outcome group time treatment cluster dynamic placebo bootstrapreps textx texty ylow ydelta yup year. So in this case, youtcome is outcome, group is id_muni and so on so forth. 5.3.8 Robustness Check One main concern for this paper to be empirically valid is that some other policies have been impulsed in the 2000s in Brazil, notably the famous PBF and Family Health Program (Programa Saude da Familia, PSF). In the robustness section, the goal is thus to make sure that those policies did not also impact our outcomes, since it would create an upward bias. They conduct several test, goal is to know if the adoption of both programs were correlated. 5.3.8.1 Figure A8 - Correlation between the PSF and the CAPS programs adoption g treatment_year_psf=year if psf_imp==1 g treatment_year_caps=year if caps==1 bys id_muni: egen min_treatment_year_psf=min(treatment_year_psf) bys id_muni: egen min_treatment_year_caps=min(treatment_year_caps) drop if min_treatment_year_psf==2002 drop if min_treatment_year_caps==2002 duplicates drop id_muni, force We create new variables using the command generate to know if one or both of the progams have been implemented in the municipality and then, using egen we can get the earliest years when it has been, respectively for PSF and CAPS. We thus use bys to have an outcome for each municipality. We also make sure that we only have one observation by municipality doing a duplicates drop. We drop the observations before 2003, as 2002 is excluded from the analysis. scatter min_treatment_year_psf min_treatment_year_caps /// lfit min_treatment_year_psf min_treatment_year_caps /// lfit min_treatment_year_psf min_treatment_year_caps [aweight=poptotal], xlabel(2003(3)2016) ylabel(2003(3)2016) /// ytitle(&quot;PSF Adoption&quot;) xtitle(&quot;CAPS Adoption&quot;) title(&quot;&quot;) note(&quot;&quot;) legend(col(2) nobox region(lstyle(none)) /// order(2 &quot;Linear fit&quot; 3 &quot;Linear fit with population weights&quot;) on) /// graph export &quot;${Figs}\\psfcorrelation.pdf&quot;, as(pdf) name(&quot;Graph&quot;) replace Finally, we create a scatter plot to show the correlation between the adoptions of both programs. Hence, we use the command scatter. By specifying lfit, we create a standard linear fit line and another that is weighted by the variable poptotal. The command legend (col(2)) adds a legend to explain what those two lines are. We use xlabel(2003(3)2016) to have the range of years we are studying with a 3-year interval. The graph export is followed by our global so that we make sure that our graph is downloaded in the right file. Authors: Adam Guerfa, Léonie Patin, Juliana Torres Cortes, students in the Master program in Development Economics and Sustainable Development (2023-2024), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2024 "],["synthetic-control-method.html", "Chapter 6 Synthetic Control Method ", " Chapter 6 Synthetic Control Method "],["effects-of-copyrights-on-science-evidence-from-the-wwii-book-republication-program.html", "6.1 Effects of Copyrights on Science: Evidence from the WWII Book Republication Program", " 6.1 Effects of Copyrights on Science: Evidence from the WWII Book Republication Program Biasi, Barbara, and Petra Moser. 2021. “Effects of Copyrights on Science: Evidence from the WWII Book Republication Program.” American Economic Journal: Microeconomics, 13 (4). https://www.aeaweb.org/articles?id=10.1257/mic.20190113 [Download Do-File corresponding to the explanations below] [Download second Do-File corresponding to the explanations below] [Download Codebook] [Link to the full original replication package paper from Open ICPSR] Highlights This paper examines the impact of copyrights on scientific progress by analyzing an exogenous shift in copyright policy during World War II. The authors rely on a triple-difference approach by comparing the differential change in citations to BRP books by English-language vs. non-English-language authors with the same differential change for Swiss books. Using citations as a proxy for knowledge creation, the paper studies the effect of how much new knowledge is built using existing works. This methodology is standard as it is a diff-in-diff approach. This document offers a comprehensive explanation of the original replication package while also providing insights into: How to set up global macros to manage file paths efficiently and to download data sets using them. Creating new variables to facilitate analysis and reshaping data from wide format to long format for easier analysis using reshape long. Generating and customizing graph appearance, such as colors, titles, and labels, for clarity and aesthetics. Performing Ordinary Least Squares (OLS) regressions with fixed effects and interaction terms (e.g., c.english#c.post). Storing and summarizing regression results using eststo and estadd. You will learn how to do tables with the fixed effectsrows. Using esttab to create and export regression tables with customized statistics. Understanding and applying triple-difference analysis, including matching and synthetic control methods. Conducting loops using the foreach command. 6.1.1 Getting Started First have to prepare your Stata environment and download your dataset. We must make sure that Stata is cleared of any existing data and that you have a smooth execution of your script without interruptions. clear set more off You should install the following libraries : ssc install estout ssc install reghdfe ssc install ftools ssc install carryforward ssc install synth Then you have to set up your working directory and define your global macros that will help you manage the paths in your script and reduce chances of committing errors while writing them. The following are the global macros used. Make sure to change “YOUR PATH” to your own paths. global Rawdata &quot;C:YOUR PATH\\raw data&quot; global Figs &quot;C:YOUR PATH\\figs&quot; global Prog &quot;C:YOUR PATH\\prog&quot; global Tables &quot;C:YOUR PATH\\tables&quot; A simple trick is to define at the beginning of your do-file your table options so that they can be recalled faster when exporting your regression tables. The bellow code defines a global macro that uses the labels instead of names since they are more comprehensible. The regression coefficients and the standard deviations are rounded to the third decimal. The level of significance is determined by the stars where 10% significance is denoted by one star*, 5% significance is denoted by two stars** and 1% significance is denoted by three stars***. global options = &quot;label b(3) se(3) starlevel(* .1 ** .05 *** .01)&quot; Now download the dataset using the global macro defined above through: use &quot;${Rawdata}/brp_dataset&quot;, clear Now drop variables you don’t need and then save and use the cleaned dataset drop p_original p_reproduction name title publisher licensee nuc emigre emigre_us text ads save &quot;${Rawdata}/simplified_dataset&quot;, replace use &quot;${Rawdata}/simplified_dataset&quot;, clear 6.1.2 Background elements: This paper investigates the impact of copyrights on science. Before the BRP, German-owned copyrights were protected under U.S. copyright law for 56 years, aligning with the 1909 Copyright Act which led U.S. researchers to depend heavily on German books for advancing research in fields like mathematics, chemistry, and physics. Driven by a desire to reduce payments to Nazi Germany and improve U.S. access to critical scientific resources, President Roosevelt authorized the Alien Property Custodian to seize enemy-owned copyrights and patents. As a result, the 1942 U.S the BRP (Book Republication Program) was launched, leading to a significant reduction (25%) in book prices through the reprinting science books owned by enemy nations. This initiative not only made scientific literature more affordable but also fostered competition by limiting licenses to six months, effectively breaking monopolies and encouraging a more competitive market for scientific books. To assess the program’s impact, a triple-difference approach was employed, comparing how citations to BRP books changed for English-speaking authors (who benefited from the program) versus non-English-speaking authors, while also comparing these changes to citations of Swiss books (control group), which were unaffected by the program, to isolate the program’s impact. The policy not only made scientific knowledge more accessible by reducing costs and increasing availability, but it also underscored the trade-offs in copyright laws, where promoting innovation through financial incentives can, at times, limit access to existing knowledge, ultimately affecting societal well-being. 6.1.3 Main strategies explanation: Difference-in-Differences and Synthetic Control Method 6.1.3.1 Difference-in-Differences In econometrics, one of our greatest concerns is omitted variable bias (OVB). OVB arises when there are variables that are correlated with both the dependent variable (Y) and the independent variable of interest (X), but are not included in the regression model. This can lead to biased and inconsistent estimators, as the relationship between X and Y may be confounded by these omitted variables. The presence of OVB threatens the validity of causal inferences because the estimated effect of X on Y may be distorted due to the influence of these unobserved variables. The endogeneity problem arises here when the explanatory variable is correlated with the error term, often due to omitted variables, reverse causality, or simultaneous causality. So, to address this OVB problem, the ideal solution is randomization. In a controlled experiment, randomization ensures that all variables (both observed and unobserved) are equally likely to affect both the treatment (X) and the outcome (Y). But as we rarely are able to conduct an economic experiment for randomization, we have methods that mimic it. One of these is the DID. The Difference-in-differences method (DID) is a quasi-experimental technique used to mimic randomization. It’s a causal inference approach to evaluate the effects of a treatment or policy intervention by comparing changes in outcomes over time between a treatment group and a control group. To apply DID, researchers must first define a treatment group that is exposed to the intervention and a control group that remains unaffected. The method focuses on the changes in outcomes over time within each group. Controlling for time-invariant confounding factor is its primary advantage. The DID Estimate calculates the causal effect of a treatment by measuring how much the outcome for the treatment group changed from before to after the treatment, and then subtracting the change observed in the control group over the same period. The key assumption is the parallel trends assumption. This assumption states that, in the absence of the treatment, the outcomes for both the treatment and the control group would have followed similar trends over time.Thus, any difference observed between the two groups after the treatment can be attributed to the intervention itself, rather than other confounding factors. In other words, if no intervention had occurred, both groups would have evolved similarly in terms of their outcomes.By assuming parallel trends, we control for any unobserved factors that may affect both groups thereby addressing the issue of endogeneity. In this paper, the authors employ triple difference method DDD, which is an extension of DID. The DDD method helps to address potential violations of the parallel trends assumption in a standard DID framework and provides additional robustness by using a second control group or an extra dimension of variation. In this study, we can see that DDD isolates the impact of the Book Republication Program (BRP) by comparing three dimensions. First, it examines changes in citations to BRP books before and after the program (time dimension). Second, it compares how these changes differ between English-speaking authors (who benefited from the program) and non-English-speaking authors (group dimension). Third, it contrasts these changes with citations to Swiss books, which were unaffected by the BRP (comparison dimension). By combining these three layers, the DDD approach ensures that the observed increase in citations for English-speaking authors is specifically due to the BRP, accounting for broader trends in citations or external factors unrelated to the program. This method provides a robust estimate of the BRP’s effect on improving access to scientific knowledge. 6.1.3.2 Synthetic Control Method The Synthetic Control Method (SCM) serves as another powerful quasi-experimental technique used to address causal inference challenges, particularly when randomized controlled trials are not feasible. SCM constructs a synthetic control unit that mimics the characteristics of a treatment unit based on pre-treatment data. The key idea is to use a weighted combination of control units that best represent the counterfactual scenario—what would have happened to the treated unit in the absence of the treatment. By comparing the actual outcomes of the treated unit with the synthetic control, SCM estimates the causal impact of the intervention while controlling for unobserved confounding factors. The primary assumption underlying SCM is the “parallel trends” assumption, similar to that in DID. It asserts that, in the absence of the treatment, the treated unit and the synthetic control would have followed similar trends over time. Thus, any differences observed between the treated unit and the synthetic control after the treatment can be attributed to the treatment itself, rather than other external factors. In this study, the authors employ SCM as robustness checks to their Mahalanobis propensity score matching to estimate the impact of the Book Republication Program (BRP). By constructing a synthetic control that combines control units based on relevant pre-treatment characteristics, the authors construct a “synthetic control” for each BRP book by estimating the weighted sum of Swiss books in the same field (math or chemistry). This allows them to isolate the effect of the BRP by accounting for broader trends and potential confounders that might otherwise distort causal inference. To address the endogeneity problem, the authors control for observed confounders and employ several fixed effects account for time-invariant unobserved factors that may be influencing both the treatment and outcome. Additionally, they apply rigorous robustness checks to ensure the validity of their estimates. So, they utilize the Synthetic Control Method (SCM). 6.1.4 Figure 1 : Citations to BRP Books from New Work in English versus Other Languages 6.1.4.1 Preparation of the Data In order to create the first figure, we need to prepare our data. In this step, we will demonstrate how to create a unique identifier and reshape the data from a wide format to a long format. We have two variables representing the number of citations for each book, separated by whether the citations are from English-speaking authors count_eng or non-English-speaking authors count_noeng. Our goal is to consolidate this information into a single variable that reflects the total number of citations per book per year, along with a dummy variable English indicating whether the citation came from English-speaking authors or not. To achieve this, we generate two new variables: count1 for citations from English authors and count0 for citations from non-English authors. These are computed by subtracting the count_eng from the total number of citations. We then use fakeid as an identifier to reshape our data from wide format to long format. Here, we encounter that the data is in a “wide” format where multiple observations are spread across columns rather than rows. To reshape this data into a more useful form, we introduce a unique identifier, fakeid which combines the id (book identifier) and year_c (year of citation). This fakeid ensures that we track each book’s data across years, allowing us to group observations correctly. Reshaping the data from wide to long format is then necessary to combine related observations into a single record, that’s why we run the reshape long command, with i(fakeid) specifying the unique identifier and j(english) indicating the variable that identifies whether the citation came from English (count_eng) or non-English (count_noeng) author, once the two variable english and count created you can label them. gen count1 = count_eng gen count0 = cit_year - count_eng replace count0 = 0 if count0 &lt; 0 //ensure that there are no negative values egen fakeid = group(id year_c) reshape long count, i(fakeid) j(english) egen field_gr = group(field) label var english &quot;English&quot; label var count &quot;count of citations&quot; 6.1.4.2 Replication of the figure the following code is being used: The sort command organizes the dataset by the variable year_c (the year of citation). Sorting ensures that the data is in chronological order. This step is important for visualizing the data because graphs rely on properly ordered time series to make sense. And to saves a temporary version of the dataset, preserve helps us in changing the data. Any changes made after this point won’t affect the original data. And later, we can use the restore command get back the initial dataset. To create this graph,we’re interested in years after 1929, when the reform is being established, and that have citations to BRP Books . Thus, We filter the data keeping only rows where the year is later than 1929 by keep if year_c &gt; 1929 that helps in doing this step. Then we drop if brp == 0 to remove rows where brp (Book Republication Program indicator) equals 0, focusing only on data related to books affected by the BRP. The collapse (mean) command aggregates the data by averaging the variable count for each combination of year_c, english and post (whether the citation occurred after the BRP). This step simplifies the dataset by condensing it into one row per combination of these variables, making it easier to analyze and plot trends over time. The graph is created using the twoway command, to plot two lines. One solid black line of medium width representing citation counts for English-speaking authors (if english == 1), the second line is blue , dashed and of medium width (lcolor(black) lpattern(dash) lwidth(medium)), it represents citation counts for non-English-speaking authors (if english == 0). A vertical dashed line is added at the year 1942 with xline, showing the year of the reform, the start of the BRP. The legend clearly distinguishes the two lines, with labels “English” and “Other,” positioned in the top-right corner without an outer ring legend(order(1 \"English\" 2 \"Other\") pos(11) ring(0)). Once you have indicated all elements to draw in the twoway graph, you can customize it by adding a comma after each variable and write the design options (style, pattern and color of the line).then another comma and put the options of the graph: its title, y and x-axis titles, legend settings, and width. For example: The y-axis features here labels from 0 to 0.8 in intervals of 0.2 ylabel(0(0.2)0.8), while the x-axis features labels from 1930 to 1970 in intervals of 10 years which is put between the years(xlabel(1930(10)1970). Also, if you want to open and modify this graph later, graph save Saves the graph to the specified file path as a .gph file (Stata’s graph format). Together, these customizations make the graph clear and informative, effectively visualizing the citation trends over time for English-speaking and non-English-speaking authors. Finally, we can restore our dataset that is not restricted to &gt;1929 and can have brp that are equal 0. sort year_c preserve keep if year_c &gt;1929 drop if brp == 0 collapse (mean) count, by(year_c english post) twoway (line count year_c if english == 1, lcolor(black) lpattern(solid) lwidth(medium)) (line count year_c if english == 0, lcolor(blue) lpattern(dash) lwidth(medium)), xline(1942, lcolor(black) lpattern(dash)) legend(order(1 &quot;English&quot; 2 &quot;Other&quot;) pos(11) ring(0)) ytitle(&quot;Citation per book and year&quot;) xtitle(&quot;Years&quot;) ylabel(0(0.2)0.8) xlabel(1930(10)1970) title(&quot;Citations to BRP Books from New Work in English versus Other Languages&quot;) graph save &quot;Graph&quot; &quot;YOUR PATH.gph&quot; graph save &quot;${Figs}/Graph1.gph&quot;, replace restore 6.1.4.3 Output of the code Graph 1 The graph shown above can be edited if you want to add a title or change the colors used you simply could click on data editor and then click edit once you finished save your changes and close the the graph editor simply by clicking on the icon. We changed here the color of the second line into blue. This graph shows that before the BRP, counts of new publications that cite BRP books in English and other languages are similar in levels and trends and starts to deviate after 1942 when the BRP was implemented by Roosevelt, taking us back to the validation of the parallel trend assumption. ###Table 1: OLS, Effect of BRP on Citations—English versus Other Language Table 1 is an OLS regression where the authors wanted to see the impact of BRP on citations of English versus other languages. We will learn here how to use different fixed effects and interactions between different fixed effects, and how to use local macros in our regressions. The following codes show how to do this. 6.1.4.4 The regressions We will preserve so the changes will not affect the original dataset. The c.english#c.post is used to do an interaction between those two variables. We clustered by id to take the correlation into account. The eststo stores the regression results into r1, that will be used in order to construct our final table with the different fixed effects. In order to calculate the mean of the dependent variable, here count, for a subset of the sample, here for citations of English language authors before 1941, and to get the percentage increase in citations in response to BRP, we will store the results in r(mean) that is considered as a temporary macro. e(sample) is used to restrict the calculation to observations that were included in the regression above. estadd is used to add a single value to the stored regression and scalar creates a new scalar named ymean in order that it is assigned the value of r(mean) preserve keep if brp == 1 //Column 1 eststo r1: reghdfe count english c.english#c.post, absorb(id year_c) cluster (id) dof(none) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; english == 1 estadd scalar ymean = `r(mean)&#39; If we don’t want to rewrite repeatedly the same regression with different fixed effects, we can define a local macro, for instance spec, which will save you time instead of rewrite the code over and over. To do so we will write the following codes : local spec = &quot;english c.english#c.post&quot; //Column 1 with fixed effects eststo r1: reghdfe count `spec&#39;, a(id year_c) cluster(id) dof(none) estadd local citation_year_FE &quot;Yes&quot;:r1 estadd local book_FE &quot;Yes&quot;:r1 estadd local field_citation_FE &quot;No&quot;:r1 estadd local pub_year_citation_year_FE &quot;No&quot;:r1 estadd local publication_year_FE &quot;No&quot;:r1 estadd local field_FE &quot;No&quot;:r1 estadd local book_publication_year_FE &quot;No&quot;:r1 estadd local ensures that the added statistic is temporary and tied to the current session. This is useful when you want to include specific information in your output without permanently altering the original estimation results. We now are going to repeat what we have done for the first regression (column 1), but we will change now the fixed effects each time to account for the variations of different combinations. We used in the second regression i. instead of c. to create the interaction because they are categorical variables. Here, the interaction models the combined effect of the year and field categories. They used those fixed effects to capture the variation across fields over time. //Column 2 eststo r2: reghdfe count `spec&#39;, a(id year_c i.field_gr#i.year_c) cluster(id) dof(none) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; english == 1 estadd scalar ymean = `r(mean)&#39; estadd local citation_year_FE &quot;Yes&quot;:r2 estadd local book_FE &quot;Yes&quot;:r2 estadd local field_citation_FE &quot;Yes&quot;:r2 estadd local pub_year_citation_year_FE &quot;No&quot;:r2 estadd local publication_year_FE &quot;No&quot;:r2 estadd local field_FE &quot;No&quot;:r2 estadd local book_publication_year_FE &quot;No&quot;:r2 //Column 3 : FE control for the variation in citations across the life cycle of a book eststo r3: reghdfe count `spec&#39;, a(id year_c i.publ_year#i.year_c) cluster(id) dof(none) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; english == 1 estadd scalar ymean = `r(mean)&#39; estadd local citation_year_FE &quot;Yes&quot;:r3 estadd local book_FE &quot;Yes&quot;:r3 estadd local field_citation_FE &quot;No&quot;:r3 estadd local pub_year_citation_year_FE &quot;Yes&quot;:r3 estadd local publication_year_FE &quot;No&quot;:r3 estadd local field_FE &quot;No&quot;:r3 estadd local book_publication_year_FE &quot;No&quot;:r3 //Column 4 eststo r4: reghdfe count `spec&#39;, a(field_gr publ_year year_c) cluster(id) dof(none) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; english == 1 estadd scalar ymean = `r(mean)&#39; estadd local citation_year_FE &quot;Yes&quot;:r4 estadd local book_FE &quot;No&quot;:r4 estadd local field_citation_FE &quot;No&quot;:r4 estadd local pub_year_citation_year_FE &quot;No&quot;:r4 estadd local publication_year_FE &quot;Yes&quot;:r4 estadd local field_FE &quot;Yes&quot;:r4 estadd local book_publication_year_FE &quot;No&quot;:r4 //Column 5 eststo r5: reghdfe count `spec&#39;, a(i.id#i.year_c) cluster(id) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; english == 1 estadd scalar ymean = `r(mean)&#39; estadd local citation_year_FE &quot;Yes&quot;:r5 estadd local book_FE &quot;No&quot;:r5 estadd local field_citation_FE &quot;No&quot;:r5 estadd local pub_year_citation_year_FE &quot;No&quot;:r5 estadd local publication_year_FE &quot;No&quot;:r5 estadd local field_FE &quot;No&quot;:r5 estadd local book_publication_year_FE &quot;Yes&quot;:r5 6.1.4.5 Replication of the Table 1 Now that we have all the regressions stored from r1 to r5 and the rmean stored in ymean as well, it’s time to get the final table. We will use the esttab command to generate a table using the stored regressions. esttab r1 r2 r3 r4 r5 using &quot;${Tables}/table1.tex&quot;, replace /// $options drop(_cons) stat(citation_year_FE book_FE field_citation_FE pub_year_citation_year_FE publication_year_FE field_FE book_publication_year_FE N r2 ymean , fmt(0 3 3) /// label( `&quot;Citation-Year FE&quot;&#39; `&quot;Book FE&quot;&#39; `&quot;Field-Citation FE&quot;&#39; `&quot;Publication-Year-Citation-Year FE&quot;&#39;`&quot;Publication-Year FE&quot;&#39; `&quot;Field FE&quot;&#39; `&quot;Book-Publication-Year FE&quot;&#39;`&quot;N&quot;&#39; `&quot;R2&quot;&#39; `&quot;Mean of dep var&quot;&#39;)) /// nomtitle booktabs nogaps title(&quot;Table 1—OLS, Effect of BRP on Citations—English versus Other Languages&quot;) restore using \"${Tables}/table1.tex\", replace saves the output as a Latex file at the ${Tables} path defined above and overwrites it if it exists we use $option as a global macro here to avoid rewriting the same options another time and save time (the global has been defined and explained above) drop(_const) : drop our constant in the table stat() : includes which statistics to display: N : number of observations r2 : R-squared ymean : mean of the dependent variable list of the fixed effects included in the five regressions fmt(0 3 3): format for each statistic, i.e. the number of decimal places 0 (integer) for N 3 for r2 and ymean label : descriptive name used to make those statics readable nomtitle : deletes model titles (default is to display r1, r2, etc., as titles) booktabs : formats the table using the LaTeX booktabs style, which improves table appearance nogaps : removes extra spacing between rows in the table, making it more compact titles : adds a title. The table shows that in column 1, the coefficient indicates that citations to BRP books increased by an additional 0.211 citations per book and year after 1941, column 2 shows that even when accounting for the variations across fields over time, English-language citations increase by 0.229 per book and year. Columns 3, 4 and 5 are showing the results with different sets of fixed effects. Table 1 6.1.5 Table 2: OLS, Effect of BRP on English-Language Citations: BRP versus Swiss Books (Matched Sample) Since it is a triple-difference, we will now explain the second identification strategy that will be afterwards combined with their first. Their second identification strategy relies on comparing after 1941 the English-language citations to BRP books with English-language citations to Swiss books that were not eligible to BRP. This identification is used to make sure that this increase in citations is not caused by post war investments in science made by the USA. To create a comparable sample of Swiss books, the authors use the Mahalanobis propensity score matching. So, they matched each BRP book with a Swiss book in the same research field and with a comparable pre-BRP stock of non-English-language citations, which will be used to construct Table 2 . They used also an alternative method which is the synthetic control as a robustness check. We will be explaining it we will need a other do file to do it, now lets replicate table 2. Table 2 looks a lot similar to Table 1 (in terms of code writing). We decided to include it because it might be helpful when carrying the Triple-differences table (Table 3). Since here you want to calculate the impact for a subset of the sample mainly the books that have been matched with Swiss books by the Mahalanobis propensity score : preserve keep if english == 1 &amp; matched == 1 The main regression is : \\[cites_{it} = βBRP_i × post_t + book_i + τ_t + ε_{it}\\] So the local macro is, as shown below, an interaction between BRP variable and post variable. local spec = &quot; c.brp#c.post&quot; The rest is just as shown in Table 1 : //Column 1 eststo r1: reghdfe count `spec&#39;, a(id year_c) cluster(id) dof(none) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; brp == 1 estadd scalar ymean = `r(mean)&#39; estadd local citation_year_FE &quot;Yes&quot;:r1 estadd local book_FE &quot;Yes&quot;:r1 estadd local field_citation_FE &quot;No&quot;:r1 estadd local pub_year_citation_year_FE &quot;No&quot;:r1 estadd local publication_year_FE &quot;No&quot;:r1 estadd local field_FE &quot;No&quot;:r1 //Column 2 : the interaction in the fixed effects captures the idiosyncratic variation in citaions accross fields over time eststo r2: reghdfe count `spec&#39;, a(id i.field_gr#i.year_c) cluster(id) dof(none) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; brp == 1 estadd scalar ymean = `r(mean)&#39; estadd local citation_year_FE &quot;No&quot;:r2 estadd local book_FE &quot;Yes&quot;:r2 estadd local field_citation_FE &quot;Yes&quot;:r2 estadd local pub_year_citation_year_FE &quot;No&quot;:r2 estadd local publication_year_FE &quot;No&quot;:r2 estadd local field_FE &quot;No&quot;:r2 //Column 3 : the interaction in the fixed effects capture the idiosyncratic variation for a book&#39;s age, with an interaction for publication year × citation year fixed effects eststo r3: reghdfe count `spec&#39;, a(id i.publ_year#i.year_c) cluster(id) dof(none) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; brp == 1 estadd scalar ymean = `r(mean)&#39; estadd local citation_year_FE &quot;Yes&quot;:r3 estadd local book_FE &quot;Yes&quot;:r3 estadd local field_citation_FE &quot;No&quot;:r3 estadd local pub_year_citation_year_FE &quot;Yes&quot;:r3 estadd local publication_year_FE &quot;No&quot;:r3 estadd local field_FE &quot;No&quot;:r3 //Column 4 eststo r4: reghdfe count `spec&#39;, a(year_c i.field_gr i.publ_year) cluster(id) dof(none) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; brp == 1 estadd scalar ymean = `r(mean)&#39; estadd local citation_year_FE &quot;Yes&quot;:r4 estadd local book_FE &quot;No&quot;:r4 estadd local field_citation_FE &quot;No&quot;:r4 estadd local pub_year_citation_year_FE &quot;No&quot;:r4 estadd local publication_year_FE &quot;Yes&quot;:r4 estadd local field_FE &quot;Yes&quot;:r4 //Table esttab r1 r2 r3 r4 using &quot;${Tables}/table2.tex&quot;, replace $options drop(_cons) stat( citation_year_FE book_FE field_citation_FE pub_year_citation_year_FE publication_year_FE field_FE N r2 ymean , fmt(0 3 3) label( `&quot;Citation-Year FE&quot;&#39; `&quot;Book FE&quot;&#39; `&quot;Field-Citation FE&quot;&#39; `&quot;Publication-Year-Citation-Year FE&quot;&#39;`&quot;Publication-Year FE&quot;&#39; `&quot;Field FE&quot;&#39; `&quot;N&quot;&#39; `&quot;R2&quot;&#39; `&quot;Mean of dep var&quot;&#39;)) nomtitle booktabs nogaps title(&quot;Table 2—OLS, Effect of BRP on English-Language Citations: BRP versus Swiss Books&quot;) restore Table 2 6.1.6 Table 3 : OLS, Effect of BRP on English-Language versus Other Citations: BRP versus Swiss Books (Matched Sample) Now the regression changes to include the differential change in citations to BRP books from English-language and other-language authors with the same differential change for Swiss books. So here, there are 2 comparisons : the first examines changes in English-language citations to BRP books compared to citations to the same books in other languages, mitigating selection bias by focusing on the same source material across different linguistic contexts the second compares changes in English-language citations to BRP books with those to Swiss books, addressing the concern that English-language citations may have increased automatically due to a post-World War II. To account for this the authors are estimating the following equation : \\[cites_{ilt} = β_1English_l +β_2 BRP_i * post_t+ β_3 English_l * BRP_i + β_4 English_l * post_t+ β_5 English_l * BRP_i × post_t + book_i + τ_t + ε_{ilt}\\] where \\(\\beta_5\\) is the coefficient of interest (the triple-difference coefficient). The local macro is defined accordingly and then, you repeat the steps of Table 1 and Table 2 with different specifications preserve keep if matched == 1 local spec = &quot;english brp c.brp#c.post c.english#c.brp c.english#c.post c.english#c.brp#c.post&quot; //Column 1 eststo r1: reghdfe count `spec&#39;, a(id year_c) cluster(id) dof(none) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; brp == 1 estadd scalar ymean =`r(mean)&#39; estadd local citation_year_FE &quot;Yes&quot;:r1 estadd local book_FE &quot;Yes&quot;:r1 estadd local field_citation_FE &quot;No&quot;:r1 estadd local pub_year_citation_year_FE &quot;No&quot;:r1 estadd local book_citation_FE &quot;No&quot;:r1 estadd local english_citation year &quot;No&quot;:r1 //Column 2 eststo r2: reghdfe count `spec&#39;, a(id year_c i.field_gr#i.year_c) cluster(id) dof(none) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; brp == 1 estadd scalar ymean =`r(mean)&#39; estadd local citation_year_FE &quot;No&quot;:r2 estadd local book_FE &quot;Yes&quot;:r2 estadd local field_citation_FE &quot;Yes&quot;:r2 estadd local pub_year_citation_year_FE &quot;No&quot;:r2 estadd local book_citation_FE &quot;No&quot;:r2 estadd local english_citation year &quot;No&quot;:r2 //Column 3 eststo r3: reghdfe count `spec&#39;, a(year_c i.field_gr i.publ_year) cluster(id) dof(none) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; brp == 1 estadd scalar ymean =`r(mean)&#39; estadd local citation_year_FE &quot;Yes&quot;:r3 estadd local book_FE &quot;No&quot;:r3 estadd local field_citation_FE &quot;No&quot;:r3 estadd local pub_year_citation_year_FE &quot;Yes&quot;:r3 estadd local book_citation_FE &quot;No&quot;:r3 estadd local english_citation year &quot;No&quot;:r3 //Column 4 eststo r4: reghdfe count `spec&#39;, a(id year_c i.id#i.year_c) cluster(id) dof(none) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; brp == 1 estadd scalar ymean =`r(mean)&#39; estadd local citation_year_FE &quot;Yes&quot;:r4 estadd local book_FE &quot;Yes&quot;:r4 estadd local field_citation_FE &quot;No&quot;:r4 estadd local pub_year_citation_year_FE &quot;No&quot;:r4 estadd local book_citation_FE &quot;No&quot;:r4 estadd local english_citation year &quot;Yes&quot;:r4 //Column 5 eststo r5: reghdfe count `spec&#39;, a(id year_c i.english##i.year_c) cluster(id) dof(none) qui sum count if e(sample) &amp; year_c &lt;= 1941 &amp; brp == 1 estadd scalar ymean =`r(mean)&#39; estadd local citation_year_FE &quot;Yes&quot;:r5 estadd local book_FE &quot;No&quot;:r5 estadd local field_citation_FE &quot;No&quot;:r5 estadd local pub_year_citation_year_FE &quot;No&quot;:r5 estadd local book_citation_FE &quot;Yes&quot;:r5 estadd local english_citation year &quot;Yes&quot;:r5 //Table esttab r1 r2 r3 r4 r5 using &quot;${Tables}/table3.tex&quot;, replace $options drop(_cons) stat(citation_year_FE book_FE field_citation_FE pub_year_citation_year_FE publication_year_FE field_FE N r2 ymean , fmt(0 3 3) label( `&quot;Citation-Year FE&quot;&#39; `&quot;Book FE&quot;&#39; `&quot;Field-Citation FE&quot;&#39; `&quot;Publication-Year-Citation-Year FE&quot;&#39;`&quot;Publication-Year FE&quot;&#39; `&quot;Field FE&quot;&#39; `&quot;N&quot;&#39; `&quot;R2&quot;&#39; `&quot;Mean of dep var&quot;&#39; )) nomtitle booktabs nogaps title(&quot;Table 3—OLS, Effect of BRP on English-Language versus Other Citations: BRP versus Swiss Books&quot;) restore Table 3 6.1.7 Robustness checks: The Synthetic control Method 6.1.7.1 Synthetic dataset In order to perform a synthetic control, we decided that we will use another do-file since this method is based on a lot of dataset generation and merging. We will then redefine our global macros and redownload our dataset: clear set more off global Rawdata &quot;C:YOUR PATH\\raw data&quot; global Data &quot;C:YOUR PATH\\data&quot; global Figs &quot;C:YOUR PATH\\figs&quot; global Prog &quot;C:YOUR PATH\\prog&quot; global Tables &quot;C:YOUR PATH\\tables&quot; use &quot;${Rawdata}/simplified_dataset.dta&quot;, clear To prepare a balanced dataset for performing the Synthetic Control Method (SCM), we start by filling any missing periods for the variable id to ensure the dataset is set as a balanced panel. tsfill, full Next, we assign integer values to each field, covering 25 mutually exclusive research fields within chemistry and 8 in mathematics, by grouping them into a new variable egen field_gr. You can see through br command that most of the missing data points are in the early years. To handle this, we first sort the data by id in ascending and then by year putting a - sign before it to order it in descending order , ensuring that missing data points are placed at the end. Remember the carryforward command being downloaded in the beginning ? We’ll use it now so that most recent available data is carried forward to fill the “early” missing years . So, for each id, the command will take the most recent (non-missing) values of field_gr, math, and publ_year and “carry” them forward to subsequent observations where those variables are missing using it to fill the gap. For example, imagine a book’s field is recorded in 1950 but missing for 1948 and 1949. The command will take the value from 1950 and copy it backward to fill in the missing values for 1948 and 1949. This ensures that no data is left blank for earlier years and provides consistent information for analysis. Afterward, the data is re-sorted by id and year in ascending order to arrange it chronologically. The second bysort command aim to ensure that any remaining missing values in the variables (field_gr, math, and publ_year) are filled consistently, even after the initial use of carryforward After the carryforward process, any missing values . in the field_gr variable that still remain are explicitly replaced with 0, to ensure that no observations are left with missing values. Finally, the dataset is restricted to keep years after 1920 to focus the analysis on the relevant period. These steps ensure the dataset is balanced and ready for SCM analysis. tsfill, full egen field_gr = group(field) br gsort id -year_c bysort id: carryforward field_gr math publ_year, replace gsort id year_c bysort id: carryforward field_gr math publ_year, replace replace field_gr = 0 if field_gr == . keep if year_c &gt; 1920 Now we are going to temporary save a subset of this the dataset that we will use later. Keeping only the 6 variables that are going to be included in the temporary dataset. id, year_c, brp count_eng , count_noeng cit_year, we sort them ascendingly, then we save this dataset that will be used then when constructing the synthetic control. preserve keep id year_c brp count_eng count_noeng cit_year sort id year_c save temp.dta, replace restore Now we are back to our original data set: We keep only the variables used for conducting the synthetic control analysis, dropping the rest. Then, we rename the year_c variable to year for consistency. Using tsset, we declare the dataset as a panel data set, using id as the panel identifier and year as the time variable, so it recognizes the repeated observations over time for each unit. It’s important to ensure once more that there are no missing values, as they will interfere with the synthetic control procedure, that’s why we use replace command. As before, sort the data by id in ascending order and by -year in descending order to correctly handle missing data points. Then creating different dummies for each research field using the gen(F) option with the tab command, generating frequency (tabulation) of the field_gr variable, creating a variable F that groups observations based on field categories. keep cit_year count_eng count_noeng field_gr decl_perc id brp year_c math publ_year rename year_c year tsset id year replace count_eng = 0 if count_eng == . replace count_noeng = 0 if count_noeng == . gsort id -year qui tab field_gr, gen(F) It’s worth recalling that we are performing Synthetic controls procedure here in order to minimize pre-treatment differences between the treated unit (BRP) and the control unit. SCM does so by assigning different weights to the control variables. Here we want to know how the outcomes specifically the number of English citations and non-English citations diverged after 1942 (the implementation date of BRP). We cannot simply compare Swiss and BRP books because they might differ in significant ways beyond the intervention that’s why the synthetic method is used, Stata has the synth package which we will be using (we already install it in the beginning). In order to use the synthetic method, we first determine our treatment and control groups for instance BRP is our treatment group and Swiss is our control group we have to declare the dataset as panel tsset and then we use the synth package. Here, we will use levelsof command then we add our variable of interest, generating unique values of id, and our conditions (brp==1 and math==1) so its our treatment variable and we use local command that stores these unique id values into the local macro BRP for later use so it can loop over it. We do the same for the control group (Swiss). The conditions are made using if to ensure filtering only those groups that meet both criteria (brp == 1 and math == 1). To create the loop, first, we use foreach command. this loop iterates over each group in the treatment group BRP and creates to each book its own synthetic control dataset, we use this brackets to begin and finish the loop {} We begin the loop using noisily command used to display messages or output without interrupting the flow of our code. Normally, disp (display) outputs text to the Results window, but if used directly, the code stops executing until the message is acknowledged. So, noisily allows you to display messages but ensures the code keeps running. Then you add the variable you want like this Book nr.'n' Inside the quotation marks, insert the current iteration value of n from the loop. For example, if n is 1, you’’ have: “Book nr. 1”. Once declaring the data as a panel using tsset, we employ the synth command. The variables after it are the outcomes of interest, you make a comma and specify tour treatment units trunit('n') (each n from the BRP macro), identify your intervention year trperiod, specify the control variable the counit and finally, keep(book_n’, replace)` is used to create the new dataset. This code will keep running for a while. Then you repeate the same steps for the chemistry fields’books where math == 0. qui levelsof id if brp == 1 &amp; math == 1, local(BRP) qui levelsof id if brp == 0 &amp; math == 1, local(Swiss) foreach n of local BRP { noisily disp &quot;Book nr.&#39;n&#39;&quot; tsset id year synth count_eng count_noeng, trunit(&#39;n&#39;) trperiod(1941) counit(&#39;Swiss&#39;) keep(book_&#39;n&#39;, replace) } qui levelsof id if brp == 1 &amp; math == 0, local(BRP) qui levelsof id if brp == 0 &amp; math == 0, local(Swiss) foreach n of local BRP { noisily disp &quot;Book nr. `n&#39;&quot; tsset id year synth count_eng count_noeng, trunit(`n&#39;) trperiod(1941) counit(`Swiss&#39;) keep(book_`n&#39;, replace) } We have our different synthetic datasets, and now we need to create our control group. First, we set obs 1 creating an empty observation, then gen x = . which adds a column x with missing values. This will create a temporary empty dataset called tempcontr.dta. This dataset will store the synthetic control group data for all treated books. For each treated book, we extract the corresponding control books and their synthetic control weights. The merge command is then used to combine these control books’ data with the original dataset. After merging, we aggregate the data using the collapse command to compute synthetic control outcomes (count_eng, count_noeng, and cit_year) over time. Finally, we append these synthetic control outcomes to the master control dataset (tempcontr.dta) and save the changes.It keeps iterating till we have all the synthetic control group. For the other details in the code: qui levelsof id if brp == 1, local(BRP): it generates a list of unique IDs for books // here for both fields preserve: Saves the current state again for each treated book to restore after processing. clear: clears data to avoid interference from previous book’s data. use book_n'.dta: Loads the specific synthetic control dataset for book n. keep id weight: Keeps only the renamed variable id and weight to focus on control information. Sort the data by id to prepare for merging. merge 1:m id using temp.dta: Merges the control book data with the tempcontr.dta dataset by id. keep if _m == 3: Keeps only the matched observations, ensuring that the merge was successful. gen id_contr = n': Creates a new variable to label the synthetic control group for book n. collapse (mean) Aggregates the data, calculating the mean of outcomes //summarizes the control group’s data for the synthetic control comparison. gen brp = 0: Adds a brp indicator set to 0, distinguishing control units. save tempcontr.dta, replace: now we have our synthetic control group preserve clear set obs 1 gen x = . save tempcontr.dta, replace restore qui levelsof id if brp == 1, local(BRP) foreach n of local BRP { preserve clear use book_`n&#39;.dta rename _Co_Number id rename _W_Weight weight keep id weight sort id merge 1:m id using temp.dta keep if _m == 3 // only the matched results drop _m drop id gen id_contr = `n&#39; collapse (mean) count_eng count_noeng cit_year, by(id_contr year) gen brp = 0 append using tempcontr.dta save tempcontr.dta, replace restore } As a final step,we add everything in the same file we first ensure that only the treated group(brp == 1). Next, we append the synthetic control group data stored in tempcontr.dta to the treated group. To avoid ID overlap between treated books and their synthetic controls, we add 50,000 to the IDs of synthetic control books, clearly distinguishing them from the treated group. We then create a linking variable that links each treated and control group for easier association between treated books and their respective synthetic control groups. To maintain consistency within each group, we calculate the maximum value of the math variable and the latest publication year (Year_from) for each group using egen. This ensures all books in a group are categorized consistently and have aligned publication years. Finally, we remove any observations where Year_from exceeds the analysis year (year), maintaining consistency across the treated and control groups. `keep if brp == 1` `append using tempcontr.dta` `replace id = id_contr + 50000 if brp == 0` `replace id_contr = id if brp == 1` bysort id_cont: egen Math = max(math) bysort id_cont: egen Year_from = max(publ_year) drop if Year_from &gt; year 6.1.7.2 Figure 2: Citations to BRP books and their synthetic controls Here we’ll show how they constructed figureA13 in the appendix presenting the SC representation. Similar to figure 1, figure A13 is constructed the same way `preserve` collapse count_eng if year &gt;= 1930, by(brp year) twoway (line count year if brp == 1, lc(black)) (line count year if brp == 0, lp(dash) lc(black)), legend(order(1 &quot;BRP&quot; 2 &quot;Synthetic controls&quot;)) xline(1942, lpattern(dash) lcolor(black)) ytitle(&quot;Citations per book and year&quot;) graph save &quot;Graph&quot; &quot;YOUR PATH\\Graph2.gph&quot; restore Graphe 2 The figure shows that the citation trends for BRP books diverge significantly from their synthetic controls following the key event (vertical line).This reinforces the main results, suggesting that the observed impact of the event on BRP books is not spurious and remains robust even when compared to a carefully constructed control group. Including this figure in the robustness checks confirms that the results are not sensitive to different specifications or methodological assumptions and strengthen the credibility of our analysis. Authors: Salma Abdelsalam, Sara Sedrak, Mirana Ranerison, students in the Master program in Development Economics and Sustainable Development (2024-2025), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2024 "],["carbon-taxes-and-co2-emissions-sweden-as-a-case-study.html", "6.2 Carbon Taxes and CO2 Emissions: Sweden as a Case Study", " 6.2 Carbon Taxes and CO2 Emissions: Sweden as a Case Study Andersson, J. (2019). Carbon Taxes and CO2 Emissions: Sweden as a Case Study. American Economic Journal: Economic Policy, 11(4): 1–30. [https://doi.org/10.1257/pol.20170144] [Download Do-File corresponding to the explanations below] [Download Codebook] [Link to the full original replication package paper from AEA web] Highlights The paper examines the impact of Sweden’s carbon tax on CO₂ emissions from the transport sector over the period ranging from 1990 to 2005. J. Andersson uses the synthetic control method to address concerns in regards to endogeneity due to the presence of unobservables affecting both Sweden’s CO₂ emissions and the implementation of the taxation policy. This methodology works well for comparative case studies, as it compares Sweden to other countries by creating a “synthetic Sweden”. The author does this by combining a weighted mix of OECD countries that share similar characteristics with Sweden before the tax was introduced, but that did not adopt carbon taxes themselves. This synthetic version gives a realistic counterfactual of what Sweden’s emissions may have looked like without the carbon tax. Our contribution to the original replication package is the documentation and detailed explanation of the provided code, making it easier for other researchers to follow and understand. We also show step-by-step guidance on the extraction of the main tables and figures directly from the Stata code. This allows users to generate all key outputs and adapt them as needed. A key element of this replication consists in integrating Newey-West standard errors ( to account for serial correlation and heteroscedasticity in time series data) and IV techniques, adding robustness to causal inference. Instead of focusing solely on the treatment and control group comparisons typical in synthetic control analysis. Throughout this replication exercise, we provide a detailed explanation of the original replication package. You will be able to become familiar with many STATA tricks, including: How to create figures and plots using graph, line, and twoway How to present your results in academic tables, by using the packages estout, ivreg2, outreg2, and ranktest How to create regression models using ivregress and regress How to handle potential autocorrelation and heteroskedasticity in time series data by using the Newey-west command How to manipulate data to generate new variables using egen, label variables using label, and merge datasets using merge How to visualize your data using descriptive statistics including tabulate, describe, summarize, and correlate. 6.2.1 Introduction “Carbon Taxes and CO₂ Emissions: Sweden as a Case Study” explores the effectiveness of carbon taxes in reducing greenhouse gas emissions, focusing on Sweden’s groundbreaking carbon tax policy introduced in 1991. Using Sweden as a natural experiment, the study investigates whether carbon taxes can significantly lower emissions without hindering economic growth. Andersson employs a quasi-experimental design within an event study framework to evaluate the policy’s impact. This approach analyzes changes in Sweden’s CO₂ emissions over time, comparing the period before and after the tax’s implementation. The study further enhances its analysis with the synthetic control method, which constructs a counterfactual scenario by combining data from other European countries that did not adopt similar policies. This synthetic control provides a benchmark to estimate what Sweden’s emissions trajectory might have been in the absence of the tax. By combining these methods, the study ensures robust results, effectively isolating the specific impact of the carbon tax while addressing potential confounding factors. This dual approach highlights the causal relationship between Sweden’s carbon tax and its success in reducing emissions. The study utilizes panel data on CO₂ emissions, economic indicators, and policy variables from Sweden and other European countries spanning several decades. Emissions data is sourced from national and international databases, ensuring consistency and comparability across countries. Economic variables, such as GDP growth, are included to examine the broader impacts of the carbon tax on Sweden’s economy. Additionally, data on energy prices and sector-specific emissions are used to control for other factors influencing emissions and to identify the policy’s sectoral effects.To strengthen the causal inference, the synthetic control method is used. This method constructs a counterfactual for Sweden by creating a weighted combination of other European countries that did not implement a similar carbon tax. The study concludes that Sweden’s carbon tax significantly reduced CO₂ emissions, particularly in sectors directly targeted by the policy, such as transportation and heating. Importantly, these reductions occurred without stalling economic growth, demonstrating that ambitious climate policies can coexist with economic stability. By effectively isolating the impact of the tax, the research provides strong evidence supporting carbon taxes as a powerful and practical tool for reducing greenhouse gas emissions. Sweden’s experience serves as a valuable model for other countries aiming to achieve substantial emission reductions through market-based climate policies. 6.2.2 Identification strategy When using the Difference-in-Difference (DiD) estimator in comparative case studies, it’s important to consider some key assumptions to ensure the identification of robust accurate results. The most critical is the parallel trends assumption, which suggests that, without the treatment, the treated unit and the control group would have followed similar trends in the outcome over time. This assumption is what allows the DiD method to account for unobserved factors that might otherwise skew the results. In the case of CO₂ emissions, this means that without Sweden’s carbon tax and VAT, emissions in Sweden and the control group would have followed parallel paths. While it’s possible to check this assumption by analyzing pretreatment trends, it’s impossible to verify fully after the treatment has occurred. If the treated unit and the control group don’t share a similar trend, the DiD estimates can end up biased. The attractiveness of using the synthetic control method relies on the fact that it provides a way to overcome this limitation. Unlike the DiD estimator, which uses a simple average of the control group, the synthetic control approach creates a weighted combination of control units that closely match the treated unit in terms of key predictors as well as pretreatment trends. This flexibility allows it to account for differences that might change over time, making it a stronger tool for comparative case studies. For example, when analyzing the effects of Sweden’s carbon tax and VAT on transport sector emissions, the synthetic control method creates a better counterfactual by closely mimicking Sweden’s emissions patterns before the policy was introduced. This helps address the weaknesses of the DiD method and provides more reliable results. 6.2.3 From the article to practice: exploring the replication code 6.2.3.1 Getting started: database access and required packages To open the Stata database and run the lines of code, the user needs to download and install various packages. In this section, we will show you step-by-step how to access the database and why these packages are necessary. First, download the original dataset from a link provided. Then, run the initial lines of code to create a shorter version of the dataset. We also provide a codebook (see the header) to help you understand it. Open the database *download and save the replication package dataset zip from: https://www.aeaweb.org/journals/dataset?id=10.1257/pol.20170144 *you should find nine datasets once you open the downloaded zip: carbontax_data.dta, carbontax_fullsample_data.dta, disentangling_regression_data.dta, tax_incidence_data.dta, descriptive_data.dta, fullsample_figures.dta, Table3.dta, leave_one_out_data.dta, and disentangling_data.dta *define your working directory, where you also just stored the dataset clear mata capture log close clear clear all set more off cd &quot;C:\\Users\\&quot; /*C:\\Users\\ = path where you also store the dataset*/ For example: use &quot;/Users/alicebrossard/Desktop/Replication Project STATA/data/disentangling_regression_data.dta&quot;, clear describe browse Subsequently, several Stata packages are necessary to execute the replication code successfully: The first package needed is the estout package. This package allows to make regression tables using regressions previously stored in the Stata memory. The second package required is the ivreg2 package. This package allows to run instrumental variables regressions. The third package is the ranktest package. This package is used for non-parametric statistical tests, which rely on the ranks of data instead of the actual values.This makes it great for comparing groups or analyzing relationships when your data contains outliers. For example, you can use it to test if two groups have different medians or to analyze repeated measurements over time. All the packages can be installed using the following lines of code. Install the required packages ssc install estout ssc install ivreg2 ssc install ranktest 6.2.3.2 Importing specific datasets To replicate Descriptive Figures: Figure 3 and Figures 11-12 of the paper, you will need to download the original dataset, descriptive_data.dta, from the original replication package. In order to do so, follow these steps: Set up your directory and import your dataset on your STATA board: For example: use &quot;/Users/alicebrossard/Desktop/Replication Project STATA/data&quot; import delimited &quot;descriptive_data.dta&quot;, clear 2. Load the dataset: use &quot;descriptive_data.dta&quot;, clear describe To replicate the main results that disentangle the Carbon Tax and VAT, you will need to download the original dataset, disentangling_regression_data.dta, from the original replication package. This specific dataset is used to replicate Table 3, while the dataset disentangling_data.dta is used to replicate complementary Figure 14. In order to do so, follow these steps: Set up your directory and import your dataset on your STATA board: For example: use &quot;/Users/alicebrossard/Desktop/Replication Project STATA/data&quot; import delimited &quot;disentangling_regression_data.dta&quot;, clear 2. Load the dataset: use &quot;disentangling_regression_data.dta&quot;, clear describe 6.2.4 Descriptive Statistics : Figure 3 and Figures 11-12 Use the dataset “descriptive_data.dta” Figure 3: CO2 Emissions per capita, Sweden vs OECD average twoway (line CO2_Sweden year if inrange(year, 1960, 2005), lwidth(medium) lcolor(black)) /// (line CO2_OECD year if inrange(year, 1960, 2005), lpattern(dash) lwidth(medium) lcolor(black)) /// , ytitle(&quot;Metric tons per capita (CO2 from transport)&quot;) xtitle(&quot;Year&quot;) /// legend(order(1 &quot;Sweden&quot; 2 &quot;OECD sample&quot;)) /// yscale(r(0 3)) xscale(r(1960 2005)) xline(1990, lpattern(dash)) /// text(1981 1 &quot;VAT + Carbon tax&quot;, place(e)) Figure 3 will help you understand how to optimize the twoway command, which is used to combine multiple layers of plots to visually identify plot trends. In our replication, we use it to plot CO2 emissions (metric tons) for Sweden and an OECD sample from 1960 to 2005. We use twoway to specify that a graph will be drawn with one or more “layers” of plots, which we enclose using parentheses: You will first use the line command to draw the line plots using the format line y_variable x_variable [if condition], [options]. For instance, our first line starts with the variable on the Y-axis, CO2_Sweden (CO2 emissions for Sweden), followed by the variable on the X-axis, year (time, 1960 to 2005), with a specification using the command if inrange(year, 1960, 2005) to make sure only data from the years 1960-2005 is included. You then set the line width to “medium” for better visibility using lwidth(medium), and the line color to black using lcolor(black). To create a second plot for CO2_OECD, you should apply the same step-by-step logic, adding the command lpattern(dash) to specify that you would like a dashed line instead of a solid one, which we used in our first line plot for Sweden. To visually structure your figure, you should use the ytitle command, specified here ytitle(\"Metric tons per capita (CO2 from transport)\"), to label the Y-axis, while adding the command xtitle(\"Year\") to label your X-axis. You should make sure to scale both axes using yscale(r(0 3)) to restrict the Y-axis range from 0 to 3 metric tons per capita, and xscale(r(1960 2005)) to set the X-axis range from 1960 to 2005. This command ensures your graph uses the same scale across multiple graphs if you are to compare different groups, without misleading axis differences. To customize an ordered legend, you can use the legend(order(1 \"Sweden\" 2 \"OECD sample\")) command, which labels the first line as “Sweden” and the second as “OECD sample.” Finally, You can add a vertical dashed line at a specific impactful year, here 1990, using the xline(1990, lpattern(dash)) command. To add context, in our case “VAT + Carbon tax”, you can use the text command text(1981 1 \"VAT + Carbon tax\", place(e)), which positions the annotation at the coordinates (1981, 1) and makes sure it does not overlap with other elements using place(e). Figure 11: GDP per capita, Sweden vs Synthetic Sweden twoway (line GDP_Sweden year if inrange(year, 1960, 2005), lwidth(medium) lcolor(black)) /// (line GDP_Synthetic_Sweden year if inrange(year, 1960, 2005), lpattern(dash) lwidth(medium) lcolor(black)) /// , ytitle(&quot;GDP per capita (PPP, 2005 USD)&quot;) xtitle(&quot;Year&quot;) /// legend(order(1 &quot;Sweden&quot; 2 &quot;Synthetic Sweden&quot;)) /// yscale(r(0 35000)) xscale(r(1960 2005)) xline(1990, lpattern(dash)) /// text(1981 10000 &quot;VAT + Carbon tax&quot;, place(e)) Now that you have the necessary tools to build multiple layers of plots using twoway, you can replicate your code from Figure 3 and apply it to Figure 11. Figure 11 essentially shows that GDP per capita in Sweden and its synthetic counterpart track each other similarly during the 30 years before and 16 years after treatment. The innovation in Figure 11 revolves around the scalar command. As in Figure 3, the xscale(r(1960 2005)) limits the X-axis to the years 1960-2005, keeping the focus on the relevant time period and avoiding extra space. The yscale(r(0 35000)) however changes here, setting the Y-axis from 0 to 35,000 to include the highest GDP per capita values (in PPP-adjusted 2005 USD) without cutting off or crowding the data. You must remain attentive to the specifications in your dataset, to make sure your variables align with your chosen axis ranges correctly. Figure 12: Gap in CO2 emissions and GDP per capita Set your graph options: set scheme s2mono twoway (line gap_CO2_emissions_transp year, lcolor(black)) /// (line gap_GDP year, lcolor(gs8) yaxis(2)), /// ytitle(&quot;Gap in metric tons per capita (CO2 from transport)&quot;) /// ylabel(-0.4(0.2)0.4) /// ytitle(&quot;Gap in GDP per capita (PPP, 2005 USD)&quot;, axis(2)) /// ylabel(-2000(1000)2000, axis(2)) /// xtitle(&quot;Year&quot;) /// xlabel(1960 1970 1980 1990 2000) /// xline(1990, lpattern(dash) lcolor(black)) /// legend(order(1 &quot;CO2 Emissions (left y-axis)&quot; 2 &quot;GDP per capita (right y-axis)&quot;)) *`gap_CO2_emissions_transp` twoway line gap_CO2_emissions_transp year, lcolor(black) /// ytitle(&quot;Gap in metric tons per capita (CO2 from transport)&quot;) /// xtitle(&quot;Year&quot;) twoway (line gap_CO2_emissions_transp year, lcolor(black)) /// (line gap_GDP year, lcolor(gs8) yaxis(2)), /// ytitle(&quot;Gap in metric tons per capita (CO2 from transport)&quot;) /// ytitle(&quot;Gap in GDP per capita (PPP, 2005 USD)&quot;, axis(2)) /// xtitle(&quot;Year&quot;) Figure 12 illustrates the correlation between the gaps in GDP per capita and CO2 emissions from transport for Sweden and its synthetic counterpart. It provides a comparison of the effects of two major recessions in Sweden during the sample period: the first in 1976–1978, and the second in 1991–1993. In Figure 12, the command set scheme s2mono is used at the beginning of our code to change the default appearance of the graph to a monochrome color scheme. You will find this command helpful in the redaction of academic papers, printed documents, or presentations, because using color schemes like s2mono helps maintain focus on the data itself, rather than the colors. You can then set the color of your lines accordingly, making your first line, for CO2 emissions here, black (lcolor(black)), and your second, for GDP here, gray (lcolor(gs8)), maintaining a simple yet effective visual differentiation without the use of bright colors. One of the key innovations here is the use of dual y-axes. This approach makes it easier for you to compare two variables with different units and scales without needing separate graphs, while still showing their relationship across the same time. The left y-axis is used for the CO2 emissions gap gap_CO2_emissions_transp, while the right y-axis is reserved for the GDP per capita gap, gap_GDP. In line with the logic applied in Figures 3 and 11, you can label your first Y-axis, here for CO2 emissions, with ytitle(\"Gap in metric tons per capita (CO2 from transport)\"), while your second for GDP is named ytitle(\"Gap in GDP per capita (PPP, 2005 USD)\", axis(2)). You can then apply the ylabel(-0.4(0.2)0.4) command to define the tick marks for the first Y-axis, ranging from -0.4 to 0.4 in steps of 0.2, and ylabel(-2000(1000)2000, axis(2)) ` for the second Y-axis, ranging from -2000 to 2000 in steps of 1000. This ensures each variable fits well within its own scale. The axis(2) option is used in Stata to specify that gap_GDP should be applied to the second Y-axis, rather than the default first Y-axis. This will be helpful to help you distinguish the two variables clearly while in the same graph. 6.2.5 Main Results: Table 3 and Figure 14 Use the dataset “disentangling_regression_data.dta” Table 3: Estimation Results from Gasoline Consumption Regressions use &quot;disentangling_regression_data.dta&quot;, clear describe tsset year eststo clear eststo: newey log_gas_cons real_carbontaxexclusive_with_vat real_carbontax_with_vat d_carbontax t, lag(16) eststo: newey log_gas_cons real_carbontaxexclusive_with_vat real_carbontax_with_vat d_carbontax t real_gdp_cap_1000, lag(16) eststo: newey log_gas_cons real_carbontaxexclusive_with_vat real_carbontax_with_vat d_carbontax t real_gdp_cap_1000 urban_pop, lag(16) eststo: newey log_gas_cons real_carbontaxexclusive_with_vat real_carbontax_with_vat d_carbontax t real_gdp_cap_1000 urban_pop unemploymentrate, lag(16) eststo: ivregress 2sls log_gas_cons (real_carbontaxexclusive_with_vat=real_energytax_with_vat) real_carbontax_with_vat d_carbontax t real_gdp_cap_1000 urban_pop unemploymentrate, vce(hac bartlett opt) eststo: ivregress 2sls log_gas_cons (real_carbontaxexclusive_with_vat=real_oil_price_sek) real_carbontax_with_vat d_carbontax t real_gdp_cap_1000 urban_pop unemploymentrate, vce(hac bartlett opt) esttab, r2 label mtitles(&quot;OLS&quot; &quot;OLS&quot; &quot;OLS&quot; &quot;OLS&quot; &quot;IV(EnTax)&quot; &quot;IV(OilPrice)&quot;) se(3) star(* 0.10 * 0.05 ** 0.01) ** p-value b1=b2 test real_carbontaxexclusive_with_vat = real_carbontax_with_vat * Instrument F-statistic and testing for weak instruments gen rctewvat= real_carbontaxexclusive_with_vat ivreg2 log_gas_cons (rctewvat = real_energytax_with_vat ) real_carbontax_with_vat d_carbontax t real_gdp_cap_1000 urban_pop unemploymentrate , bw(auto) robust first ivreg2 log_gas_cons (rctewvat = real_oil_price_sek ) real_carbontax_with_vat d_carbontax t real_gdp_cap_1000 urban_pop unemploymentrate , bw(auto) robust first * Estimated elasticities, using results from column (4) newey log_gas_cons real_carbontaxexclusive_with_vat real_carbontax_with_vat d_carbontax t real_gdp_cap_1000 urban_pop unemploymentrate , lag(16) margins, dyex(real_carbontax_with_vat real_carbontaxexclusive_with_vat) at(real_carbontax_with_vat=8.478676 real_carbontaxexclusive_with_vat=8.478676) Columns 1 through 6 in the table represent different econometric specifications, combining Ordinary Least Squares (OLS) and Instrumental Variables (IV) regressions to investigate the impact of carbon taxes on gas consumption. These models build on a quasi-experimental framework, leveraging both direct estimates and instrumental variable techniques to isolate causal effects. The dependent variable in all regressions is the natural logarithm of gas consumption log_gas_cons “Gasoline Consumption Regressions”, while the key explanatory variable,real_carbontaxexclusive_with_vat “Gas Price with VAT” is central to understanding the role of carbon taxes in influencing gas usage. By varying the inclusion of control variables and introducing instruments, the table presents a nuanced exploration of the data. Model Specifications The first four columns employ OLS regressions with Newey-West standard errors to account for autocorrelation and heteroscedasticity. Column 5 and Column 6 both adopt an Instrumental Variable (IV) to address potential endogeneity in the variable real_carbontaxexclusive_with_vat. The use of IVs – real_energytax_with_vat (Column 5) and real_oil_price_sek (Column 6) – strengthens causal inference by isolating exogenous variation in the key explanatory variable. The specification also includes heteroscedasticity- and autocorrelation-consistent (HAC) standard errors, calculated with a Bartlett kernel. Estimation Process The analysis begins with the eststo clear command, resetting previously stored estimations to ensure a clean workspace. Regressions are then performed using the newey function for OLS models and ivregress 2sls for IV models. The lag structure in Newey-West regressions lag(16) ensures robustness by accounting for potential autocorrelation over 16 periods. For IV regressions, the vce(hac bartlett opt) option calculates standard errors using the Bartlett kernel with optimal bandwidth selection, ensuring consistency and efficiency. In all models, the dependent variable log_gas_cons is regressed on a combination of explanatory variables, with progressively added controls providing insight into how the inclusion of economic and demographic factors affects the estimated relationships. The table uses eststo to store results for each regression and summarizes them using esttab. 6.2.5.1 Instrumental Variable Approach and Weak Instrument Tests Columns 5 and 6 focus on addressing endogeneity in the carbon tax variable. To assess instrument strength, the code employs the ivreg2 command with the first option, which outputs the first-stage F-statistic. This step is crucial for testing the validity of the instruments, as weak instruments can lead to biased IV estimates. The chosen instruments real_energytax_with_vat and real_oil_price_sek provide exogenous variation, allowing the analysis to disentangle the causal impact of carbon taxes on gas consumption. This code offers some distinct features that could be interpreted as complementary to or extending synthetic control approaches and are applied in Table 3: The Use of Newey-West Standard Errors: Newey used in the first 4 columns: Instead of focusing solely on the treatment and control group comparisons typical in synthetic control analysis, this code uses Newey-West standard errors to handle potential autocorrelation and heteroskedasticity in time series data. This is particularly important for robustness when working with longitudinal data over multiple years. Traditional standard error estimation assumes that: Residuals are independent and identically distributed (i.i.d.):The errors are uncorrelated across observations and have constant variance (homoscedasticity). No autocorrelation exists: Residuals from one observation are not systematically related to residuals from others. When to Use the Newey-west command: When working with cross-sectional data where the assumption of i.i.d. errors is reasonable. If residuals are uncorrelated across time or units (no serial correlation). Key Features: Lag Structure: Newey-West requires specifying a lag length lag(16) in the code, representing the maximum distance across which residuals are assumed to be correlated. HAC (Heteroscedasticity and Autocorrelation-Consistent): The method constructs standard errors that remain valid under general forms of heteroscedasticity and autocorrelation. Kernel Weighting: Residuals beyond the specified lag length are down-weighted using a kernel function. Why do we use Instrumental Variables in columns 5 and 6 (ivregress 2sls and ivreg2) ? The use of Instrumental Variables (IV) in columns 5 and 6, alongside a synthetic control methodology, is somewhat unconventional. Typically, the synthetic control method (SCM) is a standalone tool used to construct a counterfactual for a treated unit without explicitly requiring instruments, Synthetic control methods typically rely on weighted averages of untreated units to construct a counterfactual for the treated unit. Here, the code introduces instrumental variables (IV) to address endogeneity concerns, leveraging instruments like real_energytax_with_vat and real_oil_price_sek. This hybrid approach (combining regression and IV techniques) could be seen as an innovation relative to standard synthetic control frameworks to bring more robustness Using IV methods in conjunction with synthetic control methodologies is not common practice, but it is an emerging area of research, When confounding factors are suspected to bias the construction of the synthetic control (concerns that the synthetic control weights or the pre-treatment trends are influenced by endogenous factors (e.g., omitted variables like energy prices)), IV helps correct for this bias. The inclusion of these IV estimations alongside SCM ensures that the results are robust and account for potential biases not addressed by synthetic control alone. This combination demonstrates methodological innovation, bridging SCM’s strength in constructing counterfactuals with IV’s ability to address endogeneity. Additionally a Complementarity, SCM provides a framework for comparison, while IV ensures unbiased estimates under endogenous treatment variables. The use of Tests for Weak Instruments and Parameter Comparisons test and ivreg2: The code evaluates the strength of IVs and performs parameter tests, which adds robustness checks typically absent in straightforward synthetic control implementations.Generally, this framework complements the synthetic control methodology bringing additional econometric rigor and flexibility. 6.2.5.2 Modelling Figure 14 Creating data set disentangling_data.dta used for Figure 14 First, we predict gasoline consumption using the full model newey log_gas_cons real_carbontaxexclusive_with_vat real_carbontax_with_vat d_carbontax t real_gdp_cap_1000 urban_pop unemploymentrate , lag(16) predict yhat * Second, we predict gasoline consumption without the carbon tax preserve newey log_gas_cons real_carbontaxexclusive_with_vat real_carbontax_with_vat d_carbontax t real_gdp_cap_1000 urban_pop unemploymentrate , lag(16) replace d_carbontax=0 replace real_carbontax_with_vat=0 predict yhat_nocarb restore * Third, we predict emissions without the carbon tax and without the VAT preserve newey log_gas_cons real_carbontaxexclusive_with_vat real_carbontax_with_vat d_carbontax t real_gdp_cap_1000 urban_pop unemploymentrate , lag(16) replace real_carbontaxexclusive_with_vat= real_carbontaxexclusive replace d_carbontax=0 replace real_carbontax_with_vat=0 predict yhat_nocarb_novat restore The predictions are a crucial part of the synthetic control in this paper and for the econometric evaluation methodology. They enable the creation of counterfactual outcomes to isolate and measure the effects of the carbon tax and VAT on gasoline consumption. Counterfactual Analysis: These predictions create what-if scenarios that allow us to measure the effects of policies: Without the carbon tax. Without the carbon tax and VAT. Comparing actual consumption (yhat) to these counterfactuals isolates the causal effects. Decomposition of Effects: By generating multiple counterfactuals: yhat_nocarb measures the effect of the carbon tax alone. yhat_nocarb_novat measures the combined effect of the carbon tax and VAT. This helps disentangle the contributions of each policy. Figure 14: Gap in Per Capita CO2 Emissions from Transport: Synthetic Control versus Simulation use &quot;disentangling.dta&quot;, clear list year CO2_reductions_simulation CO2_reductions_synth in 1/46 * Here I Create the graph with CO2 reductions for both Synthetic Control and Simulation twoway (line CO2_reductions_synth year, lcolor(black) lwidth(medium)) /// (line CO2_reductions_simulation year, lcolor(gs14) lwidth(medium)), /// xlabel(1960(5)2005, grid) ylabel(-0.8(0.2)0.6, angle(horizontal)) /// xtitle(&quot;Year&quot;) ytitle(&quot;Gap in metric tons per capita (CO2 from transport)&quot;) /// yscale(range(-0.8 0.6)) /// xline(1990, lpattern(dash) lcolor(black) lwidth(medium)) /// legend(label(1 &quot;Synthetic Control result&quot;) label(2 &quot;Simulation result&quot;) position(2) ring(0) col(1) size(medium)) twoway (function y=-0.845, range(2000 2004.9) lcolor(gs15) fcolor(gs15) lwidth(none)) /// (function y=0, range(1960 2005) lpattern(dash) lcolor(black) lwidth(medium)) /// , addplot (scatter 0 1987, msymbol(none) mcolor(black)) /// addplot (scatter 0 1989, msymbol(none) mcolor(black)) /// addplot (line 0.3 1987 1989, lcolor(black) lpattern(solid) lwidth(medium)) /// text(1981 0.3 &quot;VAT + Carbon tax&quot;, size(medium) color(black)) Figure 14 illustrates the reduction in CO₂ emissions in the transport sector in Sweden, comparing estimates from the synthetic control method and the simulation, which align closely before the introduction of the carbon tax but diverge significantly afterward. Why should you use a Pre-Intervention Fit? Drawing the synthetic control before 1990 is a standard and critical practice when implementing the Synthetic Control Method (SCM). The reasons for doing so: Pre-Intervention Fit: Showing the synthetic control before the intervention (pre-1990) demonstrates how well the synthetic control matches the treated unit during the pre-policy period. A good pre-policy fit increases confidence that the synthetic control represents a valid counterfactual.`` Validation of the Counterfactual: By comparing the pre-1990 trajectory of the observed outcome (e.g., CO2 emissions) to that of the synthetic control, we can visually assess whether the method correctly estimates what would have happened in the absence of the policy. If the lines for the actual and synthetic outcomes align well before the intervention, the synthetic control is validated as a reliable counterfactual. Benchmark for Causal Inference: The difference between the treated and synthetic control after 1990 (post-policy period) is interpreted as the causal effect of the policy. The pre-1990 comparison serves as a baseline to confirm there were no significant differences before the intervention, ensuring that observed gaps after 1990 are attributable to the policy, not pre-existing trends or selection issues. The Simulation Result Line The main innovation in this figure lies in the use of simulation, designed to isolate the specific effect of the carbon tax on CO₂ emissions. This approach is based on a theoretical model that projects emissions according to annual adjustments of the carbon tax, while assuming that other factors remain constant. The simulated reductions are estimated for each year by incorporating the actual variations in the carbon tax. Why use simultaneously the SCM and the simulation? it helps mitigate the weighting bias inherent in the SCM while avoiding reliance on the theoretical assumptions of the simulation, such as a constant price elasticity and invariant external factors, for the entirety of the analysis. Graph Customization: One of the interesting aspects of the code in Figure 14 is the use of msymbol(none) to mark specific points, such as the years 1987 and 1989, while suppressing their visual display. This function allows you to mark precise textual annotations linked to these points without overloading the graph. Legend customization is another important feature you can use, especially when it comes to keeping your graph clear and easy to read. Placing the legend in the top-right corner will involve using commands like ring(0) to prevent it from overlapping the plot, and col(1) to arrange the entries in a single column. Authors: Nicolás Ortega, Luna Baroni, and Alice Brossard, students in the Master program in Development Economics and Sustainable Development (2024-2025), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2024 "],["rebel-on-the-canal-disrupted-trade-access-and-social-conflict-in-china-1650-1911.html", "6.3 Rebel on the Canal: Disrupted Trade Access and Social Conflict in China, 1650-1911", " 6.3 Rebel on the Canal: Disrupted Trade Access and Social Conflict in China, 1650-1911 Cao, Y., &amp; Chen, S. (2022). Rebel on the canal: Disrupted trade access and social conflict in China, 1650–1911. American Economic Review, 112(5), 1555-1590. https://www.aeaweb.org/doi/10.1257/aer.20201283 [Download Do-File corresponding to the explanations below] [Download Codebook] [Link to the full original replication package paper from Open ICPSR] Highlights Research question: How did the closure of the Grand Canal in 1825 affect social unrest in China? Methodology: A Differences-in-Differences (DiD) approach combined with: Heterogenuous treatment effects by spatial proximity and exposure Robustness checks using the Synthetic Control Method (SCM) Baseline Model: A Two-Way Fixed Effects (TWFE) approach Standard Approach: Methodology aligns with standard DiD approaches (only one treatment period), employing fixed effects to control for unobserved heterogeneity and comparing treated versus control groups over time. Originality: SCM and spatial standard errors to address concerns about differential pre-trends and geographic dependencies. Value Added: This replication provides detailed explanations and simplifications of the original analysis, introducing advanced techniques like Conley standard errors and SCM for applied researchers. Stata Tricks: Using reghdfe for high-dimensional fixed effects. Computing Conley spatial standard errors for panel data. Generating and visualizing synthetic controls using synth_runner. 6.3.1 Background Economists often focus on how a specific treatment impacts an outcome of interest. In this study, the closure of the Grand Canal in 1825 serves as a natural experiment to investigate the broader social implications of trade disruptions. The Grand Canal, historically linking Beijing and Hangzhou, was central to economic integration, facilitating the movement of goods, tribute rice, and people across vast distances. By the early 19th century, however, mounting maintenance costs and governmental exploration of alternative trade routes led to its gradual abandonment. This event created what economists call a “treatment reversal,” providing an opportunity to study the relationship between trade access and social conflict. Why focus on this period? The Grand Canal’s decline coincided with significant political and economic transitions in China, making it an ideal case study for understanding the socio-economic consequences of disrupted infrastructure. The loss of this critical trade artery not only impacted economic activity but also heightened social grievances, culminating in increased incidents of rebellion. These dynamics underscore the intertwined nature of economic and political stability. The Methodology: Economists frequently employ the Differences-in-Differences (DiD) framework to estimate causal effects by comparing outcomes in treated and control groups over time. However, challenges arise in accounting for: Time trends: Changes over time that affect all regions equally. Cross-sectional differences: Variations between treated and control regions at a single point in time. By combining both temporal and cross-sectional comparisons, the DiD approach mitigates these biases. In this study, counties along the canal (treated) are compared to those further away (controls) before and after 1825, the year marking the beginning of alternative trade route exploration. We will provide you with the code for some of the main results and graphs from the paper. Since the idea is that this document will help you understand the basic idea behind the concepts and codes we will often simplify and shorten the actual code. The full code will be found in the Do-File. 6.3.2 Empirical Approach and Descriptive Statistics Although the canal was officially closed much later—by 1901—the authors argue that the causal effect was a result of the declining use starting in 1826. The closure significantly reduced rice transportation volumes, as illustrated in Figure 1. The discontinuity observed starting in 1826 suggests an immediate and marked decline in canal activity, aligning with the government’s exploration of alternative trade routes. Visualizing the Context: The authors use historical data on tribute rice transportation to illustrate the canal’s declining role. Tribute rice, a critical commodity transported via the canal, provides a proxy for trade activity. The graphing exercise involves visualizing trends in rice shipments from 1755 to 1860, with a clear discontinuity emerging after 1825. Regression lines (lfit) for pre- and post-reform periods emphasize the shift, while a vertical line (xline) marks the pivotal year. Customization options such as line widths (lwidth), colors, and patterns enhance the clarity of the visualization. Figure 1 Stylized Trends in Rice Transportation: Figure 1 highlights the evolution of rice shipments before and after the onset of canal decline. Two fitted regression lines delineate the pre- and post-1826 periods, with a sharp break at the transition year. This visualization reinforces the absence of significant pre-treatment changes, suggesting that the observed decline is directly attributable to the policy shift rather than prior trends. duplicates drop year, force keep if year &gt; 1755 &amp; year &lt;1860 twoway (lfit lamount year if year &lt; 1826, lwidth(1.5pt) color(black) lpattern(dash)) /// (lfit lamount year if year &gt;= 1826, lwidth(1.5pt) lpattern(dash) color(black)) /// (scatter lamount year, color(gs10)), /// xline(1825, lwidth(2pt) lcolor(maroon)) /// xtitle(&quot;Years&quot;) ytitle(&quot;Shipping Volume (log million piculs)&quot;) legend(off) /// title(&quot;Figure 1: Canal Usage measured by Tribute Rice Transportation&quot;) subtitle(&quot;1755 - 1860&quot;) graph export figure1.png, replace Explanation: The dataset is restricted to 1755–1860 by dropping years outside this range. twoway combines multiple graph elements: - lfit: Fits regression lines for pre- and post-reform periods. scatter: Plots individual data points. - xline: Adds a vertical line at the treatment year (1825). Graph customization includes: Line width (lwidth), color (color), and pattern (lpattern) for regression lines. Axis titles (xtitle, ytitle), overall title (title), and subtitle (subtitle). The legend(off) command disables the legend, simplifying the output. The graph is exported using graph export, with the replace option overwriting existing files. 6.3.3 Main Results The baseline model estimates the relationship between canal access and the frequency of rebellions. The key dependent variable is the inverse hyperbolic sine transformation of rebellion counts, normalized by 1600 population. The treatment variable is an interaction term for counties along the canal (AlongCanal_c) and the post-reform period (Post_t). Model Components: Fixed effects: - Time (\\(\\sigma_t\\)) and county (\\(\\delta_c\\)). Additional controls for geography, agriculture, and demographics (added incrementally). Estimation: Columns 1–5 in Table 3 progressively add controls, including pre-reform trends, province-year fixed effects, and prefecture-level time trends. We provide only one of the columns for simplicity Model form: \\(Y_{c,t}\\) = \\(\\beta\\) * \\(AlongCanal_c\\) x \\(Post_t\\) + \\(\\delta_c\\) + \\(\\sigma_t\\) + \\(chi_{ct}\\) + \\(\\epsilon_{ct}\\). Table 1 Replication Code: The baseline TWFE models are estimated using reghdfe, a Stata command for high-dimensional fixed effects. Results are stored with eststo and formatted into LaTeX tables with esttab. use &quot;.../Rebellion.dta&quot;, clear global ctrls larea_after rug_after disaster disaster_after flooding drought flooding_after drought_after lpopdencnty1600_after maize maize_after sweetpotato sweetpotato_after wheat_after rice_after reghdfe ashonset_cntypop1600 interaction1, absorb(i.OBJECTID i.year) cluster(OBJECTID) eststo Column1 quietly tabulate OBJECTID if e(sample) scalar groups=r(r) qui su ashonset_cntypop1600 if e(sample) scalar ymean=r(mean) estadd scalar depavg=ymean:Column1 estadd scalar N_g=groups:Column1 Commands: reghdfe: Estimates TWFE models with absorb() for fixed effects. Different from xtreg since we can include multiple fixed effects without having to use dummies in our equations cluster(OBJECTID): Clusters standard errors at the county level to account for serial correlation within counties scalar() allows us to get certain values for our table (here: number of clusters and the mean of the dependent variable) eststo and estadd: Store results and add summary statistics (e.g., mean of dependent variable). The above steps are repeated for Columns 2–5, progressively adding fixed effects and controls. Spatial Conley Standard Errors: To address spatial correlation, Conley standard errors are computed using the user-written command available here: http://www.globalpolicy.science/code/. Results are added to stored estimates. preserve hdfe ashonset_cntypop1600 interaction1, clear absorb(i.OBJECTID i.year) tol(0.001) keepvars(OBJECTID year Y_COORD X_COORD) ols_spatial_HAC ashonset_cntypop1600 interaction1, lat(Y_COORD) lon(X_COORD) time(year) panel(OBJECTID) distcutoff(500) lagcutoff(262) disp star matrix V_spat=vecdiag(e(V)) matmap V_spat SE_spat, m(sqrt(@)) estadd matrix sesp=SE_spat: Column1 restore Commands: preserve and restore allow us to manipulate the dataset and reset it after we run our commands hdfe allows us to account for multiple fixed effects in the next step which is an OLS model. The dependent variable will now be the initial values minus the corresponding fixed effects for each observation ols_spatial_HAC user written code that estimates OLS models accounting for spatial (here 500km) and serial (262 periods) correlation. the rest of the steps are necessary to save the computed values for later implementation into our table. estfe Column1 Column2 Column3 Column4 Column5 esttab Column* using table3.tex , compress keep(interaction1 _cons) se(4) nomtitles nonotes /// cells(b(fmt(4)) se(fmt(4) par(( ))) sesp(fmt(4) par([ ]) drop(_cons))) collabels(&quot;&quot;,none) /// sca(OBJECTID) stats( depavg N N_g r2_a, fmt( 3 %7.0fc 0 4) labels( &quot;Mean of the dependent Variable&quot; &quot;Number of observations&quot; &quot;Number of counties&quot; &quot;Adjusted R^2&quot;)) label title(&quot;Rebellions&quot;) /// indicate(&quot;County FE =0.OBJECTID&quot; &quot;Year FE=0.year&quot; &quot;Pre-reform rebellion $\\times$ Year FE=0.year#c.ashprerebels&quot; &quot;Province $\\times$ Year FE=0.provid#0.year&quot; &quot;Prefecture Year Trend=0.prefid#c.year&quot; &quot;Controls $ \\times $ Post=$ctrls &quot;) eststo clear Final Steps to get a nice table: indicate will tell Stata when to display the yes concerning which fixed effects are included cells() to manually define which values to include (sesp() are the Conley SEs) labels(), compress, keep() etc. for a nicer output 6.3.4 Robustness Checks The robustness checks include: Pre-trend Analysis: Confirms no pre-treatment effects. Placebo Tests: Examines other trade routes and finds no similar effects. Synthetic Control Method: Constructs artificial counterfactuals for treated counties using synth_runner. SCM A6a SCM A6b Our focus in this section will be the synthetic control method. The idea is that instead of comparing treated and non-treated groups we design an artificial counterfactual. This sounds complex but is rather intuitive. Remember some counties were exposed to the canal and others weren’t. We look at individual counties and design the synthetic control by finding a combination of the non-treated counties that best fits this observation (before treatment). Imagine that county A is actually represented fairly accurately by a combination of B, C and D. We tell Stata to compute the best fitting combination, by assigning weights to each control county, to yield our counterfactual. We then check how accurate our counterfactual is before treatment (how close the lines are) and then we see how our synthetic control evolves over time after treatment and how the actual observation behaves. The observed gap will be our treatment effect. What is cool about the method is that it is told using graphs so interpretation is quite intuitive. Replication Code for SCM: synth_runner y y(1776) y(1796) y(1806) y(1816), d(interaction1) gen_var matrix P = e(pvals_std) /// Intermediate and Setup omitted for simplicity (see Do-File for full code) twoway (connected y year, lpattern(solid) msymbol(C) color(black)) /// (connected y_synth year, lpattern(dash) msymbol(T) color(gs10)), /// ytitle(&quot;Coefficients&quot;) /// xtitle(&quot;Number of years since the 1826 reform&quot;) /// legend(order(1 &quot;Canal counties (treated)&quot; 2 &quot;Synthetic controls&quot;) cols(2)) twoway (line effect year, c(l) yaxis(1) lpattern(solid) color(black) ytitle(&quot;Treatment effects&quot;, axis(1)) ylabel(-0.2(0.2)0.8,axis(1))) /// (scatter p_vals year, yaxis(2) color(gs10) ylabel(0.0(0.2)1.0, axis(2)) ytitle(&quot;p-values&quot;, axis(2))), /// xtitle(&quot;Number of years since the 1826 reform&quot;) /// xline(-5, lpattern(dash) lcolor(maroon)) /// /// xlabel(-50(10) 70) /// graphregion(color(white)) /// scheme(s2color) /// legend(order(2 &quot;P-values&quot; 1 &quot;Treatment effects&quot;) cols(2)) Commands synth_runner is the more flexible approach - compared to synth - for estimating synthetic controls. y is the dependent variable and the y() specify which pre-treatment values to use to construct the synthetic control the option d() specifies the treatment variable and gen_vars saves the estimated results as variables in the dataset In the next steps we save the estimated p-values (placebo based inference) in the matrix P placebo based inference is one of the advantages of the synth_runner command. The resulting p-values don’t originate from any defined distribution but represent the position of the estimated coefficients compared to the effect sizes of about 100,000 placebo estimations connected in the first graph streamlines the alternative combination of a scatter and line graph In the second graph we follow standard practice. This is different to the authors’ code so we get slightly different axis labeling but overall results are mostly the same (except for one p-value) 6.3.5 Discussion and Interpretation The study “Rebel on the Canal” by Cao and Chen examines the causal relationship between the disruption of trade access—caused by the abandonment of China’s Grand Canal—and subsequent social unrest in the region. The authors build an original dataset spanning 575 counties over 262 years to investigate how a plausibly exogenous shock to trade infrastructure contributed to increased rebellion frequency in counties along the canal. Key Findings 1. Direct Impact of Canal Closure: Counties with sections of the Grand Canal experienced a 117% increase in rebellion frequency compared to non-canal counties after the closure. This robust effect persisted across various model specifications. The effect size is to be interpreted carefully since there are some concerns with respect to the arcsinh transformation (see Chen &amp; Roth, 2024: https://doi.org/10.1093/qje/qjad054). The mangitude of this impact highlights the importance of trade routes in maintaining regional stability, suggesting that access to trade mitigates socio-economic and spatial inequalities that could lead to social unrest Heterogeneity of Effects: The analysis revealed that counties more reliant on the canal, either geographically (based on canal length within a county) or economically (proximity of towns to the canal), experienced greater rebellion intensity. Spillover effects extended to counties up to 150 kilometers away from the canal, indicating broader regional dependencies on this critical infrastructure. Mechanisms: Loss of trade access significantly reduced economic opportunities, particularly for urban workers reliant on the canal for employment. The closure disrupted market town growth and deprived regions of the risk mitigation benefits traditionally afforded by trade, such as smoothing income shocks during adverse events. Rigorous Robustness Checks The Difference-in-Differences (DiD) framework is supplemented by advanced techniques like the Synthetic Control Method (SCM) and Changes-in-Changes (CiC) estimation to address concerns about differential pre-trends and unobserved heterogeneity. Placebo tests on other trade routes confirm that the observed effects were unique to the Grand Canal’s abandonment, strengthening the causal inference. Broader Implications Trade Access and Stability: Findings reinforce dual role of trade in fostering economic prosperity and political stability, a topic with extreme relevance given recent debates on globalization and trade restrictions. The study shows how disruptions to economic systems can reveal existing vulnerabilities, potentially informing policy discussions on infrastructure resilience and equitable access. Urban Dynamics: Focus on urban workers underscores socio-economic importance of trade infrastructure for urban centers, expanding traditional narratives that emphasize rural impacts. Historical Contributions: Provides new insights into the dynamics of 19th-century Chinese social unrest, connecting the rise in rebellions to economic shocks rather than solely to political or military factors. Future Research Directions Investigate long-term socio-economic trajectories of regions affected by the canal’s closure with respect to trust for instance Extending the analysis to explore how modern disruptions in trade infrastructure - such as those caused by climate change - might replicate similar patterns of instability Examine similar cases globally to test generalizability of findings Authors: Ibrahim Ben Araar, Nour Ouljihate and Marie Moussé, students in the Master program in Development Economics and Sustainable Development (2024-2025), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2024 "]]
