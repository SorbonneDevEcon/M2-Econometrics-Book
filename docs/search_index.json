[["index.html", "Welcome to our replication textbook!", " Welcome to our replication textbook! This textbook is designed by students for students interested in applied econometrics, under the guidance of their econometrics professors. Our goal is to present, explain, and share with you substantial Stata code chunks based on real-life examples from well-published articles. In this textbook, you’ll find questions, strategies, and results integrated with the code to enhance clarity and spark curiosity. Our content caters to a wide audience, ranging from beginners in Econometrics 101 to advanced learners. We cover various methodologies, with examples tailored to different skill levels. Our starting point is the existing Stata code available on open data platforms like Dataverse, OpenICPSR, or from the researchers themselves. From these extensive codes, we extract a few examples and refine them to give the code chunks a more pedagogical flavor. Each section replicates a paper with a main result, a figure (when available), and a robustness test (when relevant). Each replication has its own identity, style, and tone, but all include a ‘Highlights’ section explaining the replication and Stata tricks, along with buttons to download the original datasets, a student-created do-file, and a student-created codebook. We are deeply indebted to the authors of the cited articles for their original replication packages. All errors, however, remain ours. Please also note that our replication exercises are not intended to verify or validate findings. We hope you find this textbook informative and engaging as you delve into the world of econometrics. Happy learning! This ongoing project started in the 2023/2024 academic year and continues this upcoming year, with new examples to come! The authors are students in their second year of master’s in Development Economics and Sustainable Development at the Université Paris 1 Panthéon-Sorbonne. We are financed by the Service des Usages Numériques at Université Paris 1. More information about their projects and initiatives can be found here. We thank the University Paris 1 Panthéon-Sorbonne and the Sorbonne School of Economics for their continuous support in this project. "],["randomized-control-trials.html", "Chapter 1 Randomized Control Trials ", " Chapter 1 Randomized Control Trials "],["subsidies-and-the-african-green-revolution-direct-effects-and-social-network-spillovers-of-randomized-input-subsidies-in-mozambique.html", "1.1 Subsidies and the African Green Revolution: Direct effects and social network spillovers of randomized input subsidies in Mozambique", " 1.1 Subsidies and the African Green Revolution: Direct effects and social network spillovers of randomized input subsidies in Mozambique Carter, M., Laajaj, R., &amp; Yang, D. (2021). Subsidies and the African Green Revolution: Direct effects and social network spillovers of randomized input subsidies in Mozambique. American Economic Journal: Applied Economics, 13(2), 206‑229.https://doi.org/10.1257/app.20190396 Download Do-File corresponding to the explanations below Download Codebook Download Original Data (Moz1234panel.dta from Open ICPSR repository) [Link to the full original replication package paper from Open ICPSR] Highlights The paper investigates the short-term and long-term effects (i.e. diffusion through farmers’ networks) of an input subsidy program on agricultural yields. The authors rely on a standard Randomized Controlled Trial in which farmers are randomly assigned to the treatment and control groups, eliminating potential selection bias. Yet, it departs from the standard by exploring spillover effects through social networks, assessing long-term impacts, and addressing informational market failures. The study emphasizes learning processes as part of the intervention’s impact, extending beyond the typical scope of a standard RCT. This document provides a detailed explanation of the original replication package. But you will also learn: how to create a beautiful coefficient plot tailored to your needs, with estimates stored in a matrix (this is how the authors display the main results of their paper) using mkmat together with coefplot, how to create a result table with outreg2, while adding a few statistical tests tailored to your needs in the bottom part with test and scalar, how to automatically generate a new label by keeping / removing certain parts of an existing label with subinstr, how to use a local macro together with replace and append commands in order to loop over a list of variables and store all their corresponding results into a fresh table every time the code is run. 1.1.1 Background elements This study examines the impact of a one-off input subsidy program implemented in Mozambique in 2010 within the context of the Green Revolution and the Alliance for a Green Revolution in Africa. While the Green Revolution transformed Asian and Latin American agriculture, Sub-Saharan Africa lagged behind. In response, the Maputo Declaration (2003) committed African nations to invest 10% of their budgets in agriculture. Launched in 2006, the Alliance aimed to catalyze the Green Revolution in Africa through input subsidy programs (ISPs), providing technologies at discounted rates to randomly selected farmers. Focusing on Mozambique’s 2010 Programa de Suporte aos Produtores, this paper not only assesses subsidy impacts on 704 farmers but also explores post-subsidy persistence and spillover effects through social networks. Findings reveal increased technology adoption, sustained maize yield growth post-program, and notable impacts on farmers’ networks. The program also fostered enhanced beliefs about technology returns, mitigating information-related market failures. 1.1.2 Replication of Table 2 - Regressions with spillover effects This table (Table 2 in the paper) is the output you will get from the code we explain below. Table 2 examines the impact of the government-implemented input subsidy program (ISP) on various agricultural outcomes: fertilizer use on maize, adoption of improved maize seeds, maize yield, daily consumption per capita, and expected yield with the technology package. The objective of this analysis is to determine whether the ISP has a positive and statistically significant impact on the use of fertilizer on maize and on maize yields, measured as average maize yield per hectare. How to read this table. The first two lines of this table present the program’s results estimated on treated farmers (those who received the subsidy). The effects of the program are assessed both during and after the ISP implementation. This table reveals that the program has a positive impact on all farmers’ outcomes, both during and after the program. The effects are more pronounced during the treatment period for fertilizer use, improved maize seeds, and maize yields. The next lines present the impact of the ISP on social network spillovers, which is estimated on members of treated farmers’ social networks during (round 2) and after (rounds 3 and 4) the treatment period, respectively. The general pattern observed is that the coefficients for social network variables are positive and significant in the post-intervention period but not in the intervention period. The magnitude of the coefficients increases as the number of social network contacts in the treatment group increases from one to two, and then stabilizes. This pattern suggests that the effect of social networks on technology adoption and yield increases sharply at two or more social network contacts in the treatment group. For example, after the program, having one contact in the treatment group increases maize yield by 0.18 kilogram per hectare, while having two contacts increases maize yield by 0.53 kilogram per hectare. This substantial increase in yield with two or more social network contacts justifies the authors’ estimation of spillover effects on individuals having above-median (two or more) social network members in the treatment group in Figure 2. Replication code for Table 2 Data importation After having downloaded the original dataset and saved this dataset under your working directory defined with command “cd” (for current directory), you can generate a shorter version of the dataset, named here Dataset_CLY.dta, for which we provide a detailed codebook. *A/ Download and save data &quot;Moz1234panel.dta&quot; from: https://www.openicpsr.org/openicpsr/project/116761/version/V2/view?path=/openicpsr/116761/fcr:versions/V2.1/Rep-file-Moz-Input-Subsidy/data/original/Moz1234panel.dta&amp;type=file *B/ Set your working directory where Moz1234panel.dta already is: cd &quot;XXX&quot; /*XXX=the directory where you just saved the &#39;Moz1234panel.dta&#39; dataset*/ *C/ Create a shorter dataset: use &quot;Moz1234panel.dta&quot;, clear * selecting the necessary avriables for replicating the Table 2, Figure 2, and Table A7 keep sn5up_sub_dur sn2up_sub_aft nw_talkedagmoder lfertmaizr sn2_sub_dur sn2up_sub_dur sn1_sub_aft lyieldr sn3_sub_aft lexp_yield_fertr vouch_aft_r3 sn2up_sub_aft_r3 limprovedseedsr vouch_aft sn1_sub_dur vlgid_round respid sn2up_sub_aft_r4 vouch_aft_r4 vouch_dur ldailyconsr sn3_sub_dur sn4_sub_aft sn5up_sub_aft sn4_sub_dur sn2_sub_aft fertmaizr vouch round * exponentiation of the winsorized variables back to their original forms foreach x in improvedseedsr yieldr dailyconsr exp_yield_fertr{ // it is necessary in order to add the means of the control to the table 2. This way it uses the value that is not Winsorized. gen `x&#39; = exp(l`x&#39;) } save &quot;Dataset_CLY.dta&quot;, replace To start the replication, one should always start by deleting any potential data in memory using clear all. Then, open the dataset of interest, and the command use “…”, clear to indicate the dataset you want to import in Stata. Because this project uses panel data, you need to inform Stata of the structure of the data. Use the command xtset to indicate that you are working with panel data, followed by the name of the variable you want to set as the panel variable. Here, vlgid_round is an identifier for every possible combination of locality and round. clear all use &quot;Dataset_CLY.dta&quot;, clear xtset vlgid_round global sn_treatments sn1_sub_dur sn2_sub_dur sn3_sub_dur sn4_sub_dur sn5up_sub_dur sn1_sub_aft sn2_sub_aft sn3_sub_aft sn4_sub_aft sn5up_sub_aft Now, you want to modify the labels for a set of variables that were in log format in the original dataset, to indicate that they are in their original scale. Start with creating a loop to iterate the same procedure over a set of variables, using the foreach command. Stata trick: Then, you can easily modify the labels using variable label to extract the label for each variable in the loop, remove the text “log” from the label using subinstr, remove the excess spaces using trim, and update the variable label with the modified text. foreach v in fertmaizr improvedseedsr yieldr dailyconsr exp_yield_fertr { local x : variable label l`v&#39; local x = trim(subinstr(&quot;`x&#39;&quot;,&quot;(log)&quot;,&quot;&quot;,.)) label variable l`v&#39; &quot;`x&#39;&quot; } Regressions, and table creation You are now set to running the regressions and creating a table presenting results. Stata trick: Start with initializing a local macro using local, named rep_app, and set its value to replace. The purpose of this macro is to control whether the replace option is used when exporting regression results in an Excel file using the command outreg2. In other words, it means that if the file already exists in your computer, it will be replaced. local rep_app = &quot;replace&quot; Again, use a loop to iterate regressions over the same list of variables using foreach. You want to apply several commands to the variables included in the loop only if they meet the following conditions: round==2 (survey round 2, that is, during the program) and vouch==0 (farmers who did not win the voucher, that is, the control group). Then: qui sum calculates the mean, and results are stored using the local command areg runs a fixed effects regression on panel data, using absorb to absorb fixed effects, and cluster to take into account the intra-cluster correlations outreg2 creates the regression table and saves it into an Excel file (.xls extension) Then, create a nice table using key commands: bracket generates brackets for standard errors label includes variable labels in the results table to display full variable names nocons, nor2, and noni are used to avoid displaying the constant, the R-squared and missing values in the table less(1) means that only the first row of the result table will be included (only the first regression) keep allows you to keep only the variables you want adds() is used to add control variables into the brackets foreach x in fertmaizr improvedseedsr yieldr dailyconsr exp_yield_fertr { qui sum `x&#39; if vouch==0 &amp; round==2 local control_mean=r(mean) qui xi: areg l`x&#39; vouch_dur vouch_aft ${sn_treatments} i.nw_talkedagmoder*i.round, /// absorb(vlgid_round) cluster(respid) outreg2 using &quot;table 2.xls&quot;, `rep_app&#39; bracket label nocons noni /// less(1) nor2 keep(vouch_dur vouch_aft ${sn_treatments}) adds(mean_control,`control_mean&#39;) local rep_app = &quot;append&quot; } 1.1.3 Replication of Figure 2 - Direct and Spillover Impacts of Subsidies This figure (Figure 2 in the paper) is the output you will get from the code we explain below. This figure highlights the Intention-to-Treat estimates through the direct and indirect effects on five outcomes of the subsidy program on the farmers and on their social networks. The outcomes of interest include the adoption of fertilizer and adoption of improved maize seeds, the maize yields, the consumption of the household (an indirect measure of agricultural profit), and the expected yields to the technology package. In this figure, indirect effects are estimated on individuals having at least two contacts in the treatment group, because they are the ones for whom effects are the largest, as demonstrated in Table 2. How to read this figure. The coefficient of the regressions of interest are represented with dots, while the lines represent 95 percent confidence intervals. Direct impacts of the subsidies on the voucher recipient group are estimated for all five outcomes for both the “during” (subsidized) and the “after” (post-subsidy) periods. The Figure shows that the program had a large and significant impact during the subsidy period on the adoption of fertilizers and improved seeds, also allowing for higher maize yields. The effects on consumption were only significant after the subsidy period. Yield expectations were stable across periods for treated farmers. While the effect on fertilizer use decreases in magnitude after the subsidy period, it remains substantial and statistically significant. The impact on agricultural yields also persists over time. Regarding spillover effects, impacts of the program are positive and significant on the adoption of fertilizers, on improved seeds, maize yields, and expected returns to the technology package. Replication code for Figure 2 Data preparation and confidence intervals calculation So far, a lot of modifications have been made to the variables. To construct the figure, you need the original data, without saving the transformations to the variables made until now. use &quot;Dataset_CLY.dta&quot;, clear Install the parmest package that allows for semi-parametric estimation of partially linear models. ssc install parmest Then, regress the outcome variables on the treated individuals (farmers who won the vouchers) and on individuals who have at least two contacts in the treatment group. Regressions are run separately for each of the five outcomes, using a loop - see section 1.2.2 for more information on how to create a loop. Cluster the standard error at the level of respid (respid is the variable for the respondent’s identifier, that is, cluster at the individual level), and use parmest to save the regression results in the .dta format. For each variable, you will obtain the estimated coefficients and standard errors for the periods during and after the program, for treated farmers and those with at least two contacts in the treatment group. foreach x in lfertmaizr limprovedseedsr lyieldr ldailyconsr lexp_yield_fertr { qui xi: reg `x&#39; vouch_dur vouch_aft sn2up_sub_dur sn2up_sub_aft /// i.nw_talkedagmoder*i.round i.vlgid_round, cluster(respid) parmest, saving(`x&#39;, replace) } To generate the final figure, you must create a new dataset. To do so, you need to iterate through variable names and perform certain operations on our variables. First, use the local command to define a local macro y with a value of 1. Then create a loop using foreach which includes our variables of interest. Then, apply the following commands on variables in the loop: use `x'.dta loads the dataset corresponding to the variables in the loop use the command gen to create a time variable using the observation number (_n), drop the observations for which time is greater than 4 using drop This graph aiming to generate a visually informative figure by plotting mean estimates along with their corresponding 95% confidence intervals for a set of variables. Only keep the variables estimate, stderr and time Use the round command to round the values of the variables to 2 decimal points Generate, using gen, two new variables with missing values which capture the estimates of each regression after the treatment and the standard error of these coefficients (respectfully estimate_NV and sd_NV, followed by the value taken by y). The command replace allows us to assign specific values to the new variables. With the command rename, include a suffix based on the local macro y’ to the selected variables. Drop all periods after round 2 because you only want the first two periods in the figure Creating a local y equal to y + 1, increment y to the next rank to move on to the next variable estimate_Vy+1 and estimate_NV+1 Eventually, save the variables created with the command save. local y = 1 foreach x in lfertmaizr limprovedseedsr lyieldr ldailyconsr lexp_yield_fertr { use `x&#39;.dta gen time=_n drop if time&gt;4 keep estimate stderr time replace estimate=round(estimate,0.01) replace stderr=round(stderr,0.01) gen estimate_NV`y&#39;=. replace estimate_NV`y&#39;=estimate[3] in 1 replace estimate_NV`y&#39;=estimate[4] in 2 gen sd_NV`y&#39;=. replace sd_NV`y&#39;=stderr[3] in 1 replace sd_NV`y&#39;=stderr[4] in 2 rename estimate estimate_V`y&#39; rename stderr sd_V`y&#39; local y = `y&#39;+1 drop if time&gt;2 save, replace } Merge the quantity of fertilizer used with each variable of interest (again, using a loop) based on the time variable. use lfertmaizr, clear foreach x in limprovedseedsr lyieldr ldailyconsr lexp_yield_fertr{ merge 1:1 time using `x&#39;.dta, nogen save figure2.dta, replace } Clear existing matrices and results in Stata’s memory, and load the dataset named “figure2.dta”. clear matrix clear results use &quot;figure2.dta&quot;, clear Calculate lower and upper limits of the 95% confidence interval: Create a foreach loop to iterate over a list of variables(V1, V2, …, NV5). Use the mean estimate (estimate_) and standard deviation (sd_) to calculate the lower and upper bounds of the 95% confidence interval for each variable by subtracting 1.96 times the standard deviation from the values taken by each variable in the loop. Set the upper limit to 1 if it’s greater than 1, in other words, substitute 1 for any variable value exceeding 1. Create a matrix with mkmat to store the results (estimate, lower limit, and upper limit) for each variable in the loop. foreach i in V1 V2 V3 V4 V5 NV1 NV2 NV3 NV4 NV5{ gen ll95_`i&#39; = estimate_`i&#39; - 1.96*sd_`i&#39; gen ul95_`i&#39; = estimate_`i&#39; + 1.96*sd_`i&#39; replace ul95_`i&#39;=1 if ul95_`i&#39;&gt;1 mkmat estimate_`i&#39; ll95_`i&#39; ul95_`i&#39;, matrix(`i&#39;) } Package installation and graph style settings These preprocessing steps are crucial for ensuring accurate and visually compelling representations of the estimated coefficients with their associated confidence intervals. The resulting figure is a valuable tool for conveying the uncertainty surrounding the mean estimates, aiding in the interpretation and communication of statistical findings. Install and initialize additional packages (grstyle and coefplot) for graph styling and coefficient plotting. Set graph style settings, such as background color and major grid color. ssc install grstyle ssc install coefplot grstyle init grstyle color background white grstyle color major_grid white Graph creation and exportation The command coefplot creates a graphic displaying the coefficients of the regression and the confidence interval. - matrix selects the values to be used for the graph in the matrix. - ci selects the upper and lower bounds of the confidence interval - msize is used to select the size of the graph lines. - xline(0, lpattern(solid) lw(thick)) adds a vertical line to the x-axis at position 0, with a line style of “solid” and a line thickness of “thick”. - xlabel(-0.5(0.5)1) adds axis labels from -0.5 to 1, with a step of 0.5. - bylabel is used to define labels and byopts is used to define the chart title. - ciopts(lwidth(thick thick)) defines the line thickness for confidence interval. Reproduce 5 times this process for each variable, during and after the treatment. coefplot /* */ (matrix(V1[,1]), ci((V1[,2] V1[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV1[,1]), ci((NV1[,2] NV1[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1))|| /* */ ||, bylabel( ) byopts(title(&quot;Fertilizer on maize&quot;)) xsize(2) scale(1.35) /// ylabel(,labsize(vlarge)) ciopts(lwidth(thick thick)) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(fertilizer) coefplot /* */ (matrix(V2[,1]), ci((V2[,2] V2[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV2[,1]), ci((NV2[,2] NV2[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ ||, byopts( title(&quot;Improved maize seeds&quot;)) bylabel( ) xsize(2) scale(1.35) /// ylabel(,labsize(vlarge)) ciopts(lwidth(thick thick)) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(improved) coefplot /* */ (matrix(V3[,1]), ci((V3[,2] V3[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV3[,1]), ci((NV3[,2] NV3[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1))|| /* */ ||, byopts( title(&quot;Maize yield&quot;)) bylabel( ) xsize(2) scale(1.35) /// ylabel(,labsize(vlarge))ciopts(lwidth(thick thick) ) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(yield) coefplot /* */ (matrix(V4[,1]), ci((V4[,2] V4[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV4[,1]), ci((NV4[,2] NV4[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ ||, byopts( title(&quot;Consumption&quot;)) bylabel( ) xsize(2) scale(1.35) /// ylabel(,labsize(vlarge))ciopts(lwidth(thick thick) ) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(consumption) coefplot /* */ (matrix(V5[,1]), ci((V5[,2] V5[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV5[,1]), ci((NV5[,2] NV5[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ ||, byopts( title(&quot;Expected yield with technology package&quot;)) bylabel( ) xsize(2) /// scale(1.35) ylabel(,labsize(vlarge)) ciopts(lwidth(thick thick)) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(expected) Combine all the graphs in one graph and edit it: b1title(Estimated coefficients, size(small) color(black)) adds a title to the top of the combined chart, stating “Estimated coefficients.” The title size is “small” and the text color is “black.” cols(1) charts are combined into a single column. altshrink altering scaling of text iscale(*3.2) is used to select the graph scale. The graphs here are enlarged by 3.2 times. xcommon means that the x is common to all graphs. imargine define the interior margins. graph combine fertilizer improved yield consumption expected, /// b1title(Estimated coefficients, size(small) color(black)) cols(1) altshrink iscale(*3.2) /// xsize(3) xcommon imargin(b=1 t=1) title(&quot;Direct impact&quot; &quot;on treatment group&quot;, /// size(small) color(black) position(11)) subtitle(&quot;Spillover impact via social&quot; /// &quot;network contacts&quot;, size(small) color(black) position(1)) Finally, export the graph in .png and .pdf with the function gr export. gr export figure2.png, replace gr export figure2.pdf, replace 1.1.4 Replication of Table 7 - Separated Estimation of Spillover Effects for the First and Second Years After the Program In both Table 2 and Figure 2 presented above, researchers estimate a single “after” treatment effect, pooling the two years after the intervention period. In Table 7, the researchers distinguish between spillover effects one year after, and two years after the program, separately. The objective of this table is to push the analysis of spillovers further, and is similar to conduct a robustness check in the context of this study. To dissect spillover effects over time, researchers introduce two different “after” indicators for each of the post-subsidy years. As expected, due to reduced power, fewer coefficients reach statistical significance. There is no clear systematic pattern to the coefficients across the two years, and the hypothesis that the direct and spillover effects are consistent between the first and second “after” years cannot be rejected. Replication code for Table 7 Data preparation Start with initializing a local macro using local, named rep_app and set its value to replace. For more information, see section explaining Table 2 above. use &quot;Dataset_CLY.dta&quot;, clear local rep_app = &quot;replace&quot;&quot; Begin by correctly labeling the variables that will appear in Table 7. local rep_app = &quot;replace&quot; label var vouch_dur &quot;Direct impacts during&quot; label var vouch_aft_r3 &quot;Direct impacts 1 year after&quot; label var vouch_aft_r4 &quot;Direct impacts 2 year after&quot; label var sn2up_sub_dur &quot;Spillover impacts during&quot; label var sn2up_sub_aft_r3 &quot;Spillover impacts 1 year after&quot; label var sn2up_sub_aft_r3 &quot;Spillover impacts 2 year after&quot; Regressions, and table creation The code for creating Table 7 is very similar to that for creating Table 2 (see above). The interesting novelties are the following: Using a loop, run a regression with areg, adding the prefix xi- to indicate that the regression includes indicator values (equal to either 0 or 1), and including fixed effects which will be absorbed for the variable vlgid_round with the option absorb. Standard errors are clustered at the individual level (respid variable). The command qui, for “quietly”, means that the results will not be displayed. Then test the equality of coefficients between vouch_aft_r3 and vouch_aft_r4, and between sn2up_sub_aft_r3 and sn2up_sub_aft_r4 using the test command. This command performs Wald tests of simple and composite linear hypotheses about the parameters of the most recently fitted model. This allows us to test whether there is a significant difference between the coefficients for period 3 and period 4. Next, store the p-value results using scalar. The last step is to export the results you have just obtained into our results folder using outreg2. To create a nice table easily, refer to explanations from Table 2. foreach x in lfertmaizr limprovedseedsr lyieldr ldailyconsr lexp_yield_fertr { qui xi: areg `x&#39; vouch_dur vouch_aft_r3 vouch_aft_r4 sn2up_sub_dur sn2up_sub_aft_r3 /// sn2up_sub_aft_r4 i.nw_talkedagmoder*i.round, absorb(vlgid_round) cluster(respid) test vouch_aft_r3 = vouch_aft_r4 scalar vou_di = r(p) test sn2up_sub_aft_r3 = sn2up_sub_aft_r4 scalar sn_di = r(p) outreg2 using &quot;table A7.xls&quot;, `rep_app&#39; bracket label nocons noni less(1) nor2 /// keep(vouch_dur vouch_aft_r3 vouch_aft_r4 sn2up_sub_dur sn2up_sub_aft_r3 /// sn2up_sub_aft_r4) adds(&quot;vouch_aft dif (r3-r4) p-value&quot;, vou_di, /// &quot;sn2up_sub_aft dif (r3-r4) p-value&quot; , sn_di ) local rep_app = &quot;append&quot; } Authors: Elvire Jégu, Ambre Delaunay, Chiara Balducci, Yagmur Helin Aslan, students in the Master program in Development Economics and Sustainable Development (2023-2024), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2023 "],["instrumental-variables.html", "Chapter 2 Instrumental Variables ", " Chapter 2 Instrumental Variables "],["winners-and-losers-from-agrarian-reform-evidence-from-danish-land-inequality-16821895.html", "2.1 Winners and losers from Agrarian Reform: Evidence from Danish land inequality 1682–1895", " 2.1 Winners and losers from Agrarian Reform: Evidence from Danish land inequality 1682–1895 Boberg-Fazlić, N., Lampe, M., Lasheras, P. M., &amp; Sharp, P. (2022). Winners and losers from Agrarian Reform: Evidence from Danish land inequality 1682–1895. Journal of Development Economics, 155, 102813. https://doi.org/10.1016/j.jdeveco.2021.102813 Download Do-File corresponding to the explanations below Download Codebook Download Original Data (OSF repository: click on WinnersandLosers_finaldata.dta) [Link to the full original replication package paper on OSF] Highlights The paper examines the distributional effects of land reforms between 1682-1895 in Denmark. The authors use an Instrumental Variable (IV) to deal with endogeneity issues stemming from reverse causality between the variable of interest (land productivity) and the dependent variable (land inequalities). This methodology is standard by the choice of a geological instrument. It follows the usual steps of the IV procedure, including tests of the instrument’s pertinence (F-statistic of the 1st stage estimation). Our added value to the original replication package lies mainly in the detailed explanation of the provided code. Additionally, we show how to extract various tables and figures directly from the Stata code. We explain these Stata tricks: Diagnostic Statistics: We leverage the estat firststage command to obtain diagnostic statistics pertaining to the first-stage of instrumental variable estimation, which includes extracting and storing the F-statistics for further analysis. Looping: The foreach loop structure is used, which enables the iteration over multiple years for instrumental variable regressions and allows to avoid repetitive commands in the code. Advanced Formatting: Through the utilization of the esttab command along with various formatting options, we create well-formatted tables, which results in the production of tables that are not only clear and organized but also visually appealing. 2.1.1 Introduction As a rule, agrarian reforms are viewed as fundamental for economic development, allowing, on one hand, to fuel agricultural productivity and on the other, to reallocate the necessary productive resources to industrialization. Their design has to fulfill two competing objectives: stimulating farms’ productivity and ensuring an equitable access to land. As numerous reforms have been criticized for failing the latter, this paper provides the first quantitative long-term assessment of the Danish agrarian reforms’ effects on both economic efficiency and land inequalities. To do so, the authors combine several sources of historical data on farms at the parish level with population and agricultural censuses to cover the 1682-1895 period. First, they document that the agrarian reform happened between 1784 and 1810, with a unification of property rights, favoring middle-sized farms. Then, they get the number and sizes of farms per parish for the years 1682, 1834, 1850, 1860, 1873, 1885 and 1895 from Danish land registers, as well as their productive capacity measured in units of barrel of hard grain (HK), which was the unified measure used for tax collection. Thanks to this data, they can calculate a measure for inequality within parishes for the several data points (their explained variable). They opt for the use of the Theil index to measure land inequality due to its analytically desirable properties. One key advantage is that the Theil index adheres to the principle of transfers, ensuring that a redistribution from one individual to a less affluent one results in a proportional decline in the Theil index, which is particularly convenient for focusing on changes in inequalities over time. Moreover, the Theil index provides unambiguous rankings of distributions, ensuring that two regions with identical Theil indices exhibit identical income distributions (not guaranteed by other common measures such as the Gini index). If a parish have all farms of equal productive size, then its Theil index is 0 ; if only one farm holds all land, its Theil index is the logarithm of the number of households living in the parish. Finally, they consider the natural logarithm of the total value of the land (measured in HK) of parish p as their main explanatory variable. Recall that their objective is to estimate the effect of productive capacity on inequality. From an econometric point of view, the most obvious approach would be to regress changes in parish-level land inequalities on a measure of soil productivity. Nonetheless, such a specification would be exposed to endogeneity issues, due to the reverse causality between the explained variable (evolution in land inequalities) and the variable of interest (land productivity): as higher agricultural productivity leads to Malthusian dynamics, in parishes with more fertile soils, there would be more smallholders and landless individuals, whose socio-economic status would then be deteriorated by the reforms. Consequently, areas with a higher agricultural productivity were more exposed to rises in land inequalities. At the same time, population growth has beneficial effects on innovation and thus, agricultural productivity. To tackle this, the authors adopt an IV strategy and choose as an instrument for land productivity the distribution of “Boulder Clay”, the sediment type most adapted to barley, resulting from the Weichselian glaciation. Geological variables are generally viewed as reliable instruments, since they capture long-term determinants of development that are independent from human factors. So their IV is the share of parish area classified as boulder clay (invariant in time). 2.1.2 Identification strategy When deciding to use a 2SLS strategy, several points need to be discussed in order to allow the identification of robust causal effects. The first hypothesis to be considered and which cannot be tested statistically is the exclusion restriction. It is necessary to rule out any direct effect of the instrument (boulder-clay) on the dependent variable (land inequalities). In this specific case, it can be assumed that soil composition doesn’t directly affect the level of land inequalities. In fact, the authors argue that the rise of inequalities is driven by a stronger demographic growth, due to the productive capacity of the land. This implies, beside the soil fertility, adequate land management practices and efficient agricultural technologies. The authors also have to exclude any effect of the dependent variable on the instrument. Here, once again, land inequalities and the soil fertility seem to be unrelated, as the soil’s share of boulder clay stems from the Weichselian glaciation, which occurred approximately 18,000 years ago. Moreover, the sediment classification they use was made below the impact area of cultivation practices and technologies, which allows to infirm a potential effect of inequalities on this measure of land fertility. The second hypothesis to be respected is the instrument’s relevance. The authors need to convincingly describe how the instrument affects the endogenous variable. In our case, the soil composition represents a key determinant of the land’s productivity and thus is supposed to be positively correlated with the total production of an area measured by the variable TotalHK. Unlike the aforementioned exclusion restriction, this assumption can be tested statistically. It can be done, for instance, by verifying whether after estimating the first-stage specification (1), the coefficient of the instrumental variable is statistically significant or whether the F-statistic is superior to the conventionally fixed value of 10. \\[\\begin{equation} \\tag{1} ln(TotalHK)_{p}=α_{0}+βBoulderClay_{p}+λ_{r}+X_{p}γ+ϵ_{p} \\end{equation}\\] As we can see in column 2 of Table 1, the effect of the soil composition on total production is statistically significant at 1% level. Also, the value of the F-statistic is equal to 280. Hence, we can conclude that both steps necessary to ensure the relevance and the exogeneity of the instrument have been fulfilled. That said, they further estimate the second-stage specification (2). \\[\\begin{equation} \\tag{2} ΔTheil_{p}=α_{0}+β\\widehat{ln(TotalHK)}_{p}+λ_{r}+X_{p}γ+ϵ_{p} \\end{equation}\\] After estimating both the OLS and the second-stage IV specifications, the authors find statistically significant positive effects of land productivity on land inequalities during the agrarian reforms. To ascertain the robustness of these findings, they estimate additional specifications, using alternative measures of land inequalities – such as Gini index, an alternative to the Theil index – and of land productivity – the amount of barley paid in tax. As no significant change in the results was detected, we can conclude that the econometric estimation allowed them to confirm their predictions, exposed in the theoretical part of the paper. In the next part of our narrative, we will discuss some of the main figures of the paper. We will also indicate the necessary commands to replicate them using Stata. 2.1.3 From the article to practice: exploring the replication code 2.1.3.1 Getting started: database access and required packages In order to open the Stata database and execute the following lines of code, several packages need to be downloaded and installed. In this section, we guide you through the process of accessing the database and briefly refer to these packages. In what follows, you have to download the original dataset from a link, then run these first lines of code to get the shorter version of the dataset, for which we also provide you with a codebook (see header). ***Open the database*** *download and save WinnersandLosers_finaldata.dta from: https://osf.io/jmn5y/files/osfstorage?view_only=f18f6d51efe44f04abef6e9042c0163c *define your working directeory, where you also just stored the dataset cd &quot;C:\\Users\\&quot; /*C:\\Users\\ = path where you also store the dataset*/ use &quot;WinnersandLosers_finaldata.dta&quot;, clear keep ID BygLG Lat Long year MLmean Theil_c AggTheil_c gini1682 gini1834 region ln_area LnDistCoast LnDistCPH ln_TotalFarmHK ln_Theil_1682c ln_Theil_1834c ln_Theil_1850c ln_Theil_1860c ln_Theil_1873c ln_Theil_1885c ln_Theil_1895c year_1 Gini ln_TotalFarmHK1682 ln_TotalFarmHK1834 ln_TotalFarmHK1850 ln_TotalFarmHK1860 ln_TotalFarmHK1873 ln_TotalFarmHK1885 ln_TotalFarmHK1895 ln_TotalFarmHK1682_nohouses ln_TotalFarmHK1834_nohouses ln_TotalFarmHK1850_nohouses ln_TotalFarmHK1860_nohouses ln_TotalFarmHK1873_nohouses ln_TotalFarmHK1885_nohouses ln_TotalFarmHK1895_nohouses save &quot;Dataset_AA_ALC_AT.dta&quot;, replace Subsequently, several Stata packages are necessary to execute the replication code successfully. The first package needed is the estout package. This package allows to make regression tables using regressions previously stored in the Stata memory. The second package required is the ivreg2 package. This package allows to run instrumental variables regressions. The third package is the coefplot package. This package is used to create coefficients plots which visually represent the estimated coefficients and their confidence intervals. The fourth package is outreg2. This package is used to produce illustrative tables of regression outputs. This package is able to write LaTex-format tables. All the packages can be installed using the following lines of code. ***Install the required packages*** ssc install estout ssc install ivreg2 ssc install coefplot ssc install outreg2 2.1.3.2 Understanding the replication process: code analysis of Table 1 Table 1: IV estimation using total HK and the share of parish area classified as boulder clay ***Table 1*** use &quot;Dataset_AA_ALC_AT.dta&quot;, clear ***Column (1) : OLS*** eststo ols1: qui reg D.Theil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast /// i.region if year==1834, vce(robust) eststo ols2: qui reg D.AggTheil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast /// i.region i.year if year==1834, vce(robust) ***Column (2) : First-stage*** eststo fiv1: qui reg ln_TotalFarmHK MLmean ln_area LnDistCPH Lat Long LnDistCoast /// i.region if year==1834, vce(robust) ***Column (4) : Reduced form*** eststo ziv1: qui reg D.Theil_c MLmean ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834, vce(robust) ***Column (3) : Second-stage (with diffTheil)*** eststo ivdiff1: qui ivregress 2sls D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (5) : Second-stage (with diffAggTheil)*** eststo ivdiff2: qui ivregress 2sls D.AggTheil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (6) : Second-stage (with Gini)*** eststo ginihkdiff: qui ivregress 2sls D.Gini ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Formation of Table 1*** esttab ols1 fiv1 ivdiff1 ziv1 ivdiff2 ginihkdiff, se star(* 0.10 ** 0.05 *** 0.01) b(3) /// r2 var(15) model(12) wrap keep(MLmean ln_TotalFarmHK) mtitles(&quot;OLS&quot; &quot;first stage&quot; /// &quot;second stage (diffTheil)&quot; &quot;reduced form (diff Theil)&quot; &quot;second stage (diffAggTheil)&quot; /// &quot;second stage (diffGini)&quot; ) stats(N r2 Fstat, labels(&quot;Observations&quot; &quot;R-squared&quot; /// &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) indicate(&quot;Region FE = 2.region&quot; /// &quot;Geography = LnDistCPH&quot; , labels(Y N)) label OLS regressions by replicating the columns 1,2 and 4 of Table 1 ***Table 1*** eststo clear ***Column (1) : OLS*** eststo ols1: qui reg D.Theil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast /// i.region if year==1834, vce(robust) eststo ols2: qui reg D.AggTheil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast /// i.region i.year if year==1834, vce(robust) ***Column (2) : First-stage*** eststo fiv1: qui reg ln_TotalFarmHK MLmean ln_area LnDistCPH Lat Long LnDistCoast /// i.region if year==1834, vce(robust) ***Column (4) : Reduced form*** eststo ziv1: qui reg D.Theil_c MLmean ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834, vce(robust) Columns 1, 2, and 4 of Table 1 represent two Ordinary Least Squares (OLS) regressions, a first-stage, and a reduced form, respectively, representing distinct stages through OLS regressions. These different model specifications are often used in econometric analyses to examine causal relationships between variables. These columns offer a comprehensive understanding of underlying economic relationships and allow for nuanced interpretation of results. The singular variable differing across the two OLS regressions is the initial variable, corresponding to the dependent variable: the change in the Theil index from 1682 to 1834 D.Theil_c and the change in the Theil index of 1834 aggregated to the size categories of 1682 D.AggTheil_c, respectively. In line with our approach of conducting both the first-stage and the reduced form analyses, we introduce the instrumental variable MLmean in the two last regression equations. The replication process of these columns involves several crucial steps in estimating econometric models. Firstly, the use of the eststo clear command ensures a reset of previous estimation results, creating a clean environment before running new estimations. Subsequently, the inclusion of qui – for quietly – with the reg – for regress – function allows for the temporary storage of regression results in a named matrix, such as ols1 in the first model. Moreover, the model specification, denoted as D.Theil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast i.region, details the variables included in the regression equation. It is crucial to note that this specification is essential for understanding the relationships between key variables and the dependent variable – here D.Theil_c or D.AggTheil_c. Furthermore, the use of the conditional if year==1834 filters the data, retaining only observations where the year variable is equal to 1834. This filtering step enables a focus on a particular year – here 1834 –, allowing for a more targeted analysis. Finally, the vce(robust) option is specified to estimate robust standard errors, thereby addressing potential heteroskedasticity issues. These robust standard errors are particularly important to ensure the reliability of estimation results, especially when residuals exhibit heterogeneous variations. IV regressions by replicating columns 3,5 and 6 of Table 1 ***Table 1*** ***Column (3) : Second-stage (with diffTheil)*** eststo ivdiff1: qui ivregress 2sls D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (5) : Second-stage (with diffAggTheil)*** eststo ivdiff2: qui ivregress 2sls D.AggTheil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (6) : Second-stage (with Gini)*** eststo ginihkdiff: qui ivregress 2sls D.Gini ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] The columns 3, 5, and 6 which represent second-stages, are estimated through instrumental variable regressions. Similar to OLS regressions, the command eststo ivdiff1 : qui is used to store the estimation results in a matrix named ivdiff1, ensuring an organized storage of relevant information. Secondly, the function ivregress 2sls is applied to conduct a two-stage IV regression, as indicated by 2sls. Additionally, the explanatory variables D.Theilc lnarea LnDistCPH Lat Long LnDistCoast i.region mirror those used in the OLS regressions, providing continuity in the model specification. The only variable differing across each column specification is the initial variable, corresponding to the dependent variable : the change in the Theil index from 1682 to 1834 D.Theilc, the change in the Theil index of 1834 aggregated to the size categories of 1682 D.AggTheilc, and the change in the Gini coefficient instead of the Theil index D.Gini, respectively. This nuanced change reflects the distinct focus on different dependent variables in each instance, while the remaining explanatory variables remain constant, allowing for a systematic exploration of the impact of the selected variables on the varied dependent variables. The inclusion of (lnTotalFarmHK = MLmean) specifies the endogenous variable lnTotalFarmHK and its instrument MLmean. This crucial specification distinguishes the endogenous variable and its corresponding instrument, a fundamental aspect of instrumental variable estimation. Furthermore, to address heteroskedasticity and autocorrelation, the vce(robust) command employs a robust variance-covariance matrix, ensuring the reliability of standard errors in the estimation. The upcoming lines of code that we are about to describe initially seemed somewhat unclear to us. However, despite the initial ambiguity, they prove to be valuable and, upon closer examination, are now better understood. This is why we will take the time to provide a clear explanation of these lines. First, the line estat firststage is employed to display statistics from the first-stage of the IV regression. This step is typically utilized to assess the validity of instruments and ascertain their suitability for the estimation process. Secondly, storing the first-stage results in a matrix named ‘fstat’ is accomplished through the line mat fstat = r(singleresults). This matrix captures relevant statistics from the first-stage, providing a comprehensive view of the instrumental variable performance. Finally, the line estadd scalar Fstat = fstat[1,4] introduces a new scalar variable, Fstat, into the main regression results. This step allows to extract the F-statistic from the first-stage matrix fstat and assigns it to Fstat. The F-statistic is a crucial metric in instrumental variable regression, serving as a diagnostic tool to assess the overall validity of the instruments used. A high F-statistic, that is superior to 10, suggests that the instruments collectively have a strong explanatory power for the endogenous variable. In summary, these lines of code are essential for evaluating the quality and relevance of the instrumental variables employed in the two-stage IV regression. Formatting Table 1, an additional but optional step ***Table 1*** ***Formation of Table 1*** esttab ols1 fiv1 ivdiff1 ziv1 ivdiff2 ginihkdiff, se star(* 0.10 ** 0.05 *** 0.01) b(3) /// r2 var(15) model(12) wrap keep(MLmean ln_TotalFarmHK) mtitles(&quot;OLS&quot; &quot;first stage&quot; /// &quot;second stage (diffTheil)&quot; &quot;reduced form (diff Theil)&quot; &quot;second stage (diffAggTheil)&quot; /// &quot;second stage (diffGini)&quot; ) stats(N r2 Fstat, labels(&quot;Observations&quot; &quot;R-squared&quot; /// &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) indicate(&quot;Region FE = 2.region&quot; /// &quot;Geography = LnDistCPH&quot; , labels(Y N)) label The next stage in our analysis involves the creation of a comprehensive table summarizing the results of the previously conducted regressions. It is essential to note that this step does not stand alone, rather it complements the preceding two steps in which variables were defined and regressions were estimated. The resulting table serves as a visual representation of the relationships captured in the regression models, enhancing the interpretability and communicative power of the findings. First, the command esttab ols1 fiv1 ivdiff1 ziv1 ivdiff2 ginihkdiff specifies which models to include in the table, incorporating the Ordinary Least Squares (OLS), first-stage instrumental variable (IV), and second-stage IV regression results. To enhance the clarity of the table, additional formatting options are applied. The se star(* 0.10 ** 0.05 *** 0.01) command displays standard errors with significance stars, denoting significance levels with asterisks. Thirdly, the b(3) command allows to limit coefficient estimates to three decimal places, contributing to a cleaner presentation of results. Moreover, the inclusion of r2 displays the coefficient of determination, the R-squared, in the table, providing insights about the explanatory power of the model. The var(15) option limits the display of R-squared to 15 decimal places. Additionally, with model(12), we specify the maximum number of models to display in the table, accommodating up to 12 different model specifications. The wrap command facilitates the wrapping of long variable names onto multiple lines, ensuring readability. The keep(MLmean lnTotalFarmHK) option selectively includes only the variables MLmean and lnTotalFarmHK in the table, which are our instrument and endogenous variables. Furthermore, mtitles() assigns model titles to each specified model, contributing to a more informative and organized presentation. The stats(N r2 Fstat, labels(\"Observations\" \"R-squared\" \"KP F-statistic\") fmt(%9.0fc 2 2)) option adds relevant statistics to the table, including the number of observations (N), the R-squared, and the F-statistic. The indicate (\"Region FE = 2.region\" \"Geography = LnDistCPH\", labels(Y N)) command introduces indicator in a regression model. The first part, \"Region FE = 2.region\", includes fixed effects for a specific region, here denoted as 2, representing Jutland. This accounts for unobserved variation specific to Jutland. The second part, \"Geography = LnDistCPH\", introduces an indicator variable related to geography. The labels Y and N are assigned to indicate the presence or absence of fixed effects or controls in each observation, contributing to the model’s interpretability. This step allows for a nuanced interpretation of how fixed effects and geographic controls influence the results. Finally, the label option is appended to the esttab command, indicating that variable labels should be included in the table for clarity and precision. In the replication code provided by the authors, no specific instruction regarding the exportation of tables has been included. Therefore, we recommend users to add the command using \"NameOfYourTable.tex\" right after the last variable mentioned in esttab, just before the comma in front of se star, if they intend to export the table in LaTeX format. Alternatively, users can use the command using NameOfYourTable.txt\", at the same location in the code, if they prefer the table in a text format. This flexibility allows users to choose the desired output format for the tables based on their specific needs. This last step – which involves the formation of the table – can be used in the context of other studies to generate a clear and visually appealing table summarizing regression results. However, it is crucial to emphasize that this step is not indispensable for obtaining replication results. The results can be independently observed directly using the same code without the qui option, starting from reg or ivreg. This underscores the flexibility of the code, allowing researchers to directly access and analyze the regression outcomes without relying on the table generation step. While the table contributes to a more organized presentation, it is not a prerequisite for the replication process. Users can tailor their analysis based on their preferences and requirements before proceeding to this step. The study relies on an instrumental variable method that can be challenging to comprehend. Explaining the replication code of Table 1 is deemed essential for us, as it allows a detailed exploration and validation of the authors’ instrumental variable approach. This importance is further underscored by the fact that Table 1 showcases the primary results of the study. Having elucidated the intricacies of the Table 1 replication code, we will now transition to describing the replication code for Figure 5. 2.1.3.3 Understanding the replication process: code analysis of Figure 5 Figure 5 ***Figure 5*** eststo clear foreach x in 1682 1834 1850 1860 1873 1885 1895 { qui ivregress 2sls ln_Theil_`x&#39;c ln_area LnDistCPH Lat Long LnDistCoast i.region /// (ln_TotalFarmHK`x&#39; = MLmean) if year==`x&#39;, vce(clust ID) estimates store coef`x&#39; } coefplot coef*, vert yline(0) keep(ln_TotalFarmHK*) graphregion(color(white)) /// ciopts(recast(rcap) lcol(black)) mcolor(black) xtick(1(1)7) /// xlabel(1 &quot;1682&quot; 2 &quot;1834&quot; 3 &quot;1850&quot; 4 &quot;1860&quot; 5 &quot;1873&quot; 6 &quot;1885&quot; 7 &quot;1895&quot;, ) /// grid(b) legend(off) graph export &quot;outputfile.png&quot;, replace The provided Stata code segment serves to illustrate the coefficients for second-stage estimations conducted over the years 1682 to 1895. The dependent variable under consideration is ln(Theil), and the instrumental variable utilized is the share of boulder clay MLmean. The use of the natural logarithm for the dependent variable ln(Theil) in the second-stage regression is intentional. This choice allows for the presentation of estimates for the second-stage coefficients in levels, offering insight into the relationships over the years 1682 to 1895. The focus on ln(Theil) in different years underscores a preference for examining the absolute values of Theil index rather than changes in Theil index, providing a comprehensive perspective on the dynamics of the variable across the specified temporal range. Initially, eststo clear ensures a clean slate by clearing any previously stored estimation results. Subsequently, the foreach loop in the provided Stata code serves as an iterative mechanism, allowing the execution of specified commands for each value in the specified range or list. In this case, the loop iterates over the years 1682, 1834, 1850, 1860, 1873, 1885, and 1895. For each iteration, the code within the loop conducts a 2-stage least squares regression using the ivregress 2sls command, estimating a model for the given year. The model includes various independent variables such as lnTheil’x’c, lnarea, LnDistCPH, Lat, Long, LnDistCoast as well as the endogenous variable lnTotalFarmHK instrumented by MLmean and there are fixed effects for region. The purpose of the loop in this context is to efficiently run the same regression model for multiple years, automating the process and avoiding redundant code. This is particularly useful when dealing with time-series data or when conducting analyses for various time points. The resulting coefficient plot provides a concise and visual representation of the dynamics of the variable of interest across different years. The line estimates store coef’x’ facilitates the storage of estimation results in matrices named coef’x’, where x represents the specific year. Following the loop, the coefplot coef* command generates a coefficient plot based on the stored estimation results, specifically focusing on coefficients related to the variable lnTotalFarmHK across the years. In terms of visual representation, the plot includes a vertical line at 0 for reference with the vert yline(0) command, retains only coefficients related to lnTotalFarmHK with the keep(lnTotalFarmHK*) command, and employs a white background for enhanced clarity thanks to the graphregion(color(white)). Confidence intervals are displayed using a horizontal line in black with ciopts(recast(rcap) lcol(black)), and the markers – dots – representing coefficient estimates are colored black for visibility with the mcolor(black) option. Additionally, xtick(1(1)7) and xlabel(1 \"1682\" 2 \"1834\" 3 \"1850\" 4 \"1860\" 5 \"1873\" 6 \"1885\" 7 \"1895\"), allow stick marks and corresponding labels on the x-axis to be strategically positioned to represent each year from 1682 to 1895. Gridlines are incorporated for ease of interpretation grid(b), and the legend is turned off for a clean and uncluttered visual representation thanks to the legend(off) command. The final line of code, graph export \"outputfile.png\", replace, is added by us to the replication code for the purpose of exporting the coefficient plot as a PNG file. This command utilizes Stata’s graph export feature, allowing us to save the generated graph to an external file named “outputfile.png” in the current working directory. The option \"replace\" ensures that if a file with the same name already exists, it will be overwritten. This line of code enhances the replicability of the study by facilitating the export of the coefficient plot in a widely used PNG format for further analysis or inclusion in reports and presentations. This meticulous approach allows for a comprehensive exploration of coefficient dynamics over time, offering insights into the relationship between the dependent variable, ln(Theil), and the instrumental variable MLmean – share of boulder clay – with controls included, across the specified years. With the explanation of the coefficient plots for second-stage estimations complete, our attention now shifts to describing the replication code for Table A.3 found in the Appendix. 2.1.3.4 Understanding the replication process: code analysis of Table A.3 Table 3: Robustness check: Second-stage IV estimates using Gini coefficient ***Table A.3*** ***Creation of a loop*** eststo clear foreach x in 1682 1834 { eststo ginihk`x&#39;: ivregress 2sls gini`x&#39; ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==`x&#39; &amp; gini1834!=., vce(clust ID) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] } ***Formation of Table A.3*** esttab ginihk1682 ginihk1834, se star(* 0.10 ** 0.05 *** 0.01) b(3) r2 var(15) model(11) /// wrap keep(ln_TotalFarmHK) mtitles(&quot;2nd stage: Gini diff&quot; &quot;2nd stage: gini 1682&quot; /// &quot;2nd stage: gini 1834&quot;) stats(N r2 Fstat, labels(&quot;Observations&quot; &quot;R-squared&quot; /// &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) indicate(&quot;Region FE = 2.region&quot; /// &quot;Geography = LnDistCPH&quot; , labels(Y N)) label Table A.3 in the Appendix presents robustness check results, specifically second-stage instrumental variable estimates using the Gini coefficient. This additional analysis aims to verify the robustness of the findings by employing an alternative measure. The choice of the Gini coefficient not only enhances interpretability but also provides a basis for comparing and validating the study’s results against a broader scholarly context. Creation of a loop ***Table A.3*** ***Creation of a loop*** eststo clear foreach x in 1682 1834 { eststo ginihk`x&#39;: ivregress 2sls gini`x&#39; ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==`x&#39; &amp; gini1834!=., vce(clust ID) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] } Firstly, the use of the eststo clear command ensures a reset of previous estimation results, creating a clean environment before running new estimations. The loop designated by foreach x in 1682 1834 iterates over two specific years, namely 1682 and 1834. This looping mechanism, as explained in the preceding section – Section 3.3 –, provides a concise and efficient way to conduct repetitive tasks for multiple years. The eststo ginihk‘x’ command within the loop facilitates the storage of results in matrices named ginihk‘x’, with x representing the current year in each iteration. For a more comprehensive understanding of the loop creation and its purpose, referring to the preceding Section 3.3 is recommended. Moreover, the ivregress 2sls function is employed to conduct a two-stage instrumental variable (IV) regression. The model’s explanatory variables include D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast i.region. In specifying (ln_TotalFarmHK = MLmean), the endogenous variable – the natural logarithm of the total value of the land measured in barrel of hard grain of parish – is denoted as ln_TotalFarmHK and its instrument is identified as MLmean. To ensure that the analysis includes only observations for the specified year x where the variable gini1834 is not missing, the condition if year==x &amp; gini1834!=. is incorporated. Furthermore, the command vce(clust ID) is used to adjust standard errors in the regression model, accounting for within-cluster correlation. The code that we will now describe mirrors the structure found in Table 1. Following this, estat firststage is employed to display statistics from the first-stage of the IV regression. This step is crucial for assessing the validity of instruments. The subsequent lines involve the storage of first-stage results in a matrix named fstat using the mat fstat = r(singleresults) command. This matrix captures relevant statistics from the initial stage, providing insights into the instrumental variable performance. Finally, estadd scalar Fstat = fstat[1,4] introduces a new scalar variable Fstat into the main regression results. This step extracts the F-statistic from the first-stage matrix fstat and assigns it to Fstat. For a more comprehensive understanding of the F-statistics and its purpose, referring to the preceding Section 3.3.2 is recommended. Formatting table A.3, an additional but optional step ***Table A.3*** ***Formation of Table A.3*** esttab ginihk1682 ginihk1834, se star(* 0.10 ** 0.05 *** 0.01) b(3) r2 var(15) model(11) /// wrap keep(ln_TotalFarmHK) mtitles(&quot;2nd stage: Gini diff&quot; &quot;2nd stage: gini 1682&quot; /// &quot;2nd stage: gini 1834&quot;) stats(N r2 Fstat, labels(&quot;Observations&quot; &quot;R-squared&quot; /// &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) indicate(&quot;Region FE = 2.region&quot; /// &quot;Geography = LnDistCPH&quot; , labels(Y N)) label Moving forward in our analysis, we proceed to construct a comprehensive table that consolidates the outcomes of the earlier regression analyses. Importantly, it’s crucial to emphasize that this stage is not conducted in isolation, instead it builds upon the groundwork laid in the preceding step where variables were defined and regressions within the loop were executed. The resultant table serves as a visual representation, effectively summarizing the relationships captured in the regression models. This approach enhances the interpretability and communicative power of the analytical findings. The code employed in the creation of the comprehensive table A.3 aligns with the methodology elucidated in Section 3.2.3, specifically used for Table 1. Indeed, the Stata code provided encompasses the construction of a table using the esttab command, incorporating results from the two-stage instrumental variable regressions conducted for the years 1682 and 1834. Thus, the esttab ginihk1682 ginihk1834 line specifies the models whose results will be included in the table, representing these two specific years. To enhance the clarity and readability of the table, several formatting commands are employed. The se star(* 0.10 ** 0.05 *** 0.01) line introduces significance stars denoted by asterisks, indicating the levels of statistical significance. Additionally, b(3) limits the coefficient estimates to three decimal places. The inclusion of the coefficient of determination R-squared is facilitated by r2 command, with var(15) limiting the number of decimals for R-squared to 15 digits. The model(11) command specifies the maximum number of models displayed in the table, accommodating 11 different models. Moreover, the wrap command assists in managing long variable names, allowing them to span multiple lines for improved readability. The keep(lnTotalFarmHK) line specifies the variable lnTotalFarmHK to be included in the table. Furthermore, model titles are assigned using mtitles(), and additional statistics, such as the number of observations (N), R-squared, and the F-statistic, are incorporated with stats(N r2 Fstat, labels(\"Observations\" \"R-squared\" \"KP F-statistic\")fmt(%9.0fc 2 2)). The indicate(\"Region FE = 2.region\" \"Geography = LnDistCPH\", labels(Y N)) section introduces indicator variables in a regression model. The first part, \"Region FE = 2.region\", includes fixed effects for a specific region, here denoted as 2, representing Jutland. This accounts for unobserved variation specific to Jutland. The second part, \"Geography = LnDistCPH\" introduces an indicator variable related to geography. The labels Y and N are assigned to indicate the presence or absence of fixed effects or controls in each observation, contributing to the model’s interpretability. Finally, the label option appended to the end of the esttab command ensures that variable labels are included in the table, enhancing the interpretability of the presented results. Once again, in the provided replication code by the authors, no specific instruction regarding the exportation of tables has been included. Therefore, we recommend users to add the command using \"NameOfYourTable.tex\" right after the last variable mentioned in esttab, just before the comma in front of se star, if they intend to export the table in LaTeX format. Alternatively, users can use the command using NameOfYourTable.txt\", at the same location in the code, if they prefer the table in a text format. This flexibility allows users to choose the desired output format for the tables based on their specific needs. This last step to form a table can be employed in the context of other studies to generate a clear and visually appealing table summarizing regression results. However, it is crucial to emphasize that this step is not indispensable for obtaining replication results. 2.1.3.5 Understanding the replication process: code analysis of Table A.4 Table 4: IV estimation using barley payments and the share of parish area classified as boulder clay ***Table A.4*** eststo clear ***Column (1) : OLS*** eststo ols1: qui reg D.Theil_c BygLG ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834, vce(robust) eststo ols2: qui reg D.AggTheil_c BygLG ln_area LnDistCPH Lat Long LnDistCoast i.region /// i.year if year==1834, vce(robust) ***Column (2) : First-stage*** eststo fiv1: qui reg BygLG MLmean ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834 &amp; ln_TotalFarmHK!=., vce(robust) ***Column (3) : Second-stage (diffTheil)*** eststo ivdiff1: qui ivregress 2sls D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (BygLG = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (4) : Second-stage (diffAggTheil)*** eststo ivdiff2: qui ivregress 2sls D.AggTheil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (BygLG = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Formation of Table A.4*** esttab ols1 fiv1 ivdiff1 ivdiff2, se star(* 0.10 ** 0.05 *** 0.01) b(3) r2 var(15) /// model(11) wrap keep(MLmean BygLG) mtitles(&quot;OLS&quot; &quot;first stage&quot; /// &quot;second stage (diffTheil)&quot; &quot;second stage (diffAggTheil)&quot; ) stats(N r2 Fstat, /// labels(&quot;Observations&quot; &quot;R-squared&quot; &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) /// indicate(&quot;Region FE = 2.region&quot; &quot;Geography = LnDistCPH&quot; , labels(Y N)) label Transitioning to another robustness test, presented in Table A.4 in the Appendix, we explore an alternative measure of land quality. In this test, rather than considering the total value of the parish’s HK, the authors focus solely on the amount of barley paid in tax as an indicator of land quality. This measure is derived from the digitization of a map presented by Frandsen in 1988. The results of this robustness check closely resemble the main estimations of table 1, underscoring the consistency and reliability of the analytical findings. OLS regressions by replicating the columns 1 and 2 of Table A.4 ***Table A.4*** eststo clear ***Column (1) : OLS*** eststo ols1: qui reg D.Theil_c BygLG ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834, vce(robust) eststo ols2: qui reg D.AggTheil_c BygLG ln_area LnDistCPH Lat Long LnDistCoast i.region /// i.year if year==1834, vce(robust) ***Column (2) : First-stage*** eststo fiv1: qui reg BygLG MLmean ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834 &amp; ln_TotalFarmHK!=., vce(robust) In this analysis, we employ familiar Stata commands, akin to those utilized in Table 1, with the only variable differing across each column specification is the initial variable, corresponding to the dependent variable: the change in the Theil index from 1682 to 1834 D.Theilc, the change in the Theil index of 1834 aggregated to the size categories of 1682 D.AggTheilc, and the amount of barley paid in tax as an indicator of land quality BygLG, respectively. In line with our approach of conducting the first-stage analysis, we introduce the instrumental variable MLmean to the last regression equation. First, the command eststo ols1 is used to store the estimation results, and the results matrix is designated as ols1. The qui function, signifying quietly, enables the temporary storage of results in the matrix ols1 for the initial regression. Importantly, this function ensures that the results of the regression are not displayed at this point but are preserved under the name ols1 for subsequent utilization. This approach proves particularly beneficial when authors later aim to present a comprehensive table with multiple specifications. Moreover, the reg function is utilized for Ordinary Least Squares (OLS) estimation, running a regression with specified variables. In this instance, the regression equation includes Theilc BygLG lnarea LnDistCPH Lat Long LnDistCoast i.region. A conditional statement, if year==1834 &amp; lnTotalFarmHK!=., restricts the analysis to observations where the year variable is equal to 1834, and lnTotalFarmHK is not missing. This condition ensures a focused examination of data relevant to the specified year and variable condition. Furthermore, the use of the conditional if year==1834 filters the data, retaining only observations where the year variable is equal to 1834. Lastly, the vce(robust) command is incorporated to specify the use of robust standard errors. This adjustment is made to account for potential heteroskedasticity and correlation of errors, enhancing the reliability of the estimation results. IV regressions by replicating the columns 3 and 4 of the Table A.4 ***Table A.4*** ***Column (3) : Second-stage (diffTheil)*** eststo ivdiff1: qui ivregress 2sls D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (BygLG = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (4) : Second-stage (diffAggTheil)*** eststo ivdiff2: qui ivregress 2sls D.AggTheil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (BygLG = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] In this specific analysis, we focus on the second-stage of the estimations with two dependent variables: D.Theilc, representing the change in the Theil index from 1682 to 1834, D.AggTheilc, representing the change in the Theil index of 1834 aggregated to the size categories of 1682. The endogenous variable, the amount of barley paid in tax as an indicator of land quality BygLG, is instrumented by the share of boulder clay, utilizing MLmean as the instrumental variable. This nuanced approach allows us to enhance the robustness of the estimates and address potential biases in the endogenous variable. In executing the second-stage of our analysis, employing the eststo ivdiff1: qui command allows to store the estimation results in a matrix named ivdiff1. This matrix captures the outcomes of the two-stage instrumental variable regression facilitated by the ivregress 2sls command. The regression equation includes explanatory variables D.Theilc lnarea LnDistCPH Lat Long LnDistCoast i.region, mirroring the structure of OLS regressions. Moreover, the specification (BygLG = MLmean) designates BygLG as the endogenous variable, with MLmean serving as its instrumental variable. An instrumentalization that is crucial to address potential endogeneity concerns and fortifying the validity of the estimations. The use of the conditional if year==1834 filters the data, retaining only observations where the year variable is equal to 1834. To account for heteroskedasticity and autocorrelation, the vce(robust) command employs a robust variance-covariance matrix. Subsequently, we encounter the lines of code that initially appeared daunting, but now, through regular utilization, their functionality has become comprehensible. Indeed, the estat firststage command is utilized to display statistics from the first-stage of the IV regression, offering insights into the validity of the instrumental variables. Further, the mat fstat = r(singleresults) line stores the first-stage results in a matrix named fstat, facilitating a comprehensive view of the instrumental variable performance. Lastly, the estadd scalar Fstat = fstat[1,4] command introduces a new scalar variable Fstat into the main regression results. This step extracts the F-statistic from the first-stage matrix fstat and assigns it to Once again, for a more comprehensive understanding of the F-statistics and its purpose, referring to the preceding Section 3.2.2 is recommended. We would like to remind you that the results can be independently observed directly using the same code without the qui option, starting from reg or ivreg. This underscores the flexibility of the code, allowing researchers to directly access and analyze the regression outcomes without relying on the table generation step. Formatting Table A.4, an additional but optional step ***Table A.4*** ***Formation of Table A.4*** esttab ols1 fiv1 ivdiff1 ivdiff2, se star(* 0.10 ** 0.05 *** 0.01) b(3) r2 var(15) /// model(11) wrap keep(MLmean BygLG) mtitles(&quot;OLS&quot; &quot;first stage&quot; /// &quot;second stage (diffTheil)&quot; &quot;second stage (diffAggTheil)&quot; ) stats(N r2 Fstat, /// labels(&quot;Observations&quot; &quot;R-squared&quot; &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) /// indicate(&quot;Region FE = 2.region&quot; &quot;Geography = LnDistCPH&quot; , labels(Y N)) label In the next step of our analysis, we construct a comprehensive table to concisely present the findings from our prior regression analyses. It’s important to emphasize that this stage isn’t isolated but rather complements the groundwork laid in the preceding steps, where variables were defined and regressions were conducted. The resultant table serves as a visual tool to depict the relationships identified in our regression models, thereby enhancing the clarity and communicative impact of our analytical insights. In this stage, we use the esttab command to generate a comprehensive table summarizing the results from various models, including OLS, first-stage IV, and second-stage IV regressions. The specified models, denoted as ols1 fiv1 ivdiff1 ivdiff2, capture different aspects of the regressions. The se star(* 0.10 ** 0.05 *** 0.01) option is employed to display standard errors with significance stars, where asterisks indicate different levels of significance with * for 0.10, ** for 0.05, and *** for 0.01. Setting b(3) limits, once again, the number of decimal places for coefficient estimates to 3, contributing to a cleaner presentation. Including the R-squared r2 and limiting the number of decimals for the R-squared to 15 digits var(15) provide additional insights into the explanatory power of the models. Furthermore, the model(11) option ensures that a maximum of 11 different models is displayed in the table. To accommodate long variable names, the wrap command is utilized, allowing for a more organized presentation. Moreover, the keep(MLmean BygLG) option specifies the variables to be included in the table, focusing on MLmean and BygLG. Model titles are designated using the mtitles() option for clarity and organization. Additional statistics, such as the number of observations (N), R-squared, and the F-statistic, are included using the stats(N r2 Fstat, labels(\"Observations\" \"R-squared\" \"KP F-statistic\")fmt(%9.0fc 2 2)) option, providing a more comprehensive overview. Moreover, indicator variables for fixed effects related to a specific region – here 2 representing Jutland – 2.region and geography LnDistCPH are incorporated using the indicate() option. The labels(Y N)) allows to specify these fixed effects as Y if the fixed effects are included or as N if they are not This offers further control in understanding the impact of these factors on the results. Finally, the label option ensures that variable labels are included in the table, enhancing the interpretability of the presented information. Once again, in the provided replication code by the authors, no specific instruction regarding the exportation of tables has been included. Therefore, we recommend users to add the command using \"NameOfYourTable.tex\" right after the last variable mentioned in esttab, just before the comma in front of se star, if they intend to export the table in LaTeX format. Alternatively, users can use the command using NameOfYourTable.txt\", at the same location in the code, if they prefer the table in a text format. This flexibility allows users to choose the desired output format for the tables based on their specific needs. This last step to form a table can be employed in the context of other studies to generate a clear and visually appealing table summarizing regression results. However, it is crucial to emphasize that this step is not indispensable for obtaining replication results. And although we conducted a multitude of regressions in Section 3.5.1, it is noteworthy that the final Table A.4 does not include the ols2 regression. Finally, the results can be independently used directly with the same code without the qui option, starting from reg or ivreg. This highlights the flexibility of the code, allowing researchers to directly access and analyze the regression outcomes without relying on the table generation step. While the table contributes to a more organized presentation, it is not a prerequisite for the replication process, providing users with the option to tailor their analysis based on specific preferences and requirements. Our thorough replication process aimed to transparently convey each step of the analysis, ensuring clarity and reproducibility. We hope that our detailed descriptions provide a comprehensive understanding of the code and methodology employed. Moreover, we hope that this clarity enhances the accessibility of the article and facilitates further examination and validation by other researchers. We trust that transparency is crucial for fostering rigorous and collaborative research practices. Authors: Auvray Adrien, Carette Anne-Laure, Tarlapan Alina, students in the Master program in Development Economics and Sustainable Development (2023-2024), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2023 "],["regression-discontinuity-design.html", "Chapter 3 Regression Discontinuity Design ", " Chapter 3 Regression Discontinuity Design "],["the-persistent-effects-of-perus-mining-mita.html", "3.1 The Persistent Effects of Peru’s Mining Mita", " 3.1 The Persistent Effects of Peru’s Mining Mita Dell, M. (2010). The Persistent Effects of Peru’s Mining Mita. Econometrica, 78(6), 1863-1903. Retrieved 2023-11-20, from http://www.jstor.org/stable/40928464 Download Do-File to download/generate/save the dataset for replication Download Do-File corresponding to the explanations below Download Codebook [Link to the full original replication package paper on Melissa Dell’s webpage] Highlights Dell (2010) examines the long run impacts of the mita, an extensive forced mining labor system in effect in Peru and Bolivia between 1573 and 1812. We will code a spatial regression discontinuity design that is used to compare outcomes of households on each side of the mita boundaries. This paper is a pioneering work in the application of spatial regression discontinuity designs and lays the foundations for a burgeoning literature on the impact of spatially delimited policies. In this document, we provide a detailed explanation on how to implement spatial RDD in your software Stata. A key element of our contribution consists in capturing Conley standard errors using the “x_ols” program (see Install x_ols line 142). In the appendix you will find the codes that allow the automatization of formatted output tables in a ready to publish style with minimal required manual manipulation using matrices and the “appendmodels\" program. 3.1.1 Introduction A useful method for analyzing the impact of a policy when it is implemented in a geographically delimited area is the Spatial Regression Discontinuity Design (Spatial RDD). The paper focuses on the causal effect of the mita, an extensive forced mining labor system in effect in Peru and Bolivia between 1573 and 1812, on household consumption and stunted growth in children. This identification strategy is based on a cut-off defined by geographic borders, specifically the assignment variables are examined by their distance from the mita boundary (the threshold in this research) using a discontinuity in longitude-latitude space. This framework is based on the assumption that all the characteristics of the treatment and the control groups, except the variables of interest, must vary smoothly at the mita cutoff to be able to do a comparison. Therefore, the treatment effect is firstly computed by using cubic polynomials in latitude and longitude in a multidimensional RD polynomial. Then, to strengthen the results, other two single geographical dimension specifications were used. The first one uses the cubic polynomial Euclidian distance to Potosì and the second one the cubic polynomial distance to the mita boundary. 3.1.2 Good Practices 3.1.2.1 Necessary libraries ssc install estout, replace x_ols (Appendix) appendmodels (Appendix) 3.1.2.2 Organization of the directories capture log close clear clear matrix global dirroot &quot;YOUR DIRECTORY GOES HERE&quot; global dirdata &quot;${dirroot}/&quot; global dirlogs &quot;${dirroot}/&quot; global dirtables &quot;${dirroot}/&quot; log using &quot;${dirlogs}/Replication_Dell.log&quot;, replace 3.1.2.3 Building the three different data frames Note: to generate the dataset Dataset_Dell.dta, you first need to download and run this do-file: Download Do-File to download/generate/save the dataset for replication use &quot;${dirdata}Dataset_Dell.dta&quot;, clear keep if gis_db==1 drop gis_db save &quot;${dirdata}gis_grid.dta&quot;, replace use &quot;${dirdata}Dataset_Dell.dta&quot;, clear keep if consumption_db==1 drop consumption_db save &quot;${dirdata}consumption.dta&quot;, replace use &quot;${dirdata}Dataset_Dell.dta&quot;, clear keep if height_db==1 drop height_db save &quot;${dirdata}height.dta&quot;, replace 3.1.3 Table 1 - Summary Statistics Table 1: Summary Statistics 3.1.3.1 Code for Table 1 A critical assumption for the identification strategy is that all the characteristics of the treatment and control vary smoothly at the mita cutoff. Table 1 presents summary statistics for sample characteristics within and outside of the mita boundaries. Different distances from the Mita boundaries are used in the selection of observations to include in each group. Reassuringly, there are no significant differences. We are going to use a loop to move through each distance specification. We are using for this the data gis_grid.dta. Then, with the command tabstat we are going to create a table with the mean and the number of observations for the elevation by group (pothuan_mita, which is 0 if it’s outside the Mita and 1 otherwise) and the same for the slope. This has the option not which is an abbreviation for nototal because we are not interested in the overall statistics. After this we’re going to regress the elevation and the slope with the dummy of pothuan_mita in order to capture the robust standard error of the difference in mean between the two groups. // Elevation and Slope foreach Y of num 100 75 50 25 { //Elevation use &quot;${dirdata}gis_grid.dta&quot;, clear drop if d_bnd&gt;`Y&#39; tabstat elev, by(pothuan_mita) statistics(mean n) not regress elev pothuan_mita, robust //Slope tabstat slope, by(pothuan_mita) statistics(mean n) not regress slope pothuan_mita, robust } We then repeat this with the quantity of indigenous people. We are going to be using the consumption.dta dataset. // Indigenous foreach Y of num 100 75 50 25 { use &quot;${dirdata}consumption.dta&quot;, clear tabstat QUE if (d_bnd&lt;`Y&#39; &amp; cusco!=1), by(pothuan_mita) statistics(mean n) not regress QUE pothuan_mita if (d_bnd&lt;`Y&#39; &amp; cusco!=1), robust cluster (ubigeo) } Finally, we are going to be capturing the Conley Standard Error. This is done using the program x_ols which we install at the beginning of the code. We are also going to do it for the elevation, slope and indigenous people. This command works by specifying the latitude (“xcord\"), longitude (”ycord\"), two cut points in each coordinate and the dependent (“elev\") and independent variable (”pothuan_mita\". xreg(2) and coord(2) are necessary, xreg denotes the number of regressor and coord the dimensions of the coordinates. For more information on this command: https://economics.uwo.ca/people/conley_docs/data_GMMWithCross_99/x_ols.ado //Conley Standard Errors // Elevation and Slope foreach Y of num 100 75 50 25 { // Elevation use &quot;${dirdata}gis_grid.dta&quot;, clear keep if elev&lt;. gen xcut1=1 gen ycut1=1 gen const=1 drop if d_bnd&gt;`Y&#39; x_ols xcord ycord xcut1 ycut1 elev const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 // Slope use &quot;${dirdata}gis_grid.dta&quot;, clear drop if slope==. gen xcut1=1 gen ycut1=1 gen const=1 drop if d_bnd&gt;`Y&#39; x_ols xcord ycord xcut1 ycut1 slope const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 } // Indigenous foreach Y of num 100 75 50 25 { use &quot;${dirdata}consumption.dta&quot;, clear drop if QUE==. drop if cusco==1 gen xcut1=1 gen ycut1=1 gen const=1 drop if d_bnd&gt;`Y&#39; x_ols x y xcut1 ycut1 QUE const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 } 3.1.4 Table 2 - Main Results Table 2: Main Results 3.1.4.1 Code for Table 2 The paper is interested in the impact of Mita on both economic outcomes (household consumption) and outcomes in terms of health (stunted growth) so both results are presented in the main table with the following code. Again, a loop is used to move through each distance specification. The loop ensures that the same regressions are run for each distance specification specified in the local Y. Within the loop, we use the two main datasets on consumption and height and run the same regressions on the two main dependent variables lhhequiv and desnu on the cubic polynomial of the observation’s district capital, controlling for relevant variables. The if conditions specify that the regression omits observations in Cusco and indicates the distance specification that should be used. The robust option specifies that heteroskedasticity-robust standard errors should be reported. The cluster (ubigeo) option indicates that standard errors should be clustered by district. local Y &quot;100 75 50&quot; foreach Y in `Y&#39; { // Log Consumption use &quot;${dirdata}consumption.dta&quot;, clear reg lhhequiv pothuan_mita x y x2 y2 xy x3 y3 x2y xy2 infants children adults elv_sh /// slope bfe4* if (cusco != 1 &amp; d_bnd &lt; `Y&#39;), robust cluster (ubigeo) // Stunted Growth use &quot;${dirdata}height.dta&quot;, clear reg desnu pothuan_mita x y x2 y2 xy x3 y3 x2y xy2 elv_sh slope bfe4* /// if (cusco != 1 &amp; d_bnd &lt; `Y&#39;), robust cluster (ubigeo) } 3.1.5 Table 3 - Robustness Results Table 3: Robustness 3.1.5.1 Code for Table 3 The multidimensional RD polynomial approach was a novel methodology at the time and therefore there was no empirical background able to state a priori why this strategy performs better. Given these concerns, two other single-dimension specifications were applied to examine the robustness of the findings. The first is the polynomial in distance to Potosi, which is likely to capture variation in unobservable characteristics but it is not the most precise approach in mapping RD setup. The second specification controls for a polynomial in distance to the Mita boundary, which is closer to traditional one-dimensional RD designs even if it lacks a historical explanation that states its relevancy. Looking at the results across the three specifications is not possible to reject that they are statistically identical and consequently admit the robustness of the main findings. Another time, we use the two main datasets on consumption and height to capture the causal effect on the dependent variables lhhequiv and desnu. In the loop, first, we consider the effect of the mita on the consumption by including in the regression the Euclidean distance to Potosi in its linear, quadratic, and cubic form (together with relevant control variables and excluding the observation in Cusco) within 50, 70 and 100 km (Y). The second regression is run using the same dependent variable and controls but using the distance to the mita boundary in its linear, quadratic, and cubic terms. The following two regressions have exactly the same specifications but as dependent variable is used stunted growth. Finally, the last two regressions are peculiar to the previous ones but use the observation at the three border limits (again 50, 75, and 100 km from the Mita border). local Y &quot;100 75 50&quot; foreach Y in `Y&#39; { //Log Consumption use &quot;${dirdata}consumption.dta&quot;, clear // Cubic polynomial in distance to Potosi reg lhhequiv pothuan_mita dpot dpot2 dpot3 infants children adults elv_sh slope bfe4* /// if (cusco!=1 &amp; d_bnd&lt;`Y&#39;),robust cluster (ubigeo) // Cubic polynomial in distance to the mita boundary reg lhhequiv pothuan_mita dbnd_sh dbnd_sh2 dbnd_sh3 infants children adults elv_sh /// slope bfe4* if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) // Stunted Growth use &quot;${dirdata}height.dta&quot;, clear // Cubic polynomial in distance to Potosi reg desnu pothuan_mita dpot dpot2 dpot3 elv_sh slope bfe4* if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), /// robust cluster (ubigeo) // Cubic polyonomial in distance to the mita boundary reg desnu pothuan_mita dbnd_sh dbnd_sh2 dbnd_sh3 elv_sh slope bfe4* /// if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) } reg desnu pothuan_mita dpot dpot2 dpot3 elv_sh slope bfe4* if (cusco!=1 &amp; border==1), /// robust cluster (ubigeo) reg desnu pothuan_mita dbnd_sh dbnd_sh2 dbnd_sh3 elv_sh slope bfe4* /// if (cusco!=1 &amp; border==1), robust cluster (ubigeo) 3.1.6 Appendix: Automatization of table outputs The following code allows the creation of each table without minimal manual manipulation. We use two possible approaches, one is through the use of matrices and the other through the use of “eststo\" with the”append models\" program written by Ben Jann (more information here: https://repec.sowi.unibe.ch/stata/estout/other/901.do). 3.1.6.1 Automatic Output for Table 1 eststo clear scalar drop _all clear matrix // Elevation foreach Y of num 100 75 50 25 { //Elevation use &quot;${dirdata}gis_grid.dta&quot;, clear drop if d_bnd&gt;`Y&#39; tabstat elev, by(pothuan_mita) statistics(mean) not save matrix insi_elev_`Y&#39;=r(Stat2) matrix outs_elev_`Y&#39;=r(Stat1) quietly regress elev pothuan_mita, robust matrix V=e(V) scalar se_elev_`Y&#39; = sqrt(V[1,1]) *Conley Standard Errors keep if elev&lt;. gen xcut1=1 gen ycut1=1 gen const=1 x_ols xcord ycord xcut1 ycut1 elev const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 scalar con_se_elev_`Y&#39;=con_se2 //Slope use &quot;${dirdata}gis_grid.dta&quot;, clear drop if d_bnd&gt;`Y&#39; tabstat slope, by(pothuan_mita) statistics(mean) not save matrix insi_slope_`Y&#39;=r(Stat2) matrix outs_slope_`Y&#39;=r(Stat1) tabstat slope, by(pothuan_mita) statistics(n) not save matrix N_insi_slope_`Y&#39;=r(Stat2) matrix N_outs_slope_`Y&#39;=r(Stat1) quietly regress slope pothuan_mita, robust matrix V=e(V) scalar se_slope_`Y&#39; = sqrt(V[1,1]) *Conley Standard Errors drop if slope==. gen xcut1=1 gen ycut1=1 gen const=1 x_ols xcord ycord xcut1 ycut1 slope const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 scalar con_se_slope_`Y&#39;=con_se2 } // Indigenous foreach Y of num 100 75 50 25 { use &quot;${dirdata}consumption.dta&quot;, clear tabstat QUE if (d_bnd&lt;`Y&#39; &amp; cusco!=1), by(pothuan_mita) statistics(mean) not save matrix insi_indig_`Y&#39;=r(Stat2)*100 matrix outs_indig_`Y&#39;=r(Stat1)*100 tabstat QUE if (d_bnd&lt;`Y&#39; &amp; cusco!=1), by(pothuan_mita) statistics(n) not save matrix N_insi_indig_`Y&#39;=r(Stat2) matrix N_outs_indig_`Y&#39;=r(Stat1) quietly regress QUE pothuan_mita if (d_bnd&lt;`Y&#39; &amp; cusco!=1), robust cluster (ubigeo) matrix V=e(V) scalar se_indig_`Y&#39; = sqrt(V[1,1])*100 drop if QUE==. drop if cusco==1 gen xcut1=1 gen ycut1=1 gen const=1 drop if d_bnd&gt;`Y&#39; x_ols x y xcut1 ycut1 QUE const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 scalar con_se_indig_`Y&#39;=con_se2*100 } // Inputting everything on a Matrix // Submatrix 1: Elevation matrix elev = (insi_elev_100, outs_elev_100, con_se_elev_100, insi_elev_75, /// outs_elev_75, con_se_elev_75, insi_elev_50, outs_elev_50, con_se_elev_50, /// insi_elev_25, outs_elev_25, con_se_elev_25 \\ ., ., se_elev_100, ., ., se_elev_75, ., /// ., se_elev_50, ., ., se_elev_25) // Submatrix 2: Slope matrix slope = (insi_slope_100, outs_slope_100, con_se_slope_100, insi_slope_75, /// outs_slope_75, con_se_slope_75, insi_slope_50, outs_slope_50, con_se_slope_50, /// insi_slope_25, outs_slope_25, con_se_slope_25 \\ ., ., se_slope_100, ., ., se_slope_75, /// ., ., se_slope_50, ., ., se_slope_25) // Submatrix 3: Number of observations for elevation and slope matrix num_slope = (N_insi_slope_100, N_outs_slope_100, ., N_insi_slope_75, /// N_outs_slope_75, ., N_insi_slope_50, N_outs_slope_50, ., N_insi_slope_25, /// N_outs_slope_25, .) // Submatrix 4: Indigenous matrix indig = (insi_indig_100, outs_indig_100, con_se_indig_100, insi_indig_75, /// outs_indig_75, con_se_indig_75, insi_indig_50, outs_indig_50, con_se_indig_50, /// insi_indig_25, outs_indig_25, con_se_indig_25 \\ ., ., se_indig_100, ., ., se_indig_75, /// ., ., se_indig_50, ., ., se_indig_25) // Submatrix 5: Number of observations for Indigenous matrix num_indig = (N_insi_indig_100, N_outs_indig_100, ., N_insi_indig_75, /// N_outs_indig_75, ., N_insi_indig_50, N_outs_indig_50, ., N_insi_indig_25, /// N_outs_indig_25, .) // Final Matrix matrix Table1 = (elev\\slope\\num_slope\\indig\\num_indig) matrix rownames Table1 = Elevation &quot;.&quot; Slope &quot;.&quot; Observations %Indigenous &quot;.&quot; Observations matrix colnames Table1 = Inside Outside &quot;Conley SE/SE&quot; Inside Outside &quot;Conley SE/SE&quot; /// Inside Outside &quot;Conley SE/SE&quot; Inside Outside &quot;Conley SE/SE&quot; // Show in command esttab matrix(Table1, fmt(&quot;0 0 2 0 0 2 0 0 &quot; &quot;0 0 2 0 0 2 0 0 &quot; &quot;2 2 2 2 0 2 2&quot;)), /// refcat(Elevation &quot;GIS Measures&quot; %Indigenous &quot;&quot;, nolab) /// coef(. &quot; &quot;) /// title(&quot;Table 1: Summary Statistics&quot;) nomti /// addn(&quot;The unit of observation is 20 × 20 km grid cells for the geospatial measures and the household for % indigenous. Conley standard errors for the difference in means between mita and non-mita observations are first in each SE columns. Robust standard errors for the difference in means are second. For % indigenous, the robust standard errors are corrected for clustering at the district level. The geospatial measures are calculated using elevation data at 30 arc second (1 km) resolution (SRTM (2000)). The unit of measure for elevation is 1000 meters and for slope is degrees. A household is indigenous if its members primarily speak an indigenous language in the home (ENAHO (2001)). In the first three columns, the sample includes only observations located less than 100 km from the mita boundary, and this threshold is reduced to 75, 50, and finally 25 km in the succeeding columns.&quot;) replace 3.1.6.2 Automatic Output for Table 2 eststo clear scalar drop _all clear matrix local Y &quot;100 75 50&quot; local j = 1 foreach Y in `Y&#39; { // Log Consumption use &quot;${dirdata}consumption.dta&quot;, clear quietly reg lhhequiv pothuan_mita x y x2 y2 xy x3 y3 x2y xy2 infants children adults /// elv_sh slope bfe4* if (cusco != 1 &amp; d_bnd &lt; `Y&#39;), robust cluster (ubigeo) local i = 1 scalar Nclusters_`j&#39;_`i&#39; =e(N_clust) scalar N_`j&#39;_`i&#39; =e(N) scalar R_`j&#39;_`i&#39; =e(r2) matrix V=e(V) matrix B=e(b) scalar se_`j&#39;_`i&#39; = sqrt(V[1,1]) scalar b_`j&#39;_`i&#39; = (B[1,1]) local i = `i&#39; + 1 // Stunted Growth use &quot;${dirdata}height.dta&quot;, clear quietly reg desnu pothuan_mita x y x2 y2 xy x3 y3 x2y xy2 elv_sh slope bfe4* /// if (cusco != 1 &amp; d_bnd &lt; `Y&#39;), robust cluster (ubigeo) scalar Nclusters_`j&#39;_`i&#39; =e(N_clust) scalar N_`j&#39;_`i&#39; =e(N) scalar R_`j&#39;_`i&#39; =e(r2) matrix V=e(V) matrix B=e(b) scalar se_`j&#39;_`i&#39; = sqrt(V[1,1]) scalar b_`j&#39;_`i&#39; = (B[1,1]) local j = `j&#39; + 1 } matrix Main = (b_1_1, b_2_1, b_3_1 \\ se_1_1, se_2_1, se_3_1 \\ R_1_1, R_2_1, R_3_1 \\ /// N_1_1, N_2_1, N_3_1 \\ Nclusters_1_1, Nclusters_2_1, Nclusters_3_1 \\ b_1_2, b_2_2, /// b_3_2 \\ se_1_2, se_2_2, se_3_2 \\ R_1_2, R_2_2, R_3_2 \\ N_1_2, N_2_2, N_3_2 \\ /// Nclusters_1_2, Nclusters_2_2, Nclusters_3_2) matrix rownames Main = Mita1 &quot;.&quot; R2 &quot;Number of Observations&quot; &quot;Number of Clusters&quot; Mita2 /// &quot;.&quot; R2 &quot;Number of Observations&quot; &quot;Number of Clusters&quot; matrix colnames Main = &quot;&lt;100 km of Bound.&quot; &quot;&lt;75 km of Bound.&quot; &quot;&lt;50 km of Bound.&quot; esttab matrix(Main, fmt(&quot;3 3 3 0 0 3 3 3 0 0&quot;)), /// model(17) /// varwidth(40) /// not /// refcat(Mita1 &quot;Log Equiv. Household Consumption (2001)&quot; /// Mita2 &quot;Stunted Growth, Children 6-9 (2005)&quot;, nolab) /// coef(. &quot; &quot; Mita1 &quot;Mita&quot; Mita2 &quot;Mita&quot;) nomti /// title(&quot;Table 2: Main Results for replication&quot;) 3.1.6.3 Automatic Output for Table 3 eststo clear scalar drop _all clear matrix local Y &quot;100 75 50&quot; local i=1 foreach Y in `Y&#39; { //Log Consumption use &quot;${dirdata}consumption.dta&quot;, clear ren pothuan_mita row1 //Cubic polynomial in distance to Potosi eststo id`i&#39;: quietly reg lhhequiv row1 dpot dpot2 dpot3 infants children adults /// elv_sh slope bfe4* if (cusco!=1 &amp; d_bnd&lt;`Y&#39;),robust cluster (ubigeo) quietly matrix R_`i&#39; =e(r2) quietly scalar Nclusters_`i&#39; = e(N_clust) quietly scalar N_`i&#39; = e(N) //Cubic polynomial in distance to the mita boundary ren row1 row2 local i = `i&#39;+ 1 eststo id`i&#39;: quietly reg lhhequiv row2 dbnd_sh dbnd_sh2 dbnd_sh3 infants children /// adults elv_sh slope bfe4* if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) quietly matrix R_`i&#39; =e(r2) // Stunted Growth use &quot;${dirdata}height.dta&quot;, clear //Cubic polynomial in distance to Potosi ren pothuan_mita row1 local i = `i&#39;+ 1 eststo id`i&#39;: quietly reg desnu row1 dpot dpot2 dpot3 elv_sh slope bfe4* /// if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) quietly matrix R_`i&#39; =e(r2) quietly scalar Nclusters_`i&#39; = e(N_clust) quietly scalar N_`i&#39; = e(N) //Cubic polyonomial in distance to the mita boundary ren row1 row2 local i = `i&#39;+ 1 eststo id`i&#39;: quietly reg desnu row2 dbnd_sh dbnd_sh2 dbnd_sh3 elv_sh slope bfe4* /// if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) quietly matrix R_`i&#39; =e(r2) local i = `i&#39;+ 1 } use &quot;${dirdata}height.dta&quot;, clear ren pothuan_mita row1 eststo id13: quietly reg desnu row1 dpot dpot2 dpot3 elv_sh slope bfe4* /// if (cusco!=1 &amp; border==1), robust cluster (ubigeo) quietly matrix R_13 =e(r2) quietly scalar Nclusters_13 = e(N_clust) quietly scalar N_13 = e(N) ren row1 row2 eststo id14: quietly reg desnu row2 dbnd_sh dbnd_sh2 dbnd_sh3 elv_sh slope bfe4* /// if (cusco!=1 &amp; border==1), robust cluster (ubigeo) quietly matrix R_14 =e(r2) local i=&quot;1 5 9 3 7 11 13&quot; local j=&quot;2 6 10 4 8 12 14&quot; foreach i in `i&#39; { quietly matrix coln R_`i&#39; = R_1 } foreach j in `j&#39; { quietly matrix coln R_`j&#39; = R_2 } // Append Models local i=&quot;1 5 9 3 7 11 13&quot; local k=1 foreach i in `i&#39; { eststo c`k&#39;: appendmodels id`i&#39; id`++i&#39; quietly estadd local geoc &quot;Yes&quot; quietly estadd local bound &quot;Yes&quot; quietly estadd local obs = (N_`--i&#39;) quietly estadd local clu = (Nclusters_`i&#39;) estadd matrix R2_1=R_`i&#39; estadd matrix R2_2=R_`++i&#39; local k = `k&#39; + 1 } esttab c1 c2 c3 c4 c5 c6 c7, /// keep(row1 row2 R_1 R_2) /// coeflabels(row1 &quot;Mita&quot; row2 &quot;Mita&quot; R_1 &quot;R2&quot; R_2 &quot;R2&quot;) /// cells(b(star fmt(3)) se(par fmt(3)) R2_1(fmt(3)) R2_2(fmt(3))) /// star(* 0.1 ** 0.05 *** 0.01) noobs /// title(&quot;TABLE II LIVING STANDARDS&quot;) /// ml(&quot;&lt;100 km of Bound&quot; &quot;&lt;75 km of Bound&quot; &quot;&lt;50 km of Bound&quot; &quot;&lt;100 km of Bound&quot; /// &quot;&lt;75 km of Bound&quot; &quot;&lt;50 km of Bound&quot; &quot;Border District&quot;) /// model(17) varwidth(30) /// posth(&quot;Sample within: &quot; `&quot;{hline @width}&quot;&#39;) hlinechar(&quot;-&quot;) /// mgroups(&quot;&quot; &quot;Log Equiv. Household Consumption (2001)&quot; /// &quot;Stunted Growth, Children 6-9 (2005)&quot;, pattern(1 1 0 0 1 0 0) span) /// s(geoc bound clu obs, labels(&quot;Geo. controls&quot; /// &quot;Boundary F.E.s&quot; &quot;Clusters&quot; &quot;Observations&quot;)) /// order(row1 R_1 row2 R_2) /// refcat(row1 &quot;Panel B. Cubic Polynomial in Distance to Potosi&quot; row2 /// &quot;Panel C. Cubic Polynomial in Distance to Mita Boundary&quot;, nolabel) nonote /// addnote(&quot;The unit of observation is the household in columns 1–3 and the individual in columns 4–7. Robust standard errors, adjusted for clustering by district, are in parentheses. The dependent variable is log equivalent household consumption (ENAHO (2001)) in columns 1–3, and a dummy equal to 1 if the child has stunted growth and equal to 0 otherwise in columns 4–7 (Ministro de Educación (2005a)). Mita is an indicator equal to 1 if the household&#39;s district contributed to the mita and equal to 0 otherwise (Saignes (1984), Amat y Juniet (1947, pp. 249, 284)). Panel B includes a cubic polynomial in Euclidean distance from the observation&#39;s district capital to Potosí, and panel C includes a cubic polynomial in Euclidean distance to the nearest point on the mita boundary. All regressions include controls for elevation and slope, as well as boundary segment fixed effects (F.E.s). Columns 1–3 include demographic controls for the number of infants, children, and adults in the household. In columns 1 and 4, the sample includes observations whose district capitals are located within 100 km of the mita boundary, and this threshold is reduced to 75 and 50 km in the succeeding columns. Column 7 includes only observations whose districts border the mita boundary. 78% of the observations are in mita districts in column 1, 71% in column 2, 68% in column 3, 78% in column 4, 71% in column 5, 68% in column 6, and 58% in column 7. Coefficients that are significantly different from zero are denoted by the following system: *10%, **5%, and ***1%.&quot;) tex replace 3.1.6.4 Install “x_ols” and “appendmodels” programs x_ols capt prog drop x_ols program define x_ols version 6.0 #delimit ; /*sets `;&#39; as end of line*/ /*FIRST I TAKE INFO. FROM COMMAND LINE AND ORGANIZE IT*/ local varlist &quot;req ex min(1)&quot;; /*must specify at least one variable... all must be existing in memory*/ local options &quot;xreg(int -1) COord(int -1)&quot;; /* # indep. var, dimension of location coordinates*/ parse &quot;`*&#39;&quot;; /*separate options and variables*/ if `xreg&#39;&lt;1{; if `xreg&#39;==-1{; di in red &quot;option xreg() required!!!&quot;; exit 198}; di in red &quot;xreg(`xreg&#39;) is invalid&quot;; exit 198}; if `coord&#39;&lt;1{; if `coord&#39;==-1{; di in red &quot;option coord() required!!!&quot;; exit 198}; di in red &quot;coord(`coord&#39;) is invalid&quot;; exit 198}; /*Separate input variables: coordinates, cutoffs, dependent, regressors*/ parse &quot;`varlist&#39;&quot;, parse(&quot; &quot;); local a=1; while `a&#39;&lt;=`coord&#39;{; tempvar coord`a&#39;; gen `coord`a&#39;&#39;=``a&#39;&#39;; /*get coordinates*/ local a=`a&#39;+1}; local aa=1; while `aa&#39;&lt;=`coord&#39;{; tempvar cut`aa&#39;; gen `cut`aa&#39;&#39;=``a&#39;&#39;; /*get cutoffs*/ local a=`a&#39;+1; local aa=`aa&#39;+1}; tempvar Y; gen `Y&#39;=``a&#39;&#39;; /*get dep variable*/ local depend : word `a&#39; of `varlist&#39;; local a=`a&#39;+1; local b=1; while `b&#39;&lt;=`xreg&#39;{; tempvar X`b&#39;; local ind`b&#39; : word `a&#39; of `varlist&#39;; gen `X`b&#39;&#39;= ``a&#39;&#39;; local a=`a&#39;+1; local b=`b&#39;+1}; /*get indep var(s)...rest of list*/ /*NOW I RUN THE REGRESSION AND COMPUTE THE COV MATRIX*/ quietly{; /*so that steps are not printed on screen*/ /*(1) RUN REGRESSION*/ tempname XX XX_N invXX invN; scalar `invN&#39;=1/_N; if `xreg&#39;==1 {; reg `Y&#39; `X1&#39;, noconstant robust; mat accum `XX&#39;=`X1&#39;,noconstant; mat `XX_N&#39;=`XX&#39;*`invN&#39;; mat `invXX&#39;=inv(`XX_N&#39;)}; /* creates (X&#39;X/N)^(-1)*/ else{; reg `Y&#39; `X1&#39;-`X`xreg&#39;&#39;, noconstant; mat accum `XX&#39;=`X1&#39;-`X`xreg&#39;&#39;,noconstant; mat `XX_N&#39;=`XX&#39;*`invN&#39;; mat `invXX&#39;=inv(`XX_N&#39;)}; /* creates (X&#39;X/N)^(-1)*/ predict epsilon,residuals; /* OLS residuals*/ /*(2) COMPUTE CORRECTED COVARIANCE MATRIX*/ tempname XUUX XUUX1 XUUX2 XUUXt; tempvar XUUk; mat `XUUX&#39;=J(`xreg&#39;,`xreg&#39;,0); gen `XUUk&#39;=0; gen window=1; /*initializes mat.s/var.s to be used*/ local i=1; while `i&#39;&lt;=_N{; /*loop through all observations*/ local d=1; replace window=1; while `d&#39;&lt;=`coord&#39;{; /*loop through coordinates*/ if `i&#39;==1{; gen dis`d&#39;=0}; replace dis`d&#39;=abs(`coord`d&#39;&#39;-`coord`d&#39;&#39;[`i&#39;]); replace window=window*(1-dis`d&#39;/`cut`d&#39;&#39;); replace window=0 if dis`d&#39;&gt;=`cut`d&#39;&#39;; local d=`d&#39;+1}; /*create window*/ capture mat drop `XUUX2&#39;; local k=1; while `k&#39;&lt;=`xreg&#39;{; replace `XUUk&#39;=`X`k&#39;&#39;[`i&#39;]*epsilon*epsilon[`i&#39;]*window; mat vecaccum `XUUX1&#39;=`XUUk&#39; `X1&#39;-`X`xreg&#39;&#39;, noconstant; mat `XUUX2&#39;=nullmat(`XUUX2&#39;) \\ `XUUX1&#39;; local k=`k&#39;+1}; mat `XUUXt&#39;=`XUUX2&#39;&#39;; mat `XUUX1&#39;=`XUUX2&#39;+`XUUXt&#39;; scalar fix=.5; /*to correct for double-counting*/ mat `XUUX1&#39;=`XUUX1&#39;*fix; mat `XUUX&#39;=`XUUX&#39;+`XUUX1&#39;; local i=`i&#39;+1}; mat `XUUX&#39;=`XUUX&#39;*`invN&#39;; }; /*end quietly command*/ tempname V VV; mat `V&#39;=`invXX&#39;*`XUUX&#39;; mat `VV&#39;=`V&#39;*`invXX&#39;; matrix cov_dep=`VV&#39;*`invN&#39;; /*corrected covariance matrix*/ /*THIS PART CREATES AND PRINTS THE OUTPUT TABLE IN STATA*/ local z=1; local v=`a&#39;; di _newline(2) _skip(5) &quot;Results for Cross Sectional OLS corrected for Spatial Dependence&quot;; di _newline _col(35) &quot; number of observations= &quot; _result(1); di &quot; Dependent Variable= &quot; &quot;`depend&#39;&quot;; di _newline &quot;variable&quot; _col(13) &quot;ols estimates&quot; _col(29) &quot;White s.e.&quot; _col(42) &quot;s.e. corrected for spatial dependence&quot;; di &quot;--------&quot; _col(13) &quot;-------------&quot; _col(29) &quot;----------&quot; _col(42) &quot;-------------------------------------&quot;; while `z&#39;&lt;=`xreg&#39;{; tempvar se1`z&#39; se2`z&#39;; local beta`z&#39;=_b[`X`z&#39;&#39;]; local se`z&#39;=_se[`X`z&#39;&#39;]; gen `se1`z&#39;&#39;=cov_dep[`z&#39;,`z&#39;]; gen `se2`z&#39;&#39;=sqrt(`se1`z&#39;&#39;); di &quot;`ind`z&#39;&#39;&quot; _col(13) `beta`z&#39;&#39; _col(29) `se`z&#39;&#39; _col(42) `se2`z&#39;&#39;; scalar con_se`z&#39;=`se2`z&#39;&#39;; // ADDED BY THE REPLICATORS: This line is to // capture Conley S.E. local z=`z&#39;+1}; end appendmodels capt prog drop appendmodels program appendmodels, eclass *! version 1.0.0 14aug2007 Ben Jann // using first equation of model version 8 syntax namelist tempname b V tmp foreach name of local namelist { qui est restore `name&#39; mat `tmp&#39; = e(b) local eq1: coleq `tmp&#39; gettoken eq1 : eq1 mat `tmp&#39; = `tmp&#39;[1,&quot;`eq1&#39;:&quot;] local cons = colnumb(`tmp&#39;,&quot;_cons&quot;) if `cons&#39;&lt;. &amp; `cons&#39;&gt;1 { mat `tmp&#39; = `tmp&#39;[1,1..`cons&#39;-1] } mat `b&#39; = nullmat(`b&#39;) , `tmp&#39; mat `tmp&#39; = e(V) mat `tmp&#39; = `tmp&#39;[&quot;`eq1&#39;:&quot;,&quot;`eq1&#39;:&quot;] if `cons&#39;&lt;. &amp; `cons&#39;&gt;1 { mat `tmp&#39; = `tmp&#39;[1..`cons&#39;-1,1..`cons&#39;-1] } capt confirm matrix `V&#39; if _rc { mat `V&#39; = `tmp&#39; } else { mat `V&#39; = /// ( `V&#39; , J(rowsof(`V&#39;),colsof(`tmp&#39;),0) ) \\ /// ( J(rowsof(`tmp&#39;),colsof(`V&#39;),0) , `tmp&#39; ) } } local names: colfullnames `b&#39; mat coln `V&#39; = `names&#39; mat rown `V&#39; = `names&#39; eret post `b&#39; `V&#39; eret local cmd &quot;whatever&quot; end; exit; Authors: Alejandro Arciniegas Herrera, Marcella De Giovanni, Anselm Rabaté, Kenan Topalovic, students in the Master program in Development Economics and Sustainable Development (2023-2024), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2023 "],["political-fragmentation-and-government-stability-evidence-from-local-governments-in-spain.html", "3.2 Political Fragmentation and Government Stability: Evidence from Local Governments in Spain", " 3.2 Political Fragmentation and Government Stability: Evidence from Local Governments in Spain Carozzi, F., Cipullo, Da. &amp; Repetto, L. (2022). Political Fragmentation and Government Stability: Evidence from Local Governments in Spain. American Economic Journal: Applied Economics, 14 (2): 23-50. https://www.aeaweb.org/articles?id=10.1257/app.20200128 Download Do-File corresponding to the explanations below Download Codebook Download Original Data (from Open ICPSR: click on Data_final.dta and Data_final_allparties.dta) [Link to the full original replication package paper on OSF] Highlights The paper investigates the impact of political fragmentation on government stability. The authors employ a fuzzy Regression Discontinuity Design (RDD) model. It belongs to the the regression discontinuity design broad methodology, which is a quasi experimental method used to estimate the causal effect of an intervention over a threshold or a cutoff point. It enables the comparison of individuals immediately above and below this specific cutoff point to assess the impact of a treatment on a particular outcome. In a fuzzy RDD design the assignment rule determining how units are selected into the treatment is probabilistic. The probability of treatment exhibits a discontinuity at the cutoff, though not as a definitive 0 to 1 jump. In regards to the original replication package, we present here a simplified version of its main elements for an easy replication with only one dataset and one do-file. Through this exercise, on top of standard Stata commands, we present a more in-depth explanation on how to make results’ tables using the esttab command, make more complex graphs with the twoway command and, specially, discover some commands from the rdrobust package, key for RDD analysis in Stata. 3.2.1 Introduction The replication of research papers plays a pivotal role for Economics students. It allows them to learn about the implementation of econometrics methods, and to enhance the use of statistical software programs to further develop empirical works. This document provides a replication exercise for a fuzzy regression discontinuity design (RDD) model that estimates the impact of political fragmentation on government stability. It builds on a paper by Carozzi, Cipullo and Repetto (2022). Specifically, this project aims to provide a comprehensive guide on executing an RDD model in Stata. It details the execution of the model through the use of commands and code, while explaining the fundamental concepts of this methodology in the process. The content is structured in six parts. Section one provides an explanation on RDD methodology and a summary of the paper. Section two briefly explains how to set up your Stata environment to be able to do smoothly run this replication. Section three explains how to obtain and present summary statistics. Section four addresses the empirical strategy, explaining how to plot discontinuity graphs. Section five presents the main results, while showing how to perform RDD regressions and store and present results in tables. Finally, section six discusses the importance of robustness checks and provides explanations on the helpful commands to perform them in the context of the presented methodology. 3.2.2 RDD Methodology and Paper Summary The regression discontinuity design (RDD) is a quasi experimental method used to estimate the causal effect of an intervention over a threshold or a cutoff point. It enables the comparison of individuals immediately above and below this specific cutoff point to assess the impact of a treatment on a particular outcome. The method relies on the assumption that individuals near the cutoff point share similar observed and unobserved characteristics. An RDD may adopt a fuzzy design, wherein the assignment rule determining how units are selected into the treatment is probabilistic. The probability of treatment exhibits a discontinuity at the cutoff, though not as a definitive 0 to 1 jump. The fuzziness in the design derives from factors such as imperfect compliance or manipulation of the selected threshold. In terms of results, an RDD estimates local average treatment effects around the cutoff point, where treatment and comparison individuals are most similar. In the paper we replicate, the authors adopt a fuzzy RDD model in order to study how political fragmentation affects government stability. The data employed is a panel of Spanish municipalities from 1979 to 2014. The time dimension corresponds to each legislature, indexed by the year of the corresponding municipal election (1979 to 2011). The main data sources consist of electoral records, data on individual mayors and mayoral changes, municipal demographics (population, density, etc.), and data on the composition of regional and national governments. Electoral outcomes in municipal elections were obtained from the Ministry of Internal Affairs and residential registry. The sample consists of municipalities with more than 250 inhabitants for a total of 51,000 elections. The final dataset includes 42,259 elections because of additional data restrictions. To obtain causal estimates of the effect of fragmentation – measured as the number of parties in the council – on government stability, they exploit the existence of a 5% vote-share threshold for admission to the local council. This threshold causes parties with vote-shares just below 5% to be excluded from the council, generating exogenous variation in the number of parties with representation. Results show that each additional party with representation in the parliament increases the probability that the incumbent government is unseated by 5 percentage points. 3.2.3 Good Practices Before Starting In this section, we advise on two procedures before starting an empirical analysis in Stata. First, the creation of the toy dataset from the original replication package, for which we also provide a codebook (see header for downloads). Then, the creation of folders, which assures an organized storage of the analysis’ inputs and outputs. Finally, the installation of all packages required to successfully carry out the replication. Toy dataset clear set more off *download datasets Data_final.dta and Data_final_allparties.dta from https://www.openicpsr.org/openicpsr/project/125341/version/V1/view?path=/openicpsr/125341/fcr:versions/V1/PFGS_Replication_package_AEJ/Data/Clean&amp;type=folder global dt &quot;PUT YOUR OWN WORKING DIRECTORY HERE (where you stored the data)&quot; ******************************************************************************* *DATASET FOR DESCRIPTIVE STATISTICS ******************************************************************************* use &quot;$dt\\Data_final.dta&quot;, clear * Drop problematic obs. keep if tag == 0 | tag == 6 *Keep some variables keep id_towns log_surface surface pop* tag* election_year nominated_date3 total_seats CCAA party1 party2 population_scrutiny votes_blank votes_void province nmayors votes_total_muni turnout nparties_seats nparties allvalidvotes log_pop log_surface party_five_percent* abs_majority party_mayor* seats_total mocion_5 egen population_average=mean(population_legal), by(id_towns) replace population_average=population_average/1000 egen number_elections=sum(1), by(id_towns) label var population_average &quot;Mean Population 000s (1979-2014)&quot; label var surface &quot;Surface (in km2)&quot; label var number_elections &quot; # of Elections in sample &quot; gen PP_mayor=(party_mayor1==&quot;PP&quot;| party_mayor1==&quot;AP/PDP/UL&quot;)*100 gen PSOE_mayor=(party_mayor1==&quot;PSOE&quot;)*100 gen IU_mayor=(party_mayor1==&quot;IU&quot;| party_mayor1==&quot;PCE&quot;)*100 gen CIU_mayor=(party_mayor1==&quot;CIU&quot;)*100 gen mocion_5_100=mocion_5*100 replace abs_majority=abs_majority*100 label var mocion_5_100 &quot;Vote of No Confidence (%)&quot; label var nparties &quot;# of Parties Running&quot; label var seats_total &quot;# of Council Seats&quot; label var abs_majority &quot;Single-party Majority (%)&quot; label var nparties_seats &quot;# of Parties in Council&quot; label var PP_mayor &quot;1st Mayor - PP (%)&quot; label var PSOE_mayor &quot;1st Mayor - PSOE (%)&quot; label var IU_mayor &quot;1st Mayor - IU (%)&quot; label var CIU_mayor &quot;1st Mayor - CIU (%)&quot; gen Descrip_sample = 1 label var Descrip_sample &quot;1 if observation corresponds to sample data to replicate the descriptive statistics&quot; keep id_towns election_year population_average surface number_elections nparties nparties_seats seats_total mocion_5_100 abs_majority PP_mayor PSOE_mayor IU_mayor CIU_mayor abs_majority nparties_seats mocion_5 Descrip_sample save &quot;$dt\\Database1.dta&quot;, replace ****************************************************************************** *DATASET FOR FIGURE 3 AND MAIN RESULTS ****************************************************************************** use &quot;$dt\\Data_final_allparties.dta&quot;, clear * Use as r.v. the vote share distance of each party from 5% * replace rv = all_rv replace d = all_d replace rv_d = all_rv_d *Drop problematic obs. keep if tag == 0 | tag == 6 *Label label var mocion_5 &quot;Mayor uns.&quot; *keeping useful variables keep log_pop log_surface election_year seats_total id_towns nparties nparties_seats rv d rv_d mocion_5 gen Mainresults_sample = 1 label var Mainresults_sample &quot;1 if observation corresponds to sample data to replicate the main results and robustness&quot; save &quot;$dt\\Database2.dta&quot;, replace ******************************************************************************* *FINAL REPLICATION DATASET ******************************************************************************* use &quot;$dt\\Database1.dta&quot;, clear append using &quot;$dt\\Database2.dta&quot; save &quot;$dt\\Dataset_CCR.dta&quot;, replace Folder creation Create the appropriate folders and globals to store your datasets and results. A global is a named storage location that can hold a value or a string of text. We use the command “global” to define it and the prefix “$” to access it. clear all set more off global dt &quot;C:\\Users\\Dell\\Desktop\\laurine_stata_files\\CCR&quot; Package Installation To carry out the replication of this paper, the following packages must be installed: ssc install missings // Provides tools and commands for working with missing data. ssc install rdrobust // Provides tools and commands to execute a regression discontinuity // design methodology. ssc install ivreg2 // Extends Stata’s built-in instrumental variables (IV) estimation // capabilities. ssc install estout // Provides additional formatting and exporting options for regression // results. ssc install outreg2 // Provides additional options formatting regression results for // output in tables. ssc install ranktest // Provides additional tests for rank-related issues, such as rank // correlation coefficients. 3.2.4 Descriptive Statistics After setting the Stata environment, we can begin to explore the data characteristics through the computing of descriptive statistics to achieve the replication of Table 1. The dataset provided for this replication exercise contains two samples: The first sample, used to make the descriptive statistics, consists of a town panel by election year. Therefore, each observation corresponds to a town on an specific election year. To access it, either keep at the beginning only the observations for which the sample indicator is equal to 1 or state it as a condition at the end of every estimation code line. We suggest using the first option: keep if Descrip_sample == 1 The second sample, used to produce the main results and robustness tests for the paper, consists of a party dataset, where in each observation a party appears once per town and election year. To access it, either keep at the beginning only the observations for which the sample indicator is equal to 1 or state it as a condition at the end of every estimation code line. We suggest using the first option: keep if Mainresults_sample == 1 Table 1: Descriptive Statistics Now, in order to replicate each panel from Table 1. Descriptive Statistics we first open the dataset and, as stated before, keep only the observations needed for the replication of the descriptive statistics: use &quot;$dt\\Dataset_CCR.dta&quot;, clear keep if Descrip_sample == 1 Panel A. General information We now use the ”preserve” command to create a copy of the dataset in the Stata memory, so that any changes or modifications will only affect this temporary copy. This is done as subsequently the variable “election year” is dropped and the town duplicates too. This leaves us with a new dataset that corresponds to one observation per town and their respective population, surface and number of elections averages measures. preserve drop election_year duplicates drop id_towns, force Now, we compute some summary statistics (mean, standard deviation, minimum, and maximum) using the “tabstat” command and store them in a matrix with the help of the “eststo: estpost” commands. After, we restore the dataset to its initial state before the “preserve” command was initialized. eststo clear //Clear any previously stored matrix eststo panelA: estpost tabstat population_average surface number_elections, /// stat(mean sd min max) columns(statistics) restore Finally, we can replicate the table of Panel A of the Descriptive statistics table using the “esttab” command, which takes stored results from one or more estimation commands and formats them into a table. esttab panelA, cells((mean(fmt(%18.2fc)) sd(fmt(2)) min(fmt(1)) max(fmt(0)))) nomtitles /// nonum collabels(&quot;Mean&quot; &quot;Std. dev.&quot; &quot;Min&quot; &quot;Max&quot;) /// title(&quot;TABLE 1. DESCRIPTIVE STATISTICS&quot;) /// refcat(population_average &quot;A. General information&quot;, nolabel) /// obslast varwidth(35) label /// addnotes(Notes: &quot;Panel A provides descriptives at the municipal level for all municipalities that appear at least once in our sample.&quot;) Explanation of “esttab” syntaxis and options: cells: specifies which results to show. nomtitles: removes the variable names in the columns’ titles. nonum: removes the numbering rows from the columns. collabels: adds custom column labels. title: adds a title for the table. refcat: creates an additional line with descriptions above some certain variables /groups of variables. obslast: puts the number of observations last. varwidth: specifies the width of the variable names displayed in the table. label: shows labels instead of variable names. addnotes: adds footnotes to the table. The same computation of the summary statistics, storage and tabulation process is repeated for the remaining panels. Panel B. Municipal Elections and Local Government eststo panelB: estpost tabstat nparties nparties_seats seats_total mocion_5_100 /// abs_majority PP_mayor PSOE_mayor IU_mayor CIU_mayor , stat(mean sd min max) /// columns(statistics) esttab panelB, cells((mean(fmt(%18.2fc)) sd(fmt(2)) min(fmt(1)) max(fmt(0)) )) nomtitles /// nonum collabels(&quot;Mean&quot; &quot;Std. dev.&quot; &quot;Min&quot; &quot;Max&quot;) /// refcat(nparties &quot;B. Municipal Elections and Local Government&quot;, nolabel) /// obslast varwidth(35) label /// addnotes(Notes: &quot;Panel B provides descriptives on electoral outcomes at the municipality-council level.&quot;) Panel C.1 - Local Government - Stable Mayor eststo panelC1: estpost tabstat abs_majority nparties_seats if mocion_5==0, /// stat(mean sd min max) columns(statistics) esttab panelC1, cells((mean(fmt(%18.2fc)) sd(fmt(2)) min(fmt(1)) max(fmt(0)) )) /// nomtitles nonum collabels(&quot;Mean&quot; &quot;Std. dev.&quot; &quot;Min&quot; &quot;Max&quot;) refcat(abs_majority /// &quot;C1. Local Government - Stable Mayor&quot;, nolabel) obslast varwidth(35) label /// addnotes(Notes: &quot;Panel C splits this sample into councils that approved at least one vote of no confidence during the term (C2), and those that did not(C1)&quot;) Panel C.2 - Local Government - Vote of No Confidence eststo panelC2: estpost tabstat abs_majority nparties_seats if mocion_5==1, /// stat(mean sd min max) columns(statistics) esttab panelC2, cells((mean(fmt(%18.2fc)) sd(fmt(2)) min(fmt(1)) max(fmt(0)) )) /// nomtitles nonum collabels(&quot;Mean&quot; &quot;Std. dev.&quot; &quot;Min&quot; &quot;Max&quot;) /// refcat(abs_majority &quot;C2. Local Government - Vote of No Confidence&quot;, nolabel) /// obslast varwidth(35) label /// addnotes(Notes: &quot;Panel C splits this sample into councils that approved at least one vote of no confidence during the term (C2), and those that did not (C1)&quot;) 3.2.5 Empirical Strategy After having carried out the descriptive statistics analysis, in this section we dive into the empirical strategy of the research paper. First, we describe the model specification, then we provide explanations on how to plot RDD graphs and, finally, we address how to perform the first stage. The paper employs the following main specification: \\[\\begin{equation} \\tag{1} Y_{it} = α_{1} + τ_{1}N_{it} + β_{1}V_{pit} + β_{2}V_{pit}D_{pit} + π_{it} \\end{equation}\\] Yit an indicator equal to 1 if the mayor of municipality i is unseated and replaced by a new mayor during term t – to the measure of fragmentation. This corresponds to the variable “mocion 5” in the dataset. Nit the number of parties with seats in the council. This corresponds to the variable “nparties seats” in the dataset. Vpit the running variable, representing the difference between the vote-share of each party p and 5% in each municipality i and for each term t. This corresponds to the variable “rv” in the dataset. The number of parties N is instrumented with an indicator D equal 1 for a party being above the 5% threshold. This corresponds to the variable “d” in the dataset. First stage estimation: \\[\\begin{equation} \\tag{2} N_{it} = α_{0} + γ_{1}D_{pit} + δ_{1}V_{pit} + δ_{2}V_{pit}D_{pit} + u_{pit} \\end{equation}\\] Figure 1 shows both the first stage and the reduced form graphs. These figures are relevant on RDD analysis, since they provide a visual representation of the existence of a discontinuity at the threshold. Figure 1: The effect of fragmentation on instability - first stage and reduced form (RD graph) In order to replicate these graphs, the mains results and the robustness checks, first we are going to keep only the observations needed. use &quot;$dt\\Dataset_CCR.dta&quot;, clear keep if Mainresults_sample == 1 Figure 1: Top Panel Since this is a fuzzy design, the first panel plots the running variable against the variable that it will instrument. In this case, the running variable against the number of party seats in council. In order to do an RDD plot, it is necessary to establish a bandwidth (to take into account only observations close to the threshold) and if there are many, sort the observations into bins to make them easier to plot. preserve local increment = 0.0025 // This is done to create the bins of the running variable, which // refer to intervals in which the data is grouped or divided. egen rv_bin = cut(rv), at(-0.025(`increment&#39;)0.025) // Create a new variable rv_bin that // categorizes the variable rv into 7 // bins based on the specified range // and increment // Generate x-axis points as the average within the bin: egen nparties_mean=mean(nparties_seats) if nparties_seats != ., by(rv_bin) replace rv_bin = rv_bin+`increment&#39;/2 // Center bins in the midpoint instead of left-end Plotting the First Stage Graph Now we create a two-way graph with linear fit confidence intervals (lfitci) and scatter points (scatter) twoway (lfitci nparties_seats rv if rv&lt;=0 &amp; rv&gt;=-0.025, lwidth(thick) /// fintensity(inten80) lcolor(gs6) alcolor(none) acolor(gs12*0.1) fcolor(gs14) /// alp(dash) ) (lfitci nparties_seats rv if rv&gt;0 &amp; rv&lt;=0.025 , lwidth(thick) /// fintensity(inten80) lcolor(gs6) alcolor(none) acolor(gs12*0.1) fcolor(gs14) /// alp(dash)) (scatter nparties_mean rv_bin, msymbol(O) mlcolor(gs11) /// mfcolor(gs15) msize(medlarge ) xlabel(-.025(.025).025, labsize(large))) /// (scatteri 3 0 4 0, recast(line) ) , ylabel(3(.5)4, labsize(large)) /// ytitle(N. of parties, size(large)) xtitle(Distance to threshold, /// size(large)) legend(off) graph export &quot;$dt/First_stage_025_linear.png&quot;, replace restore Explanation of twoway syntaxis and options: lwidth(thick): specifies the line width as thick. intensity(inten80): sets the intensity of the confidence interval to 80%. lcolor(gs6): sets the line color to grayscale code 6. alcolor(none): specifies no color for the area under the line. **acolor(gs12*0.1): sets the color of the confidence interval area to grayscale code 12 with 10% opacity. fcolor(gs14): sets the color of the fit line to grayscale code 14. alp(dash): specifies a dashed line for the fit. msymbol(O): specifies circular markers for the scatter plot. mlcolor(gs11): sets the line color of the markers to grayscale code 11. mfcolor(gs15): sets the fill color of the markers to grayscale code 15. msize(medlarge): sets the marker size to mediumlarge. xlabel(-.025(.025).025, labsize(large)): specifies custom x-axis labels. scatteri 3 0 4 0 specifies a vertical line segment between the points (0, 3) and (0, 4). recast(line): instructs Stata to recast the scatter plot as a line plot. graph export**: exports the graph in a specified folder and designated format. Figure 1: Bottom Panel For the second stage we repeat the same but for the reduced form, plotting directly the Probability of No confidence Votes against the the running variable (Distance to threshold). preserve local increment = 0.0025 egen rv_bin = cut(rv), at(-0.025(`increment&#39;)0.025) egen mocion_mean=mean(mocion_5) if mocion_5 != ., by(rv_bin) replace rv_bin = rv_bin+`increment&#39;/2 // Reduced form graph twoway /// (lfitci mocion_5 rv if rv&lt;=0 &amp; rv&gt;=-0.025, lwidth(thick) fintensity(inten80) /// lcolor(gs6) alcolor(none) acolor(gs12*0.1) fcolor(gs14) alp(dash) ) /// (lfitci mocion_5 rv if rv&gt;0 &amp; rv&lt;=0.025 , lwidth(thick) fintensity(inten80) /// lcolor(gs6) alcolor(none) acolor(gs12*0.1) fcolor(gs14) alp(dash)) /// (scatter mocion_mean rv_bin, msymbol(O) mlcolor(gs11) mfcolor(gs15) msize(medlarge ) /// xlabel(-.025(.025).025, labsize(large))) (scatteri 0 0 0.06 0, recast(line) ) /// , ylabel(0(.02).06, labsize(large)) ytitle(P(No-confidence vote), size(large)) /// xtitle(Distance to threshold, size(large)) legend(off) graph export &quot;$dt/RF_mocion_025_linear.png&quot;, replace restore 3.2.6 Main Results Now, we address how to perform the 2SLS estimation present the results on tables. Table 2 reports the estimates of the reduced-form and second-stage coefficients, respectively in Panel A and Panel B. Table 2. Panel A: RF and 2SLS estimates - P(no confidence vote) To run only this part, import the dataset again and keep only the observations from the Main Results sample: use &quot;$dt\\Dataset_CCR.dta&quot;, clear keep if Mainresults_sample == 1 Then, create globals for the regression to be able to easily add as controls the surface and population (in logs) of the municipality, include election year and council seat fixed effects, and include the running variable “rv” and its interaction with the indicator D “rv d”. global weights_on = 1 global controls &quot;log_pop log_surface&quot; global fixed_effects &quot;i.election_year i.seats_total&quot; global rv &quot;rv rv_d&quot; Additionally, if we want to run the regression with each observation weighted by the inverse of the number of parties running in the election, as the authors of the paper do, we store the global $weights_on = 1. If this is the case, the next command lines sorts the data by town and the election year and generates a new variable which counts the number of observations within each combination of id_town and election_year, and then creates the weights. if $weights_on == 1 { bys id_towns election_year: gen count = _N g weights = 1/nparties global weights_bw &quot;weights(weights)&quot; global weights_reg &quot;[aw = weights]&quot; } if $weights_on == 0 { global weights_bw &quot;&quot; global weights_reg &quot;&quot; } Panel A: Reduced-Form results Before running the regressions, it is important to establish a bandwidth, which will restrict the sample used based on the distance of the observations to the threshold. We use the command “rdbwselect” for bandwidth selection using the robust regression discontinuity design methods. The option c(0): indicates that the running variable has a discontinuity at zero; p(1): specifies the order of the polynomial used in the local polynomial regression; fuzzy(): indicates the fuzzy regression discontinuity design and specifies the variable to be used as the fuzzy instrument; vce(): specifies that the standard errors should be clustered at the level specified; masspoints(off): specifies that mass points are turned off in the estimation; covs($controls): indicates that additional covariates should be included in the regression. eststo clear rdbwselect mocion_5 rv, c(0) p(1) fuzzy(nparties_seats) vce(cluster id_towns) /// kernel(uni) $weights_bw masspoints(off) covs($controls) local CCT_bw_l = e(h_mserd) // stores the value of the estimated bandwidth Now, we run each regression of our outcome variable on the running variable, the indicator and their interaction, restricting the analysis to the observations where the absolute value of the running variable is less than the bandwidth that was previously computed and stored in “CCT bw l”. //Simple regression. eststo: reg mocion_5 $rv d $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; Additionally, “estadd scalar” is used to add a scalar to the estimation results, and it is assigned the value of the previously calculated and stored bandwidth. Then an scalar is also added with the value of the mean of the variable mocion 5. summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) The addition of controls and fixed effects results in the different columns of the main results, which are stored in a matrix using the “eststo” command. //For this column, we include controls, but not fixed effects. eststo: reg mocion_5 $rv d $controls $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) //For this column, we include fixed effects, but not controls. eststo: reg mocion_5 $rv d $fixed_effects $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, /// cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) //For this column, we include both controls and fixed effects. eststo: reg mocion_5 $rv d $controls $fixed_effects $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, /// cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) We create the first part of the table with the main results using “esttab” (see pag.7) esttab, se(3) nolabel b(3) sfmt(0) keep(d) nostar nonotes /// refcat(d &quot;A. Reduced-form results&quot;, nolabel) /// s(vmean band N, label(&quot;Mean of dep.var.&quot; &quot;Bandwidth&quot; &quot;Obs.&quot; ) /// fmt(3 3 0)) coeflabels(d &quot;Above threshold&quot;) varwidth(35) /// title(&quot;Table 2. Reduced-form and 2SLS Estimates - Fragmentation and Stability&quot;) /// mtitles(&quot;Mayor uns.&quot; &quot;Mayor uns.&quot; &quot;Mayors uns.&quot; &quot;Mayor uns.&quot;) Explanation of the esttab syntax: se(3): displays standard errors instead of t-statistics with 3 decimal points. nolabel: to not use labels instead of variables names. b: displays point estimates. sfmt(): sets format(s) for scalars. keep: keep individual coefficients. nostar: suppress stars in the table footer. nonotes: suppress notes in the table footer. s: specifies the statistics or summary measures you want to include in the output. Table 2. Panel B: 2SLS results - Fragmentation and Stability Notes: Reduced-form and 2SLS estimates of the effect of the number of parties on the probability of unseating the mayor (equation 2). Sample restricted to elections in which the largest party receives less than 40 percent of valid votes. The dependent variable is an indicator taking value 1 if the mayor was unseated by a vote of no confidence during the legislature. Controls and FE are included as indicated in each column. Controls: surface and population (in logs). FE: number of available seats and election year fixed effects. The optimal bandwidth is calculated using the CCT method. Standard errors are clustered at the municipality level. Now, we replicate the different columns of Panel B, this time using an instrumental variable. As in Panel A, the bandwidth selection is done first with the “rdbwselect” command: eststo clear rdbwselect mocion_5 rv, c(0) p(1) fuzzy(nparties_seats) vce(cluster id_towns) /// kernel(uni) $weights_bw masspoints(off) covs($controls) local CCT_bw_l = e(h_mserd) Then, we estimate the second stage of the main specification with the help of the command “ivreg”. The instrument “d” for number of party seats in council is then specified in parenthesis (nparties seats = d). The estimation is also conditional on the running variable’s absolute value being less than the bandwidth that was previously computed and stored in “CCT bw l”. //Simple regression. eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, /// cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) Additional to the scalars with the bandwidth and the mean of the dependant variable, we use “estadd local” to add a local macro labeled as FE and controls with the sign “N” or “Y” to the estimation results. This to add to the output table to indicate whether that estimation contained Fixed Effects and/or controls. quietly estadd local FE &quot;N&quot; quietly estadd local controls &quot;N&quot; //For this column, we include controls, but not fixed effects. eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $controls $weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) quietly estadd local FE &quot;N&quot; quietly estadd local controls &quot;Y&quot; //For this column, we include fixed effects, but not controls. eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $fixed_effects $weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) partial($fixed_effects) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) quietly estadd local FE &quot;Y&quot; quietly estadd local controls &quot;N&quot; //For this column, we include both controls and fixed effects. eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $controls /// $fixed_effects $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) /// partial($fixed_effects) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) quietly estadd local FE &quot;Y&quot; quietly estadd local controls &quot;Y&quot; To replicate the Panel B of Table 3 we use a similar code to the one presented previously to recreate Panel A: esttab, se(3) nolabel b(3) sfmt(0) keep(nparties_seats) nostar nonotes /// refcat(nparties_seats &quot;B. 2SLS results&quot;, nolabel) s(vmean band N FE controls, /// label(&quot;Mean of dep.var.&quot; &quot;Bandwidth&quot; &quot;Obs.&quot; &quot;Fixed Effects&quot; &quot;Controls&quot;) fmt(3 3 0)) /// coeflabels(nparties_seats &quot;N. Parties&quot;) varwidth(35) /// title(&quot;Table 2. Reduced-form and 2SLS Estimates - Fragmentation and Stability&quot;) /// mtitles(&quot;Mayor uns.&quot; &quot;Mayor uns.&quot; &quot;Mayors uns.&quot; &quot;Mayor uns.&quot;) /// addnotes(&quot;Notes: Reduced-form and 2SLS estimates of the effect of the number of parties on the probability of unseating the mayor (equation 2). Sample restricted to elections in which the largest party receives less than 40 percent of valid votes. The dependent variable is an indicator taking value 1 if the mayor was unseated by a vote of no confidence during the legislature. Controls and FE are included as indicated in each column. Controls: surface and population (in logs). FE: number of available seats and election year fixed effects. The optimal bandwidth is calculated using the CCT method.Standard errors are clustered at the municipality level.&quot;) 3.2.7 Robustness Checks In this last section, we explain how to perform two standard robustness checks for the RDD methodology among the five proposed by the paper, this to assess the sensitivity of the results to changes in model specifications or changes in the definition of the sample. By doing so, we ensure that the specific choices made during the analysis do not unduly influence the conclusions. As stated in Table 3, first, we select the municipalities with 17 or more seats in council to verify that the fragmentation effect persists when concentrating solely on the set of compliers, in which the sample is restricted to municipality-election pairs where the 5 percent threshold is likely to be enforceable. (i.e. municipalities with 17 or more seats in the council). Additionally, we present the “global quadratic polynomial” to be able to capture possible nonlinearities in the conditional expectation of the outcome, although it requires us to rely on more observations that are far from the threshold. Table 3. Robustness Checks To run only this part, previously import the dataset again, keep only the observations from the Main Results sample and run again the globals and weights presented in page 14 of this document. A. Total Seats over 17 In the first case, we only keep the sample in which the number of Council seats is greater than 17 to check the robustness. We store both regression results and IV regression results to be shown in different columns (see page 6 for the esttab and eststo explanations). eststo clear preserve keep if seats_total&gt;=17 rdbwselect mocion_5 rv, c(0) p(1) fuzzy(nparties_seats) vce(cluster id_towns) /// kernel(uni) $weights_bw covs($controls) local CCT_bw_l = e(h_mserd) local band1 = round(`CCT_bw_l&#39;,0.001) eststo: reg nparties_seats d $rv $fixed_effects $controls $weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd local band `band1&#39; eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $fixed_effects $controls$weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) restore esttab, se(3) nolabel b(3) sfmt(0) keep(d nparties_seats) nostar nonotes model(35) /// refcat(d &quot;A. Large Councils Only (#seats &gt;= 17)&quot;, nolabel) s(band N, /// label(&quot;Bandwidth&quot; &quot;Obs.&quot;) fmt(3 0)) /// coeflabels(nparties_seats &quot;N. Parties&quot; d &quot;Above threshold&quot;) varwidth(36) /// title(&quot;Table 5. ROBUSTNESS CHECKS&quot;) /// mtitles(&quot;First-Stage&quot; &quot;2SLS Estimate (N. Parties)&quot;) /// addnotes(&quot;Notes: Column 1 shows the first-stage estimate of our instrument when estimating equation 3. Column 2 reports the associated 2SLS estimate of the effect of number of parties on stability obtained by estimating equation 2. Each panel corresponds to a different robustness check: A) estimates obtained restricting the sample to municipalities with 17 or more seats in the council. Standard errors clustered at the municipality level. Bandwidths obtained using the CCT method in all panels.&quot;) Table 3. Robustness Checks E. Global Quadratic Polynomial This check considers a global quadratic polynomial, where the estimates are obtained using a large 5% bandwidth and a quadratic polynomial in the running variable. To do this, first we generate two different variables which are the square of ”rv” (rv 2) and the square of ”rv” interacting with ”d” (rv d 2). Additionally, we define a global to store these values and use them in the regressions. Finally, we define and store a local macro with a 5% bandwidth. eststo clear preserve local CCT_bw_l = 0.05 gen rv_2=rv^2 gen rv_2_d=(rv^2)*d global rv_2 &quot;rv rv_d rv_2 rv_2_d&quot; local band5 = round(`CCT_bw_l&#39;,0.001) eststo: reg nparties_seats d $rv_2 $fixed_effects $controls $weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd local band `band5&#39; eststo: ivreg2 mocion_5 $rv_2 (nparties_seats = d) $fixed_effects /// $controls$weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) restore esttab, se(3) nolabel b(3) sfmt(0) keep(d nparties_seats) nostar nonotes model(35) /// refcat(d &quot;E. Global Quadratic Polynomial&quot;, nolabel) s(band N, /// label(&quot;Bandwidth&quot; &quot;Obs.&quot;) fmt(3 0)) /// coeflabels(nparties_seats &quot;N. Parties&quot; d&quot;Above threshold&quot;) varwidth(36) /// title(&quot;Table 5. ROBUSTNESS CHECKS&quot;) /// mtitles(&quot;First-Stage&quot; &quot;2SLS Estimate (N. Parties)&quot;) /// addnotes(&quot;Notes: Column 1 shows the first-stage estimate of our instrument when estimating equation 3. Column 2 reports the associated 2SLS estimate of the effect of number of parties on stability obtained by estimating equation 2. Each panel corresponds to a different robustness check: E) estimates obtained using a large 5% bandwidth and a quadratic polynomial in the running variable. Standard errors clustered at the municipality level. Bandwidths obtained using the CCT method in all panels.&quot;) Authors: Carolina Arboleda Lenis, Elif C¸anga, Mariana Navarro Torres, Martina Pugliese, students in the Master program in Development Economics and Sustainable Development (2023-2024), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2023 "],["differences-in-discontinuity-design.html", "Chapter 4 Differences in Discontinuity Design ", " Chapter 4 Differences in Discontinuity Design "],["equality-of-opportunity-and-human-capital-accumulation-motivational-effect-of-a-nationwide-scholarship-in-colombia.html", "4.1 Equality of opportunity and human capital accumulation: Motivational effect of a nationwide scholarship in Colombia", " 4.1 Equality of opportunity and human capital accumulation: Motivational effect of a nationwide scholarship in Colombia Rachid Laajaj, Andrés Moya, Fabio Sánchez, Equality of opportunity and human capital accumulation: Motivational effect of a nationwide scholarship in Colombia, Journal of Development Economics, Volume 154, 2022, 102754, ISSN 0304-3878, .https://doi.org/10.1016/j.jdeveco.2021.102754 Download Do-File corresponding to the explanations below Download Codebook Download Original Data (Data>Originals>SPP_ready.dta) [Link to the full original replication package paper from Open ICPSR] Highlights The research question of this paper is the following: How did the opportunity to receive a scholarship influence the performance of low-income students in the national exit and 9th grade exams, and their university enrollment rates? This replication exercise will deal with the quasi-experimental Regression Discontinuity Design (RDD) method, with an eligibility cutoff and a linear function (and its extension in the form of difference-in-discontinuities model). This methodology is applied in this context because the scholarship eligibility is based on an eligibility threshold, which allows the authors to compare similar students and thus, avoid the issue of selection on unobservable. The special feature of this study is the combination of the regression discontinuity design extension, with the quantile regression method. Throughout this replication exercise, we provide a detailed explanation of the original replication package. Through this exercise, you will learn many tricks on Stata: How to present your results in academic tables, by using the packages estout and outreg2 How to use bandwidths in a regression, here the optimal bandwidth from Calonico et al. (2014) using the rdrobust package How to run multiple loops within another loop How to use the grc1leg package to merge different graphs in only one figure, using an online package How to conduct regression discontinuity analysis using the package rddensity, and more specifically to test for manipulation (Cattaneo et al., 2018) 4.1.1 Before we start If you wish to replicate yourself, you will need to download the original dataset SPP_ready.dta from the original replication package. Then, follow these instructions: Set the directory where you have stored the dataset SPP_ready.dta and where you will store your datasets and results: cd &quot;put here your own working directory&quot; *Data Preparation use &quot;SPP_ready.dta&quot;, clear *keep only the variables we need for the replication keep ranking eligible_post eligible non_eligible post sisben sisben_eligible sisben_post sisben_eligible_post area_ciudades area_urbano icfes_padre_nivel1 icfes_padre_nivel2 icfes_padre_nivel3 icfes_madre_nivel1 icfes_madre_nivel2 icfes_madre_nivel3 edad sexo_sisben departamento_cod_* saber_rk_col area_sisben_dnp puntaje_sisben_dnp year *rename the variables rename area_ciudades cities rename area_urbano other_urban rename icfes_padre_nivel1 father_educ_prim rename icfes_padre_nivel2 father_educ_second rename icfes_padre_nivel3 father_educ_high rename icfes_madre_nivel1 mother_educ_prim rename icfes_madre_nivel2 mother_educ_second rename icfes_madre_nivel3 mother_educ_high rename edad age rename sexo_sisben sex rename area_sisben_dnp area_sisben rename puntaje_sisben_dnp score_sisben rename saber_rk_col saber_avg_school *change labels for the variable area_sisben label define new_labels 1 &quot;14 Main Cities&quot; 2 &quot;Urban Rest&quot; 3 &quot;Rural&quot; label values area_sisben new_labels *use a loop to rename several variables forval i = 1/33 { local oldvar &quot;departamento_cod_`i&#39;&quot; local newvar &quot;department_`i&#39;&quot; rename `oldvar&#39; `newvar&#39; } gen status_eligible= 1 if non_eligible == 0 &amp; post ==0 replace status_eligible= 2 if non_eligible == 1 &amp; post ==0 replace status_eligible = 3 if non_eligible == 0 &amp; post ==1 replace status_eligible = 4 if non_eligible ==1 &amp; post ==1 label define status_eligible_labels 1 &quot;Need-based Eligible 2013-2014&quot; 2 &quot;Non-Eligible 2013-2014&quot; 3 &quot;Need-based Eligible 2015&quot; 4 &quot;Non-Eligible 2015&quot; * Apply the labels to the categorical variable label values status_eligible non_eligible_post_labels drop non_eligible label variable score_sisben &quot;Score Sisbén DNP&quot; label variable area_sisben &quot;Area Sisbén DNP&quot; label variable father_educ_prim &quot;Father&#39;s primary education&quot; label variable father_educ_second &quot;Father&#39;s secondary education&quot; label variable father_educ_high &quot;Father&#39;s high school education&quot; label variable mother_educ_prim &quot;Mother&#39;s primary education&quot; label variable mother_educ_second &quot;Mother&#39;s secondary education&quot; label variable mother_educ_high &quot;Mother&#39;s high school education&quot; label variable sex &quot;Sex&quot; label variable cities &quot;14 Main Cities&quot; label variable other_urban &quot;Other cities&quot; label variable ranking &quot;Ranking Saber test&quot; label variable sisben &quot;Distance to the threshold&quot; label variable post &quot;Before or after SPP&quot; label variable eligible &quot;Eligibility&quot; label variable eligible_post &quot;Treated or not&quot; label variable status_eligible &quot;Student&#39;s status&quot; label variable saber_avg_school &quot;Saber average score of middle school&quot; save &quot;dataset_LMS.dta&quot;, replace // save the dataset created in a new file You will also have to install a few packages and set up a global listing the control variables used in the regressions. ** Necessary packages to run the code: ssc install estout, replace ssc install outreg2, replace ssc install rdrobust, replace net install grc1leg.pkg, replace ssc install rddensity, replace **Set a global for all the control variables used in regressions global controls cities other_urban father_educ_prim father_educ_second father_educ_high mother_educ_prim mother_educ_second mother_educ_high age sex department_1-department_32 saber_avg_school Now you can open the dataset: use &quot;dataset_LMS.dta&quot;, clear 4.1.2 Introduction In 2014, a scholarship named Ser Pilo Paga (SPP), meaning “Being a Good Student Pays Off”, was introduced in Colombia in order to allow meriting students coming from low- income households to go to high-quality universities by funding their entire undergraduate education and living expenses. The eligibility for this scholarship is based on two criteria. First, students must come from a household that scored below a certain cutoff on the Sisbén index, which is a socioeconomic index. Second, students must have scored above a given cutoff on the Saber 11, which is the national high school exit exam. Data on the Sisbén scores are available at the Department Nacional de Planeacion, and the information on the Saber 11 test scores is provided by the Instituto Colombiano para el Fomento de la Educación Superior. This paper analyzes the motivational effect of this scholarship on three dimensions: the students’ performances at the Saber 11, the enrollment rates in high-quality universities and the students’ performances at the Saber 9 to test for the ex-ante motivational effect. This replication paper exercise will be focused on the effect of the SPP on the students’ test scores at the Saber 11. The authors used a difference-in-discontinuities model, an extension of the Regression Discontinuity Design (RDD) approach. The latter consists in estimating a local average treatment effect around a given cutoff to compare individuals with similar characteristics and be able to attribute the difference in outcomes to the only effect of the treatment. The difference-in-discontinuities extension allows studying the differences around a discontinuity threshold as in a standard RDD, and at two different time periods. Here, the authors study the discontinuity around the need-based eligibility cutoff, comparing students just below and above the threshold, two and ten months after the introduction of SPP. Together with quantile regression, the authors are able to show a positive and significant effect of the SPP concentrated at the top of the distribution (from the 70th percentile), emphasizing the SPP’s contribution to a reduction in socioeconomic achievement gap between eligible and non-eligible students. Throughout a replication exercise of the article by Laajaj, Moya and Sánchez, the following aims at providing an explanation of the main insights of the Regression Discontinuity Design and Quantile Regression frameworks, as well as some methodological and code explanation to improve your Stata skills. 4.1.3 Main strategies explanation: Regression Discontinuity Design and Quantile Regressions 4.1.3.1 Regression Discontinuity Design The Regression Discontinuity Design is a method that leverages a natural experiment, utilizing a threshold or cutoff point to determine who receives treatment. The core idea is to compare the outcomes of individuals who receive treatment to those who do not by examining both sides of this threshold. The running variable X assigns observations to treatment given the cutoff value C. Here, the RDD takes the form of a standard eligibility cutoff with a linear function. Though, in other RDD settings, you might encounter spatial RDD with a border as threshold (Black, 1999) or Regression Kink Design, with a discontinuity showing a kink in the treatment variable (Card et al., 2015). The key identifying assumption here is that individuals situated just above or below the threshold share similarity in all aspects except for the treatment they are subjected to. This assumption enables researchers to attribute disparities in results solely to the treatment, sidestepping the issue of selection bias. To extrapolate the discontinuity, one needs to set up a window called bandwidths, containing the treated and control units. There are two main designs within RDD: the sharp design and the fuzzy design. In the sharp design, the treatment assignment is a deterministic function of the running variable X. Units are either treated or not, depending on whether they fall on one side or the other of the threshold. In the fuzzy design, the treatment assignment is not strictly determined by the threshold, there can be instances of imperfect compliance. The probability of receiving treatment might jump at the threshold. Some units may not receive treatment even if they were expected to, and vice versa. To identify the treatment effect at the discontinuity, one needs to specify the potential outcomes model as a flexible function of the running variable. The specification of the functions is empirically important, as it needs to be flexible enough to get the functional form of the relationship between the outcome variable and the running variable. In our paper, the authors are using a standard eligibility cutoff, in a sharp design, estimating the change in ranking of the national high school exit exam (Saber 11) after the introduction of a merit- and need-based scholarship in Columbia (SPP). The treatment is a specific cutoff on the Sisbén (the socioeconomic index used by the government to target subsidies and social programs): students coming from low-income households under the cutoff are eligible and those above are not eligible. The authors are using a linear function for the functional form of the relationship between Sisbén and the ranking. The Sisbén cutoff is based on the households’ geographic location: 57,21 for the 14 main cities 56,32 for other urban areas 40,75 for rural areas The running variable here is the distance between the student’s Sisbén score and the need-based eligibility cutoff. Students taken into account in the study are assumed to be similar, the only difference being the threshold and thus the possibility of receiving the scholarship or not ex-post. To isolate the confounding effect of other social programs established prior to SPP sharing the same eligibility cutoffs, the authors use the difference-in-discontinuities extension of the RDD. This method allows the authors to study, around the need-based eligibility cutoff, the motivational effect of SPP by comparing two time periods. The model was introduced by &lt;a href=” https://doi.org/10.1257/app.20150076 target=“_blank”&gt;Grembi et al. (2016). The difference-in-discontinuities estimate compares two cohorts: the 2015 cohort, composed of students who passed the Saber 11 exam in 2015, 10 months after the implementation of SPP, and the 2014 cohort that comprises students who passed the Saber 11 exam only two months after the implementation of SPP. The 2014 cohort is taken as a control group: since SPP was launched for the first time not long before their Saber 11 exam, their motivation and results cannot be influenced by the program itself. However, the program can be considered as a credible signal for 2015 students, hence the comparison in the motivational effect of SPP between eligible and non-eligible students (on both sides of the threshold), two months after SPP implementation (2014 students, post=0) and ten months after (2015 cohort, post=1). 4.1.3.2 Quantile Regression In addition to their RDD specification, the authors are testing for heterogeneous effects of the scholarship program on students with different test scores in Saber 11 using quantile regressions. While an OLS model is minimizing the sum of squared residuals, the quantile regression model minimizes the objective loss function (sum of absolute weighted deviations). Contrary to OLS, there is no explicit solution for this minimization. Thus, it should be done numerically via the simplex method (a method using iterations). This explains why, each time you run a quantile regression code, the Stata software will display several successive iterations in the “Results” window before displaying the final regression result (see image below). They regress the test scores (variable “ranking”) on the scholarship eligibility for students who passed the Saber 11 exam after the introduction of SPP (variables “eligible” and “eligible_post”) and the running variable (variable “sisben”) at the 25th , 50th, 75th and 90th percentiles. This method has the advantage of studying the effect of a treatment on the entire distribution of interest, not only on its conditional mean. In this paper, this method is crucial because the authors find no motivational average effect of the SPP on the Saber 11 test scores, while they find a positive effect starting from the 75th percentile. These results can be explained by the level of the merit-based cutoff. Indeed, as this ranking cutoff is away from the median, students at the bottom of the distribution will feel discouraged and give up as it would require too much effort to try reaching it. 4.1.4 Descriptive statistics - Replication of Table 1 and Figure 4 4.1.4.1 Main insights of descriptive statistics It is essential to run some descriptive statistics at every stage of a research project. Descriptive statistics play a crucial role in an econometric regression framework by providing a comprehensive summary of the data under consideration. By depicting the key insights and trends of the variables of interest, they serve to demonstrate the motivation behind the analysis, and provide justification for the chosen empirical strategy. Good descriptive statistics should also highlight the main results, ensuring the alignment of the research in the intended direction. Descriptive statistics also serve as an initial step in exploring the dataset, verifying the coherence of values and identifying any potential missing data issues. The command summarize in Stata shows you directly the mean, standard deviation, minimum, and maximum of the desired variables. However, descriptive statistics can also take the form of a graph depicting the correlation between two variables of interest, a figure, a map or even non-causal regressions. Nevertheless, whatever the form it adopts, the primary focus should remain on data visualization: descriptive statistics must be clear, and allow us to catch the main information very quickly. 4.1.4.2 Replication of Table 1 - Average rank of eligible and non-eligible students in the Saber 11 Replication of the means for average and quantiles scores using tabstat and the package estout The paper presents different sets of descriptive statistics. The focus is on replicating Table 1, which displays the average rank of eligible and non-eligible students in the Saber 11 test two months after the implementation of SPP (2013-2014 cohort) and ten months after (2015 cohort). It also details the rank of students at the 25th, 50th, 75th, and 90th percentiles within each group. The replication of Table 1 will take place in two steps: first, the replication of rows (1), (2), (4) and (5) of parts A and B of the table and then row (7) from part C. At the end of this part 1 of the replication exercise, you should have the following .txt output of Table 1, parts (A) and (B). The first part of Table 1 (rows (1), (2), (4) and (5) of parts A and B) displays the means for averages scores and for each quantile (25, 50, 75, 90). To replicate it, you will use the tabstat command with the “status_eligible” variable. With the tabstat command, you can select the desired statistics of the “ranking” variable with statistics, in this case the number of observations (N), the mean, the quantiles p25, p50, p75 and p90 (q and p90). To do so, you will first need to save it with the estpost command, with the name of your choice (here: A). The estpost, eststo and esttab commands are available in the estout package. ssc install estout eststo A: estpost tabstat ranking, by(status_eligible) statistics(N mean q p90) columns(statistics) tabstat displays summary statistics (here the number of observations, the mean and the 25th, 50th, 75th and the 90th quantiles). Then, you can export it in the desired format with esttab (here .txt) with the desired features: fmt(1) gives the desired number of decimal places, label gives the name of the labels in the output table, varwidth(30) gives the width of the column containing the variable names. esttab A using &quot;means.txt&quot;, cells(&quot;mean(fmt(1)) p25(fmt(1)) p50(fmt(1)) p75(fmt(1)) p90(fmt(1))&quot;) title(&quot;Table 1. Saber 11 rank of eligible and non-eligible students before and after the motivational effect&quot;) noobs nonumber nonote nostar label replace varwidth(30) Replication of the descriptive diff-in-diff in average and by quantile, using outreg2 To replicate the last part of table 1, row (7) of part C, which is the descriptive diff-in-diff in average then by percentile, you will do it in two steps: Step 1: Run a basic diff-in-diff regression of ranking on the variables “eligible_post”, “eligible” and “post” and export it in excel format reg ranking eligible_post eligible post, vce (robust) Once the regression is estimated, we can use the command outreg2 to report the regression outputs from Stata into Excel, Word, Latex or any other format. It gives you the type of presentation found in academic papers. To use it, you have to install the package outreg2 first. ssc install outreg2 outreg2 using &quot;descr_DiD_.xls&quot;, label /// ctitle(&quot;Average Rank&quot;) replace less (1) Step 2: Run the same regression but in quantiles, using a loop for each quantile, and then export it in an excel format (.xls) and append it to the basic diff-in-diff table you got in Step 1. foreach j in .25 .50 .75 .90 { qreg ranking eligible_post eligible post, q(`j&#39;) vce (robust) outreg2 using &quot;descr_DiD_.xls&quot;, /// ctitle(`j&#39;) append less (1) } At the end of this part 2) of the replication exercise, you should have the following .xls output of Table 1, part (C). The following results can be drawn from the table: First, non-eligible students have in general a better ranking than eligible, revealing the importance of targeting this subpopulation. (Part A and B of table 1). Then, there is a reduction of the gap between eligible and non-eligible students after the implementation of SPP (by calculating the difference between non-eligible’s rank and eligible’s rank). This suggests a potential positive motivational effect of the program on students’ results. With a simple difference-in-differences regression, the authors compute the significance of the gap reduction (part C of table 1). The gap reduction is much more important from the 75th percentile of the distribution, suggesting that even with an effect at the average, the impact of SPP is only significant for students ranked at the top of the distribution. Two main conclusions can be drawn from this table: It provides support for estimating a causal effect of SPP on motivation, by comparing eligible and non-eligible students. It highlights the interest of a quantile regression to study the potential heterogeneous treatment effect at different points of the distribution of ranking. 4.1.4.3 Replication of Figure 4 - Graphical representation of the Regression-in-Discontinuities estimators on average effect, and at each quantile Moreover, RDD is a special case where graphical representations as descriptive statistics are indispensable. Prior to any RDD estimation, it is imperative to conduct a graphical analysis to justify the credibility of the chosen strategy. The graphical representation should demonstrate a notable jump in the outcome and treatment assignment at the threshold, attributable to the treatment. For this purpose, one can plot the relationship between (i) treatment assignment and the running variable, between (ii) the outcome and running variable, (iii) between control variables and running variable, and (iv) the density of the running variable. The latter provides justification for the absence of manipulation of the running variable and is often done as a robustness check (see replication of Figure A2 below for more details). In what follows, you will replicate Figure 4, which plots the relationship between the running variable (Sisbén score, centered at the cutoff, variable “sisben”) and the outcome variable (ranking change, variable “ranking”). It provides a graphical representation of the RDD estimators on average effect, and at the 25th, 50th, 75th, 90th percentile. It shows evidence of the presence of a discontinuity at the threshold, and offers insights for a significant effect from the 75th percentile only. The code is quite imposing and can be discouraging. Let’s not get ourselves intimidated and break the code step by step. Step 1 - Data preparation: defining some scalars and creating bins The scalar x command stores a single numeric value into a scalar named x. Scalars are useful to hold numerical outcomes, constants, or intermediate results of computations without creating new variables. The command scalar drop _all deletes any scalar Stata has in memory, and can be used prior to any new computation. You will first use scalars to set the width of bins, as well as the value of the CERSUM optimal bandwidth (by Calonico et al. 2014) to 3.35. The optimal bandwidth restricts the regression to the subsample of students whose distance between the Sisbén score and the need-based eligibility cutoff is inferior to 3.35. Focus on bandwidths Bandwidths are employed to define a specific data window around the cutoff in order to compare eligible students to non-eligible ones. The authors apply the CERSUM (Coverage Error-Rate Optimal Bandwidths) optimal bandwidth suggested by Calonico et al. (2014). Coverage error is a type of non-sampling error that occurs when there is not a one-to-one correspondence between the target population and the sampling frame from which a sample is drawn. This can lead to violations of the Regression Discontinuity assumptions, and bias estimates calculated using survey data. CERSUM bandwidths are chosen to minimize the probability that the confidence interval for the regression discontinuities treatment effect does not contain the true effect. It thus allows for bias-corrected robust confidence intervals for the average treatment effect at the cutoff. In Stata, the computation of these CERSUM optimal bandwidths can be done using the package rdrobust introduced by Calonico et al. (2014). For more details about bandwidth calculation, you can look at the code in Section 4.1.5. In the context of a RDD graph, bins refer to intervals along the running variable in which observations are grouped for visualization or analysis. Each bin represents a range of values around the threshold. Here, bins of two different sizes are defined. scalar drop _all scalar bw_dif_cersum = 3.3502511 scalar small_binsize = .01 scalar large_binsize = .3 set seed 1984 The set seed command is used to set the seed for the random number generator, ensuring that Stata produces the same sequence of random numbers (here 1984) every time the program is run. Use the keep if command to keep only observations for which the absolute value of the running variable (distance between student’s Sisbén score and the need-based eligibility cutoff) is inferior to 3.35 (scalar bw_dif_cersum), that is keep only the need-based eligible. keep if abs(sisben)&lt;bw_dif_cersum Create a variable “pre” using the command gen, that is the opposite of the variable post. It is equal to 1 for students who passed the Saber 11 exam in 2015 before and didn’t benefit from the SPP. gen pre = (post == 0) You can then generate bins that will appear in the graph. In an RDD graph, observations are often grouped into bins based on their values on the assignment variable. The graph displays the average ranking in the Saber 11 for each bin. gen large_bins = round(sisben + large_binsize/2, large_binsize) - large_binsize/2 if abs(sisben)&lt;bw_dif_cersum gen small_bins = round(sisben,small_binsize) // Each small bin is set to 0.01 around the sisben score for each individual. Step 2 - Prepare the OLS (average) estimation depicted in the graph You will prepare in this section the main regression to be represented in the OLS graph, and the percentiles. Firstly, regress (using the command reg dependant independents, robust) the ranking in Saber score (outcome) according to control variables and passing the Saber exam after the implementation of SPP. You will then have to save the residuals of this regression, creating a variable resid_ols. The predict command always refers to the last regression Stata has in memory. You have to specify after the comma what you want to predict. For clearer and less heavy regressions, don’t forget to define a global with all your controls of interest. This avoids having to rewrite the full list of covariates in every regression. global controls cities other_urban father_educ_prim father_educ_second father_educ_high mother_educ_prim mother_educ_second mother_educ_high age sex department_1-department_32 saber_avg_school /*if not done yet*/ reg ranking ${controls} post, robust predict resid_ols, residuals The next step is to compute local percentiles. This is done in 2 stages: Stage 1: Compute means of the outcome variable and residuals Using a loop, compute means for the variables ranking and resid_ols, within subsets defined by the variables small_bins and large_bins, for two time periods (pre and post). It creates new variables for each combination of time period, variable, and subset. For examples: ranking_ols_pre is the mean of ranking within each small bin, before the introduction of SPP. ranking_ols_r_pre is the mean of the residuals (of the regression of ranking on post) within each small bin, before the introduction of SPP. ranking_ols_r_largeb_post is the mean of the residuals within each bin, before the introduction of SPP. foreach t in pre post { bysort small_bins: egen ranking_ols_`t&#39;= mean(ranking) if `t&#39;==1 bysort small_bins: egen ranking_ols_r_`t&#39;= mean(resid_ols) if `t&#39;==1 bysort large_bins: egen ranking_ols_r_largeb_`t&#39;= mean(resid_ols) if`t&#39;==1 } A loop repeats the same lines of code for different values of a specified variable, here for each value of pre and post. It allows you to save time, by avoiding copying and pasting the exact same code for different variables. Don’t forget to open the bracket at the end of the first line after specifying the variable to repeat the loop on, to return to line to write your code, and to return to another line to close the bracket. The command bysort tells Stata to apply an operation (here creating means) separately for each value taken by the variable small_bins. Please note that to compute means, you have to use the command egen and not gen. Stage 2: Adjust for the constant, so that the average is the same as the data You will calculate means for the variables computed in stage 1 (ranking_ols_r_pre, ranking_ols_r_post, ranking_ols_r_largeb_pre, ranking_ols_r_largeb_post), and then adjust the values of those variables based on the differences between the mean of the original variable (ranking) and the calculated means on residuals. You can notice here that you can include several loops within the first one: foreach t in pre post { foreach x in ranking_ols_r_`t&#39; ranking_ols_r_largeb_`t&#39; { quietly sum`x&#39;if `t&#39; == 1 //This quietly calculates the mean of each variable within each group for the specified time period. scalar m_`x&#39; = r(mean) //Store the previous mean in a scalar (m_`x&#39;) quietly sumranking if `t&#39; == 1 // This calculates the mean of ranking within each group for the specified time period. scalar ols_ranking = r(mean) // Store the previous mean in a scalar (ols_ranking) scalar dify = ols_ranking - m_`x&#39;// This calculates the difference between the mean of the original ranking variable and the mean of the residuals variables. replace `x&#39; = `x&#39; + dify if `t&#39; == 1 // Replace the values of the variables by their adjusted values based on the calculated difference } } Step 3 - Prepare the quantile estimations depicted in the graph With a loop , you can run the same code as in the OLS estimation, on each quantile (foreach q in 25 50 75 90) ; that is regress the ranking in Saber score according to controls and passing the Saber exam after the implementation of SPP ; calculation of local percentiles ; and adjust for the constant. You have to be careful though, when running quantile regression, that Stata is able to compute enough iterations. In quantile regression, the estimation procedure follows a numerical minimization procedure through the simplex method: the algorithm repeats computations to find parameter estimates minimizing the objective loss function. To make sure Stata will compute enough iterations, we suggest you to set the maximum number of iterations to 1000 with the command set maxiter 1000. Step 4 - Collapse data to keep only one observation per bin It is frequent using quantiles that multiple observations take the same value. Instead of counting them as one observation, each of the observations is assigned a weight. For example, if you have three observations taking the same value, counting them as one observation and assigning them a weight of 3 reflects the fact that these three observations contribute to the percentile calculation. One will be used for counting and then weight based on the number of observations used to calculate the percentile. First of all, for predictions, you need to replace each control by its average value: foreach x in cities other_urban father_educ_prim father_educ_second father_educ_high mother_educ_prim mother_educ_second mother_educ_high age sex saber_rk_col department_1 department_2 department_3 department_4 department_5 department_6 department_7 department_8 department_9 department_10 department_11 department_12 department_13 department_14 department_15 department_16 department_17 department_18 department_19 department_20 department_21 department_22 department_23 department_24 department_25 department_26 department_27 department_28 department_29 department_30 department_31 department_32 { quietly sum`x&#39;//This quietly calculates the mean of each control scalar aux = r(mean) //Store the previous results in the scalar &quot;aux&quot;. quietly replace`x&#39; = aux //Replace each control variable by its mean value. } Then you will create binary indicator variables (“one_pre” and “one_post”) that take the value 1 for observations corresponding to the specified time periods (pre or post, that is if t==1) and 0 for all other observations. foreach t in pre post { gen one_`t&#39; = 1 if `t&#39; == 1 } Finally, collapse by bins, in order to keep only one observation per bin, which makes you avoid artificial significance through repetition of observations: collapse ranking_*sisben large_bins eligible_post eligible post sisben_eligible sisben_post sisben_eligible_post ${controls} (count) one_pre one_post, by(small_bins) You should end up with only 675 observations, one for each unique value of small bins. For each of the residuals variables, you then have to generate a variable equal to the difference between their pre and post value. We do so in the OLS average and in the quantile estimations using a loop. This is the code for the OLS loop: foreach x in ranking_ols_r_ ranking_ols_r_largeb_ { gen `x&#39;dif = `x&#39;post - `x&#39;pre } Don’t forget to save the database you just created in a new file. It is important to never delete the original database. You rather want to save your modifications in a separate file. save&quot;bybeans_pre_post_dif.dta&quot;, replace Step 5 - Construct the OLS average graph and its confidence intervals We are getting closer to creating the graph! Once again, let’s decompose into several stages. By the end of these stages, you should have the following .png output of Figure 4: Stage 1: Main regression to display in the graph Use the original dataset to run this regression. reg ranking eligible_post eligible post sisben sisben_eligible sisben_post sisben_eligible_post ${controls} if abs(sisben)&lt;bw_dif_cersum, ro Stage 2: Data preparation for the confidence intervals Here, use the second dataset you created (the one processed by bins). Interacting a variable with post restricts the estimation on the sample of students who passed the Saber 11 exam in 2015, 10 months after the implementation of SPP. The regression in discontinuities design compares treated and untreated students (on both sides of the threshold) before (post=0, 2014 cohort) and after the implementation of SPP (post=1, 2015 cohort). However, in order to represent the discontinuity effect graphically and construct confidence intervals, you have to get rid of this temporal dimension. The setting shifts closely to a standard RDD representation that solely depicts the relation between the running variable determining the eligibility to SPP and the output variable (ranking), independently of the year students passed their Saber 11 exam and benefited from SPP. To do so, the authors suppress the interaction with the post variable. They replace each variable that is interacted with post, by the corresponding simple non-interacted variable. For instance, they attribute the values of the eligible variable to the eligible_post variable. The variable of interest (initially eligible_post - that is whether students are eligible to the scholarship and pass the Saber 11 exam in 2015) is translated to simply being eligible to the scholarship. It now estimates the effect of being eligible to SPP on the ranking in the Saber 11 exam, independently from whether students benefited from SPP two or ten months before writing the Saber 11 exam. replace eligible_post = eligible replace sisben_post = sisben replace sisben_eligible_post = sisben_eligible post, eligible, sisben, and sisben_eligible are now present twice in the dataset, once under their original name, and a second time under their post interaction name (that you just modified the value). You now want to set the values of the original variables to 0 to avoid multicollinearity in the regression. foreach x in post eligible sisben sisben_eligible { replace`x&#39; = 0 } Stage 3: Predicted values of the main regression Using the predict command, you will calculate linear predicted values from the regression above (creating a new variable yh_ols); as well as standard errors of the prediction xb (saving them in a new variable y_stdp_ols) predict yh_ols, xb predict y_stdp_ols, stdp Stage 4: Adjustment for the constant You want here to adjust the values yh_ols variable, so that the average is the same as the q'th percentile. To do so, you need to add to each observation of the *yh_ols* variable the calculated average difference between mean of *ranking_ols_r_dif* and mean of *yh_ols.* Note that with many commands, you can also use thequietly` command when you don’t want Stata to show the results. quietly sum yh_ols // Calculate mean for the variable yh_ols scalar m_yh_ols = r(mean) // Store the previous results in a scaler (m_yh_ols) quietly sum ranking_ols_r_dif // Calculate mean for the variable ranking_ols_r_dif scalar ols_ranking_dif = r(mean) // Store the previous results in a scaler (ols_ranking_dif) scalar dify = ols_ranking_dif - m_yh_ols // Calculate the difference between the mean of ranking_ols_r_dif and the mean of yh_ols and store it in the scalar (dify) replace yh_ols = yh_ols + dify // Adding the calculated difference to each observation Stage 5: Compute confidence intervals Create here two new variables for the lower and upper bound of the confidence intervals, taking missing values for each observation, and then attribute values to the CI bounds, to estimate confidence intervals at the 95% level. For each small and large bin, the value of the higher bound of the confidence interval is equal to: (the coefficient estimates - 1.96 * the standard error estimated). gen ci_h_ols=. gen ci_l_ols=. replace ci_h_ols = yh_ols+1.96*y_stdp_ols replace ci_l_ols=yh_ols-1.96*y_stdp_ols Stage 6: Create the graph! One particularity of this RDD design is the linear fit on each side of the threshold. Within this framework, you want to attribute the values of the variable small_bins to sisben. replace sisben = small_bins Using the twoway command, you will create a scatter plot, displaying the relationship between ranking in the Saber 11 and the Sisbén score at the cutoff. In the command scatter (and within the same brackets) you can specify options after a comma to define the symbol to employ and its color (msymbol and mcolor). You can also add lines in your scatter plot, using the command line, and specify options after a comma to define the style, pattern and color of the line (pstyle, lpattern and lcolor). For instance, here, we add a line plot of ci_l_ols against small_bins for cases where sisben is inferior to 0. The line has point-style (p3 p3), a dashed line pattern, is sorted, and has green color. Once you have indicated all elements to draw in the twoway graph, add a comma and write the options of the graph: its title, y and x-axis titles, legend settings, and design options (style and color of the graph region, background color of the graph). twoway(scatter ranking_ols_r_largeb_dif large_binsif large_bins == small_bins, msymbol(O) mcolor(gray)) (line yh_ols small_bins if sisben &lt;0, pstyle(p) sort lcolor(blue)) (line ci_l_ols small_bins if sisben &lt;0, pstyle(p3 p3) lpattern(dash) sort lcolor(green)) (line ci_h_ols small_bins if sisben &lt;0, pstyle(p3 p3) lpattern(dash) sort lcolor(green)) (line yh_ols small_bins if sisben &gt;0, pstyle (p) sort lcolor(blue)) (line ci_l_ols small_bins if sisben &gt;0, pstyle (p3 p3) lpattern(dash) sort lcolor(green)) (line ci_h_ols small_bins if sisben &gt;0, pstyle (p3 p3) lpattern(dash) sort lcolor(green)), ytitle(&quot;Ranking change in average&quot; &quot; &quot;) xtitle(&quot;Eligible Not Eligible&quot; &quot; &quot; &quot;{it:Sisbén} score (centered at cutoff)&quot;) legend(label(1 &quot;Change by bin&quot;) label(2 &quot;Dif in RD linear prediction&quot;) label(3 &quot;95% CI of linear prediction&quot;) order(2 1 3)) title(Average Effect) graphregion(style(none) color(gs16)) bgcolor(white) xline(0, lcolor(red)) name(DifRD_ols, replace) Step 7: Export the graph into the format of your choice, here .png. You just replicated one of the elements of Figure 4! graph export &quot;DIf_RD_Fig_Lin_ols.png&quot;, replace Step 6 - Construct the quantile graphs and their confidence intervals We redo the same procedure as in the OLS estimation (steps 1 to 7), with a loop to get one graph for each quantile (download our do-file in section Highlights for details). At this stage of the replication exercise, you should have four .png outputs of Figure 4, one for each quantile (as the output below for the 25th percentile). Step 7 - Merge the five graphs into one figure using the grc1leg package To be able to use the grc1leg command, you probably have to install the package. However, the package is not provided by the Sata package library. You have to import it directly from its author, Vince Wiggins (2010). net install grc1leg, from (http://www.stata.com/users/vwiggins) You will first merge the three first graphs on a single-row graph named row1, then the two remaining ones on another single-row graph named row2. We finally merge row1 and row2 in a single one column graph. grc1leg DifRD_ols DifRD_25 DifRD_50, rows(1) name(row_1, replace) graphregion(color(white)) grc1leg DifRD_75 DifRD_90, rows(1) name(row_2, replace) graphregion(color(white)) grc1leg row_1 row_2, cols(1) graphregion(color(white)) graph export&quot;figure_4.png&quot;, replace At the end of this part of this replication exercise, you should have the following .png output of Figure 4: 4.1.5 Main results - Replication of Table 3 In their main results, the model they estimate is the following (based on Grembi et al, 2016): \\[\\begin{equation} \\tag{1} Ranking_{it} = β_{0} + β_{1} eligible\\_post_{it} + β_{2} eligible_{i} + β_{3} sisben_{i} + β_{4} sisben\\_eligible_{i} + β_{5} sisben\\_post_{it} + β_{6} sisben\\_eligible\\_post_{it} + X_{it} α + ϵ_{it} \\end{equation}\\] where \\(Ranking_{it}\\) is the dependent variable, the rank (from 0 to 100) on the Saber 11 exam of student \\(i\\) who passed the test in year \\(t\\), \\(β_{1}\\) is the difference-in-discontinuities estimate (our coefficient of interest, the change in discontinuity in test scores before and after the introduction of SPP), and \\(sisben_{i}\\) is the running variable. Table 3 summarizes the main results of the paper from the above equation. The first column reports the results estimated with the regression-in-discontinuities specification (mean effect) while the columns 3 to 5 report the results obtained with quantile regressions. At the end of this part of this replication exercise, you should have the following .xls output of Table 3: Step 1 - Bandwidths calculation Before running this regression, it is necessary to define a bandwidth around the cutoff to compare eligible students to non-eligible ones. Focus on bandwidths - The authors apply the CERSUM (Coverage Error-Rate Optimal Bandwidths) optimal bandwidth suggested by Calonico et al. (2014). Coverage error is a type of non-sampling error that occurs when there is not a one-to-one correspondence between the target population and the sampling frame from which a sample is drawn. This can lead to violations of the Regression Discontinuity assumptions, and bias estimates calculated using survey data. CERSUM bandwidths are chosen to minimize the probability that the confidence interval for the regression discontinuities treatment effect does not contain the true effect. It thus allows for bias-corrected robust confidence intervals for the average treatment effect at the cutoff. In our case, the optimal bandwidth restricts the regression to the subsample of students whose distance between the Sisbén score and the need-based eligibility cutoff is inferior to 3.35. In Stata, the computation of these CERSUM optimal bandwidths can be done using the package rdrobust introduced by Calonico et al.(2014). First you will have to install the package rdrobust. ssc install rdrobust Then, to get the CERSUM optimal bandwidths, you will: Calculate it for pre-period (year 2013 and 2014, post=0) with the regression discontinuity analysis command rdrobust, using the uniform kernel function: *pre cersum: rdrobust ranking sisben if post == 0, kernel(uniform) bwselect(cersum) Create a scalar variable and assign it to the value of the estimated bandwidth stored in e(h_l) scalar bw_pre_cersum = e(h_l) Do the same for the post-period (post=1) *post cersum: rdrobust ranking sisben if post == 1, kernel(uniform) bwselect(cersum) scalar bw_post_cersum = e(h_l) Create a scalar variable and assign it to the mean of both above calculated bandwidths. scalar bw_dif_cersum = (bw_pre_cersum +bw_post_cersum)/2 To see the resulting bandwidths estimation, use scalar list _all Once you have done this computation a first time and obtained the optimal bandwidth value (3.35 in our case), you can simply store this value in a scalar for simplicity. This avoids you redoing the complete computation every time you want to use it. scalar bw_dif_cersum = 3.3502511 Then, set a global for the control variables required in this regression. global controls cities other_urban father_educ_prim father_educ_second father_educ_high mother_educ_prim mother_educ_second mother_educ_high age sex department_1-department_32 saber_rk_col Step 2 - Regression at the eligibility cutoff and saving the results with outreg2 command You will regress, at the eligibility cutoff, the ranking in the Saber 11 (variable ranking) on variables for whether students are eligible to the scholarship and pass the Saber 11 exam in 2015 (variable eligible_post), on the running variable sisben, as well as on control variables. The coefficient of interest (variable eligible_post) gives the change in discontinuity in test scores before and after the introduction of SPP. To do so, you will use the command reg and add the condition which states that the absolute value of the variable sisben must be inferior to the optimal bandwidth, as explained above. reg ranking eligible_post eligible post sisben sisben_eligible sisben_post sisben_eligible_post ${controls} if abs(sisben)&lt;bw_dif_cersum, robust Once the regression is estimated, use the command outreg2 to report the regression outputs from Stata into Excel, Word, Latex or any other format. It gives you the type of presentation found in academic papers. To use it, you have to install the package outreg2 first. ssc install outreg2 outreg2 using &quot;DIF_RD_main.xls&quot;, addtext(Quantiles,&quot;OLS Dif RD&quot;) ctitle(&quot;LATE&quot;) less(1) keep(eligible_post) nocons replace The option less() specifies how many less significant digits to be displayed for the auxiliary (non-coefficient) statistics. Step 3 - Quantile regressions with qreg command For the quantile regressions, use the command qreg to run the regression, as before, the dependent ranking on variables of interest, the running variable, and on control variables. Though, in this case, the regression is performed with a loop foreach to repeat it for each quantile. Once again, you will have to specify the condition of eligibility and use a robust estimator of variance vce(r). To display the results for each quantile regression in an academic table, you can once again use the outreg2 command in the loop. However, before running the regression, you will have to set the maximum number of iterations to 1,000. Indeed, in quantile regression, the estimation procedure follows a numerical minimization procedure through the simplex method: the algorithm repeats computations (here set at 1000 repetitions) to find parameter estimates minimizing the loss objective function. set maxiter 1000 foreach q in 25 50 75 90 { qreg ranking eligible_post eligible post sisben sisben_eligible sisben_post sisben_eligible_post ${controls}if abs(sisben)&lt;bw_dif_cersum , q(`q&#39;) vce(robust) outreg2 using &quot;DIF_RD_main.xls&quot;, addtext(Quantiles,&quot;`q&#39; Dif RD&quot;) ctitle(&quot;`q&#39;&quot;) less(1) keep(eligible_post) nocons append } How to read Table 3? Table 3 above shows that the introduction of the scholarship Ser Pilo Paga has non-significant effect on the Saber 11 test scores at the mean, as well as at the 25th and 50th percentiles. However, looking at heterogeneity in the ranking, there are positive and statistically significant effects for the 75th and 90th percentiles, respectively. In other words, a significant gap reduction in test scores between eligible and non-eligible students can be observed at the top of the distribution of the ranking in test scores. The effect of the scholarship on the Saber 11 ranking is around 1.5 rank for eligible students. These findings are in line with the Figure 4 replicated just above. 4.1.6 Robustness check - Replication of Figure A2 An important and interesting robustness check to conduct in RDD is to study the density of the running variable. This allows checking whether there are too many observations on one side of the cut-off compared to the other side. In other words, you want to verify whether individuals have manipulated the running variable in order to benefit from the program provided by the policy in place. McCrary (2008) was the first to introduce the idea of manipulation testing in the context of regression discontinuity (RD) designs. In our case, the authors want to demonstrate that the Sisbén score has not been manipulated around the eligibility cutoff. To this end, they will test for discontinuity in observation densities around the cutoff. A cluster below the cutoff would suggest that households have manipulated their Sisbén score in order to qualify for the program, creating a selection effect and posing a serious problem in terms of unobservable differences between students on either side of the cutoff. This would make for a biased comparison between the two groups. To test for that, you will replicate Figure A2 below. It should show no significant discontinuity in the Sisbén score density, when examining the three types of geographical areas taken into account by the Colombian statistics office: the 14 main cities, the other urban areas and the rural areas (each with a different cutoff). 4.1.6.1 Test for manipulation around the eligibility cutoff using the rddensity package To test for manipulation around the eligibility cutoff, the authors are using the local polynomial density estimation technique introduced by Cattaneo et al. (2017). To start, you will first have to find the p-value using the Cattaneo et al.’s method. For that, you will need to install the package rddensity. If the p-value is lower than 0.05, it suggests that there has been some manipulation. ssc install rddensity Since the year of interest for manipulation is 2015, set keep if year == 2015. Then, run a regression discontinuity analysis, specifying a first-order polynomial (p(1)) around the cutoff point in the sisben variable for each of the sisben area. For example, to take into account the 14 largest Colombian cities, you have to set the variable area_sisben=1. rddensity sisben if area_sisben==1, p(1) You should see that these results are robust for the three types of geographical areas taken into account by the Colombian statistics office: the 14 main cities, the other urban areas and the rural areas (each with a different cutoff). p-value for 14 main cities: 0.3183 p-value for other urban areas: 0.7747 p-value for rural areas: 0.1285 Indeed, all the three p-values are higher than 0.05. Compared to the McCrary density test (2008) usually used so far, the technique of Cattaneo et al. (2017) avoids pre-binning the data and is constructed intuitively based on easy-to-interpret kernel functions. 4.1.6.2 Replication of the chart for the 14 major cities, then for urban and rural using twoway To plot the discontinuity, use the command twoway, which is a basic command for graphs fitting Y on X and use the histogram function for the variable score_sisben. At this step, you may: - Choose the color of your histogram (here black (fcolor(black)) with gray outlines (bcolor(gray))), - Place a red line to place the cutoff of the 14 main Colombian cities using the command scatteri, - Add the p-value you got in the part A for each type of area (here for the 14 main cities 0.3183) at the lop left of the graph, - Give the respective names Sisbén - Socioeconomic Index and Density to the X and Y axes and finally, - Export the graph as .png file format ```{.Stata language=“Stata” numbers=“none”} Replicate first graph of Figure A1 twoway(histogram score_sisben if area_sisben==1, fcolor(black) bcolor(gray)) /// (scatteri 0 56.32 0.025 56.32 (9), c(l) m(i) color(red)), /// plots a single red line at coordinates (0.025, 56.32) text(0.025 57.21 “14 Cities”, place(e) size(medium)) /// adds the text for the line text(0.025 0 “P-Val=.3183”, place(e) size(medium)) /// adds the p-value at the top left of the graph legend(off) xtitle(“{it:Sisbén} - Socioeconomic Index”) ylabel(0(.01).025) /// no legend, name of the var on the x- and y-axis ytitle(“Density”) graphregion(style(none) color(gs16)) name(“histogram_14cities”, replace) // title graph export “histogram_14cities.png”, replace // Exports the graph as a PNG file named “histogram_14cities1.png” to the directory specified #### Combination and exportation of the three graphs together ```{.Stata language=&quot;Stata&quot; numbers=&quot;none&quot;} graph combine histogram_14cities histogram_urban histogram_rural, /// rows(3) graphregion(style(none) color(gs16)) /// imargin(medsmall) xcommon ycommon graph export&quot;figure_A2.png&quot;, replace Authors: Angélique FANTOU, Agathe LOYER, Emma VERHILLE, students in the Master program in Development Economics and Sustainable Development (2023-2024), Sorbonne School of Economics, Université Paris 1 Panthéon Sorbonne. Date: December 2023 "]]
