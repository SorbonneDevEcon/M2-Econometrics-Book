[["index.html", "M2 Econometrics Book Chapter 1 Introduction", " M2 Econometrics Book This site is currently under construction. 2024-03-14 Chapter 1 Introduction "],["randomized-control-trials.html", "Chapter 2 Randomized Control Trials ", " Chapter 2 Randomized Control Trials "],["subsidies-and-the-african-green-revolution-direct-effects-and-social-network-spillovers-of-randomized-input-subsidies-in-mozambique.html", "2.1 Subsidies and the African Green Revolution: Direct effects and social network spillovers of randomized input subsidies in Mozambique", " 2.1 Subsidies and the African Green Revolution: Direct effects and social network spillovers of randomized input subsidies in Mozambique Carter, M., Laajaj, R., &amp; Yang, D. (2021). Subsidies and the African Green Revolution: Direct effects and social network spillovers of randomized input subsidies in Mozambique. American Economic Journal: Applied Economics, 13(2), 206‑229. https://doi.org/10.1257/app.20190396 Highlights The paper investigates the short-term and long-term effects, including diffusion through farmers’ networks, of a one-off input subsidy program on agricultural yields over time? The authors conduct a standard Randomized Controlled Trial (RCT) in which individuals are randomly assigned to the treatment and control groups, eliminating potential selection bias. Yet, it departs from the standard by exploring spillover effects through social networks, assessing long-term impacts, and addressing informational market failures. The study emphasizes learning processes as part of the intervention’s impact, extending beyond the typical scope of a standard RCT. This document provides a detailed explanation of the original replication package. In addition to the replication of the original outputs, this work will introduce you to two practical techniques which you may find useful for your future Stata projects. You will learn (1) how to automatically generate a new label by keeping / removing certain parts of an existing label with the subinstr command and (2) how to use a local macro together with replace and append commands in order to loop over a list of variables and store all their corresponding results into a fresh table every time the code is run. More background on the study: This study examines the impact of a one-off input subsidy program implemented in Mozambique in 2010 within the context of the Green Revolution and the Alliance for a Green Revolution in Africa. While the Green Revolution transformed Asian and Latin American agriculture, Sub-Saharan Africa lagged behind. In response, the Maputo Declaration (2003) committed African nations to invest 10% of their budgets in agriculture. Launched in 2006, the Alliance aimed to catalyze the Green Revolution in Africa through input subsidy programs (ISPs), providing technologies at discounted rates to randomly selected farmers. Focusing on Mozambique’s 2010 Programa de Suporte aos Produtores, this paper not only assesses subsidy impacts on 704 farmers but also explores post-subsidy persistence and spillover effects through social networks. Findings reveal increased technology adoption, sustained maize yield growth post-program, and notable impacts on farmers’ networks. The program also fostered enhanced beliefs about technology returns, mitigating information-related market failures. Link to the article: https://www.aeaweb.org/articles?id=10.1257/app.20190396 Link to the original replication package: Data and Code for Subsidies and the African Green Revolution: Direct Effects and Social Network Spillovers of Randomized Input Subsidies in Mozambique (openicpsr.org) 2.1.1 TABLE 2 - Regressions with spillover effects 2.1.1.1 Final output for Table 2 Table 2 examines the impact of the government-implemented input subsidy program (ISP) on various agricultural outcomes, including fertilizer use on maize, adoption of improved maize seeds, maize yield, daily consumption per capita, and expected yield with the technology package. The objective of this analysis is to determine whether the ISP has a positive and statistically significant impact on the use of fertilizer on maize and on maize yields, measured as average maize yield per hectare. The first two lines of this table present the program’s results estimated on treated farmers (those who received the subsidy). The effects of the program are assessed both during and after the ISP implementation. This table reveals that the program has a positive impact on all farmers’ outcomes, both during and after the program. The effects are more pronounced during the treatment period for fertilizer use, improved maize seeds, and maize yields. The next lines present the impact of the ISP on social network spillovers, which is estimated on members of treated farmers’ social networks during (round 2) and after (rounds 3 and 4) the treatment period, respectively. The general pattern observed is that the coefficients for social network variables are positive and significant in the post-intervention period but not in the intervention period. The magnitude of the coefficients increases as the number of social network contacts in the treatment group increases from one to two, and then stabilizes. This pattern suggests that the effect of social networks on technology adoption and yield increases sharply at two or more social network contacts in the treatment group. For example, after the program, having one contact in the treatment group increases maize yield by 0.18 kilogram per hectare, while having two contacts increases maize yield by 0.53 kilogram per hectare. This substantial increase in yield with two or more social network contacts justifies the authors’ estimation of spillover effects on individuals having above-median (two or more) social network members in the treatment group in Figure 2. 2.1.1.2 Replication code for Table 2 Data importation To start the replication of the authors’ code, one should always start by deleting any potential data in memory using clear all. Then, open the dataset of interest using global base, which indicates the path leading to the file you want to store this project in, and the command use “…”, clear to indicate the dataset you want to import in Stata. Because this project uses panel data, you need to inform Stata of the structure of the data. Use the command xtset to indicate that you are working with panel data, followed by the name of the variable you want to set as the panel variable. Here, vlgid_round is an identifier for every possible combination of locality and round. clear all global base &quot;PUT YOUR OWN WORKING DIRECTORY HERE (and store the data under this folder)&quot; use &quot;Dataset_CLY.dta&quot;, clear xtset vlgid_round global sn_treatments sn1_sub_dur sn2_sub_dur sn3_sub_dur sn4_sub_dur sn5up_sub_dur /// sn1_sub_aft sn2_sub_aft sn3_sub_aft sn4_sub_aft sn5up_sub_aft Now, you want to modify the labels for a set of variables that were in log format in the original dataset, to indicate that they are in their original scale. Start with creating a loop to iterate the same procedure over a set of variables, using the foreach command. Stata trick: Then, you can easily modify the labels using variable label to extract the label for each variable in the loop, remove the text “log” from the label using subinstr, remove the excess spaces using trim, and update the variable label with the modified text. foreach v in fertmaizr improvedseedsr yieldr dailyconsr exp_yield_fertr { local x : variable label l`v&#39; local x = trim(subinstr(&quot;`x&#39;&quot;,&quot;(log)&quot;,&quot;&quot;,.)) label variable l`v&#39; &quot;`x&#39;&quot; } 2.1.1.2.1 Regression and table creation You are now set to running the regressions and creating a table presenting results. Stata trick: Start with initializing a local macro using local, named rep_app, and set its value to replace. The purpose of this macro is to control whether the replace option is used when exporting regression results in an Excel file using the command outreg2. In other words, it means that if the file already exists in your computer, it will be replaced. local rep_app = &quot;replace&quot; Again, use a loop to iterate regressions over the same list of variables using foreach. You want to apply several commands to the variables included in the loop only if they meet the following conditions: round==2 (survey round 2, that is, during the program) and vouch==0 (farmers who did not win the voucher, that is, the control group). Then: qui sum calculates the mean, and results are stored using the local command areg runs a fixed effects regression on panel data, using absorb to absorb fixed effects, and cluster to take into account the intra-cluster correlations outreg2 creates the regression table and saves it into an Excel file (.xls extension) Then, create a nice table using key commands: bracket generates brackets for standard errors label includes variable labels in the results table to display full variable names nocons, nor2, and noni are used to avoid displaying the constant, the R-squared and missing values in the table less(1) means that only the first row of the result table will be included (only the first regression) keep allows you to keep only the variables you want adds() is used to add control variables into the brackets foreach x in fertmaizr improvedseedsr yieldr dailyconsr exp_yield_fertr { qui sum `x&#39; if vouch==0 &amp; round==2 local control_mean=r(mean) qui xi: areg l`x&#39; vouch_dur vouch_aft ${sn_treatments} i.nw_talkedagmoder*i.round, /// absorb(vlgid_round) cluster(respid) outreg2 using &quot;table 2.xls&quot;, `rep_app&#39; bracket label nocons noni /// less(1) nor2 keep(vouch_dur vouch_aft ${sn_treatments}) adds(mean_control,`control_mean&#39;) local rep_app = &quot;append&quot; } 2.1.2 FIGURE 2 - Direct and Spillover Impacts of Subsidies 2.1.2.1 Final output for Figure 2 This figure highlights the Intent-to-Treat estimates through the direct and indirect effects on five outcomes of the subsidy program on the farmers and on their social networks. The outcomes of interest include the adoption of fertilizer and adoption of improved maize seeds, the maize yields, the consumption of the household (an indirect measure of agricultural profit), and the expected yields to the technology package. In this figure, indirect effects are estimated on individuals having at least two contacts in the treatment group, because they are the ones for whom effects are the largest, as demonstrated in Table 2. The coefficient of the regressions of interest are represented with dots, while the lines represent 95 percent confidence intervals. Direct impacts of the subsidies on the voucher recipient group are estimated for all five outcomes for both the “during” (subsidized) and the “after” (post-subsidy) periods. The Figure shows that the program had a large and significant impact during the subsidy period on the adoption of fertilizers and improved seeds, also allowing for higher maize yields. The effects on consumption were only significant after the subsidy period. Yield expectations were stable across periods for treated farmers. While the effect on fertilizer use decreases in magnitude after the subsidy period, it remains substantial and statistically significant. The impact on agricultural yields also persists over time. Regarding spillover effects, impacts of the program are positive and significant on the adoption of fertilizers, on improved seeds, maize yields, and expected returns to the technology package. 2.1.2.2 Replication code for Figure 2 Data preparation and confidence intervals calculation So far, a lot of modifications have been made to the variables. To construct the figure, you need the original data, without saving the transformations to the variables made until now. use &quot;Dataset_CLY.dta&quot;, clear Install the parmest package that allows for semi-parametric estimation of partially linear models. ssc install parmest Then, regress the outcome variables on the treated individuals (farmers who won the vouchers) and on individuals who have at least two contacts in the treatment group. Regressions are run separately for each of the five outcomes, using a loop - see section 1.2.2 for more information on how to create a loop. Cluster the standard error at the level of respid (respid is the variable for the respondent’s identifier, that is, cluster at the individual level), and use parmest to save the regression results in the .dta format. For each variable, you will obtain the estimated coefficients and standard errors for the periods during and after the program, for treated farmers and those with at least two contacts in the treatment group. foreach x in lfertmaizr limprovedseedsr lyieldr ldailyconsr lexp_yield_fertr { qui xi: reg `x&#39; vouch_dur vouch_aft sn2up_sub_dur sn2up_sub_aft /// i.nw_talkedagmoder*i.round i.vlgid_round, cluster(respid) parmest, saving(`x&#39;, replace) } To generate the final figure, you must create a new dataset. To do so, you need to iterate through variable names and perform certain operations on our variables. First, use the local command to define a local macro y with a value of 1. Then create a loop using foreach which includes our variables of interest. Then, apply the following commands on variables in the loop: use `x'.dta loads the dataset corresponding to the variables in the loop use the command gen to create a time variable using the observation number (_n), drop the observations for which time is greater than 4 using drop This graph aiming to generate a visually informative figure by plotting mean estimates along with their corresponding 95% confidence intervals for a set of variables. Only keep the variables estimate, stderr and time Use the round command to round the values of the variables to 2 decimal points Generate, using gen, two new variables with missing values which capture the estimates of each regression after the treatment and the standard error of these coefficients (respectfully estimate_NV and sd_NV, followed by the value taken by y). The command replace allows us to assign specific values to the new variables. With the command rename, include a suffix based on the local macro y’ to the selected variables. Drop all periods after round 2 because you only want the first two periods in the figure Creating a local y equal to y + 1, increment y to the next rank to move on to the next variable estimate_Vy+1 and estimate_NV+1 Eventually, save the variables created with the command save. local y = 1 foreach x in lfertmaizr limprovedseedsr lyieldr ldailyconsr lexp_yield_fertr { use `x&#39;.dta gen time=_n drop if time&gt;4 keep estimate stderr time replace estimate=round(estimate,0.01) replace stderr=round(stderr,0.01) gen estimate_NV`y&#39;=. replace estimate_NV`y&#39;=estimate[3] in 1 replace estimate_NV`y&#39;=estimate[4] in 2 gen sd_NV`y&#39;=. replace sd_NV`y&#39;=stderr[3] in 1 replace sd_NV`y&#39;=stderr[4] in 2 rename estimate estimate_V`y&#39; rename stderr sd_V`y&#39; local y = `y&#39;+1 drop if time&gt;2 save, replace } Merge the quantity of fertilizer used with each variable of interest (again, using a loop) based on the time variable. use lfertmaizr, clear foreach x in limprovedseedsr lyieldr ldailyconsr lexp_yield_fertr{ merge 1:1 time using `x&#39;.dta, nogen save figure2.dta, replace } Clear existing matrices and results in Stata’s memory, and load the dataset named “figure2.dta”. clear matrix clear results use &quot;figure2.dta&quot;, clear Calculate lower and upper limits of the 95% confidence interval: Create a foreach loop to iterate over a list of variables(V1, V2, …, NV5). Use the mean estimate (estimate_) and standard deviation (sd_) to calculate the lower and upper bounds of the 95% confidence interval for each variable by subtracting 1.96 times the standard deviation from the values taken by each variable in the loop. Set the upper limit to 1 if it’s greater than 1, in other words, substitute 1 for any variable value exceeding 1. Create a matrix with mkmat to store the results (estimate, lower limit, and upper limit) for each variable in the loop. foreach i in V1 V2 V3 V4 V5 NV1 NV2 NV3 NV4 NV5{ gen ll95_`i&#39; = estimate_`i&#39; - 1.96*sd_`i&#39; gen ul95_`i&#39; = estimate_`i&#39; + 1.96*sd_`i&#39; replace ul95_`i&#39;=1 if ul95_`i&#39;&gt;1 mkmat estimate_`i&#39; ll95_`i&#39; ul95_`i&#39;, matrix(`i&#39;) } Package installation and graph style settings These preprocessing steps are crucial for ensuring accurate and visually compelling representations of the estimated coefficients with their associated confidence intervals. The resulting figure is a valuable tool for conveying the uncertainty surrounding the mean estimates, aiding in the interpretation and communication of statistical findings. Install and initialize additional packages (grstyle and coefplot) for graph styling and coefficient plotting. Set graph style settings, such as background color and major grid color. ssc install grstyle ssc install coefplot grstyle init grstyle color background white grstyle color major_grid white Graph creation and exportation The command coefplot creates a graphic displaying the coefficients of the regression and the confidence interval. - matrix selects the values to be used for the graph in the matrix. - ci selects the upper and lower bounds of the confidence interval - msize is used to select the size of the graph lines. - xline(0, lpattern(solid) lw(thick)) adds a vertical line to the x-axis at position 0, with a line style of “solid” and a line thickness of “thick”. - xlabel(-0.5(0.5)1) adds axis labels from -0.5 to 1, with a step of 0.5. - bylabel is used to define labels and byopts is used to define the chart title. - ciopts(lwidth(thick thick)) defines the line thickness for confidence interval. Reproduce 5 times this process for each variable, during and after the treatment. coefplot /* */ (matrix(V1[,1]), ci((V1[,2] V1[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV1[,1]), ci((NV1[,2] NV1[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1))|| /* */ ||, bylabel( ) byopts(title(&quot;Fertilizer on maize&quot;)) xsize(2) scale(1.35) /// ylabel(,labsize(vlarge)) ciopts(lwidth(thick thick)) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(fertilizer) coefplot /* */ (matrix(V2[,1]), ci((V2[,2] V2[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV2[,1]), ci((NV2[,2] NV2[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ ||, byopts( title(&quot;Improved maize seeds&quot;)) bylabel( ) xsize(2) scale(1.35) /// ylabel(,labsize(vlarge)) ciopts(lwidth(thick thick)) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(improved) coefplot /* */ (matrix(V3[,1]), ci((V3[,2] V3[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV3[,1]), ci((NV3[,2] NV3[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1))|| /* */ ||, byopts( title(&quot;Maize yield&quot;)) bylabel( ) xsize(2) scale(1.35) /// ylabel(,labsize(vlarge))ciopts(lwidth(thick thick) ) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(yield) coefplot /* */ (matrix(V4[,1]), ci((V4[,2] V4[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV4[,1]), ci((NV4[,2] NV4[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ ||, byopts( title(&quot;Consumption&quot;)) bylabel( ) xsize(2) scale(1.35) /// ylabel(,labsize(vlarge))ciopts(lwidth(thick thick) ) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(consumption) coefplot /* */ (matrix(V5[,1]), ci((V5[,2] V5[,3])) msize(large) xline(0, lpattern(solid) lw(thick)) /// xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ (matrix(NV5[,1]), ci((NV5[,2] NV5[,3])) msize(large) /// xline(0, lpattern(solid) lw(thick)) xscale(range(-0.5(0.5)1)) xlabel(-0.5(0.5)1)) || /* */ ||, byopts( title(&quot;Expected yield with technology package&quot;)) bylabel( ) xsize(2) /// scale(1.35) ylabel(,labsize(vlarge)) ciopts(lwidth(thick thick)) /* */ coeflabels(r1=&quot;During&quot; r2=&quot;After&quot; ) mlabel mlabposition(1) mlabsize(vlarge) /// format(%04.2f) name(expected) Combine all the graphs in one graph and edit it: b1title(Estimated coefficients, size(small) color(black)) adds a title to the top of the combined chart, stating “Estimated coefficients.” The title size is “small” and the text color is “black.” cols(1) charts are combined into a single column. altshrink altering scaling of text iscale(*3.2) is used to select the graph scale. The graphs here are enlarged by 3.2 times. xcommon means that the x is common to all graphs. imargine define the interior margins. graph combine fertilizer improved yield consumption expected, /// b1title(Estimated coefficients, size(small) color(black)) cols(1) altshrink iscale(*3.2) /// xsize(3) xcommon imargin(b=1 t=1) title(&quot;Direct impact&quot; &quot;on treatment group&quot;, /// size(small) color(black) position(11)) subtitle(&quot;Spillover impact via social&quot; /// &quot;network contacts&quot;, size(small) color(black) position(1)) Finally, export the graph in .png and .pdf with the function gr export. gr export figure2.png, replace gr export figure2.pdf, replace 2.1.3 TABLE 7 - Separated Estimation of Spillover Effects for the First and Second Years After the Program 2.1.3.1 Final output for Table 7 In both Table 2 and Figure 2 presented above, researchers estimate a single “after” treatment effect, pooling the two years after the intervention period. In Table 7, the researchers distinguish between spillover effects one year after, and two years after the program, separately. The objective of this table is to push the analysis of spillovers further, and is similar to conduct a robustness check in the context of this study. To dissect spillover effects over time, researchers introduce two different “after” indicators for each of the post-subsidy years. As expected, due to reduced power, fewer coefficients reach statistical significance. There is no clear systematic pattern to the coefficients across the two years, and the hypothesis that the direct and spillover effects are consistent between the first and second “after” years cannot be rejected. 2.1.3.2 Replication code for Table 7 Data preparation Start with initializing a local macro using local, named rep_app and set its value to replace. For more information, see section 2.1.1.2.1. use &quot;Dataset_CLY.dta&quot;, clear local rep_app = &quot;replace&quot;&quot; Begin by correctly labeling the variables that will appear in Table 7. local rep_app = &quot;replace&quot; label var vouch_dur &quot;Direct impacts during&quot; label var vouch_aft_r3 &quot;Direct impacts 1 year after&quot; label var vouch_aft_r4 &quot;Direct impacts 2 year after&quot; label var sn2up_sub_dur &quot;Spillover impacts during&quot; label var sn2up_sub_aft_r3 &quot;Spillover impacts 1 year after&quot; label var sn2up_sub_aft_r3 &quot;Spillover impacts 2 year after&quot; Regression and table creation The code for creating Table 7 is very similar to that for creating Table 2. Please refer to section 1 for more detail on the commands used here. Using a loop, run a regression with areg, adding the prefix xi- to indicate that the regression includes indicator values (equal to either 0 or 1), and including fixed effects which will be absorbed for the variable vlgid_round with the option absorb. Standard errors are clustered at the individual level (respid variable). The command qui, for “quietly”, means that the results will not be displayed. Then test the equality of coefficients between vouch_aft_r3 and vouch_aft_r4, and between sn2up_sub_aft_r3 and sn2up_sub_aft_r4 using the test command. This command performs Wald tests of simple and composite linear hypotheses about the parameters of the most recently fitted model. This allows us to test whether there is a significant difference between the coefficients for period 3 and period 4. Next, store the p-value results using scalar. The last step is to export the results you have just obtained into our results folder using outreg2. To create a nice table easily, refer to section 2.1.1.2.1 . foreach x in lfertmaizr limprovedseedsr lyieldr ldailyconsr lexp_yield_fertr { qui xi: areg `x&#39; vouch_dur vouch_aft_r3 vouch_aft_r4 sn2up_sub_dur sn2up_sub_aft_r3 /// sn2up_sub_aft_r4 i.nw_talkedagmoder*i.round, absorb(vlgid_round) cluster(respid) test vouch_aft_r3 = vouch_aft_r4 scalar vou_di = r(p) test sn2up_sub_aft_r3 = sn2up_sub_aft_r4 scalar sn_di = r(p) outreg2 using &quot;table A7.xls&quot;, `rep_app&#39; bracket label nocons noni less(1) nor2 /// keep(vouch_dur vouch_aft_r3 vouch_aft_r4 sn2up_sub_dur sn2up_sub_aft_r3 /// sn2up_sub_aft_r4) adds(&quot;vouch_aft dif (r3-r4) p-value&quot;, vou_di, /// &quot;sn2up_sub_aft dif (r3-r4) p-value&quot; , sn_di ) local rep_app = &quot;append&quot; } Authors of the replication: Elvire Jégu, Ambre Delaunay, Chiara Balducc, Yagmur Helin Aslan Date: December 2023 "],["instrumental-variables.html", "Chapter 3 Instrumental Variables ", " Chapter 3 Instrumental Variables "],["winners-and-losers-from-agrarian-reform-evidence-from-danish-land-inequality-16821895.html", "3.1 Winners and losers from Agrarian Reform: Evidence from Danish land inequality 1682–1895", " 3.1 Winners and losers from Agrarian Reform: Evidence from Danish land inequality 1682–1895 Boberg-Fazlić, N., Lampe, M., Lasheras, P. M., &amp; Sharp, P. (2022). Winners and losers from Agrarian Reform: Evidence from Danish land inequality 1682–1895. Journal of Development Economics, 155, 102813 Highlights The paper examines the distributional effects of land reforms between 1682-1895 in Denmark The authors use an Instrumental Variable (IV) to deal with endogeneity issues stemming from reverse causality between the variable of interest (land productivity) and the dependent variable (land inequalities). This methodology is standard by the choice of a geological instrument. It follows the usual steps of the IV procedure, including tests of the instrument’s pertinence (F-statistic of the 1st stage estimation). Our added value to the original replication package lies in the detailed explanation of the provided code. Additionally, we demonstrate how to extract various tables and figures directly from the Stata code. While the authors have created a well-realized replication package, our contribution enhances its accessibility and usability. We make use of these Stata tricks: Diagnostic Statistics: We leverage the estat firststage command to obtain diagnostic statistics pertaining to the first-stage of instrumental variable estimation, which includes extracting and storing the F-statistics for further analysis. Looping: The foreach loop structure is used, which enables the iteration over multiple years for instrumental variable regressions and allows to avoid repetitive commands in the code. Advanced Formatting: Through the utilization of the esttab command along with various formatting options, we create well-formatted tables, which results in the production of tables that are not only clear and organized but also visually appealing. Link to the paper: https://osf.io/jmn5y/?view_only=f18f6d51efe44f04abef6e9042c0163c Link to the replication package: https://doi.org/10.1016/j.jdeveco.2021.102813 Click here to download the Stata Do-File to replicate the results Error in library(downloadthis): aucun package nommé &#39;downloadthis&#39; n&#39;est trouvé Error in download_link(link = &quot;https://git-link.vercel.app/api/download?url=https%3A%2F%2Fgithub.com%2FSorbonneDevEcon%2FM2-Econometrics-Book%2Fblob%2Fmain%2Fresources%2FAA_ALC_AT%2FDoFile_AA_ALC_AT.do&amp;filename=DoFile_AA_ALC_AT.do&quot;, : impossible de trouver la fonction &quot;download_link&quot; 3.1.1 Introduction As a rule, agrarian reforms are viewed as fundamental for economic development, allowing, on one hand, to fuel agricultural productivity and on the other, to reallocate the necessary productive resources to industrialization. Their design has to fulfill two competing objectives: stimulating farms’ productivity and ensuring an equitable access to land. As numerous reforms have been criticized for failing the latter, this paper provides the first quantitative long-term assessment of the Danish agrarian reforms’ effects on both economic efficiency and land inequalities. Rather efficiency-oriented, these reforms were designed to support the owners of medium-sized farmers and were therefore detrimental to smallholders and landless agricultural laborers. As land inequalities became more stringent in the most fertile areas, the objective of the paper is to find the underlying mechanisms of these patterns and more specifically, the role of agrarian reforms in this context. To do so, the authors combine several sources of historical data on farms at the parish level with population and agricultural censuses to cover the 1682- 1895 period and enhance the robustness and depth of their analysis. The utilization of a singular, extensive dataset allows for a thorough examination of long-term land inequality trends and patterns, including nuances and variations that might be overlooked in studies with more limited or less comprehensive datasets. The authors’ access to a unique dataset positions their study as a valuable contribution to the literature on historical agrarian economies, providing nuanced perspectives that contribute to a broader understanding of the subject matter. The authors opted for the use of the Theil index to measure land inequality due to its analytically desirable properties. One key advantage is that the Theil index adheres to the principle of transfers, ensuring that a redistribution from one individual to a less affluent one results in a proportional decline in the Theil index, which is particularly convenient for focusing on changes in inequalities over time. Moreover, the Theil index provides unambiguous rankings of distributions, ensuring that two regions with identical Theil indices exhibit identical income distributions. This is not guaranteed by other common measures such as the Gini index. The preference for the Theil index would allow for a higher precision and interpretability of their findings. From an econometric point of view, the most obvious approach would be to regress changes in parish-level land inequalities on a measure of soil productivity – Total HK, translated as “barrels of hard grain” – and some geographical characteristics of the parish, as well as regional fixed effects. Nonetheless, such a specification would be exposed to endogeneity issues, due to the reverse causality between the explained variable (evolution in land inequalities) and the variable of interest (land productivity): as higher agricultural productivity leads to Malthusian dynamics, in parishes with more fertile soils, there would be more smallholders and landless individuals, whose socio-economic status would be deteriorated by the reforms. Consequently, areas with a higher agricultural productivity were more exposed to rises in land inequalities. At the same time, population growth has beneficial effects on innovation and thus, agricultural productivity. To tackle this, the authors adopt an IV strategy and choose as an instrument for land productivity the distribution of “Boulder Clay”, the sediment type most adapted to barley, resulting from the Weichselian glaciation. Geological variables are generally viewed as reliable instruments, since they capture long-term determinants of development that are independent from human factors. 3.1.2 Identification strategy When deciding to use a 2SLS strategy, several points need to be discussed in order to allow the identification of robust causal effects. The first hypothesis to be considered and which cannot be tested statistically is the exclusion restriction. It is necessary to rule out any direct effect of the instrument (boulder-clay) on the dependent variable (land inequalities). In this specific case, it can be assumed that soil composition doesn’t directly affect the level of land inequalities. In fact, the authors argue that the rise of inequalities is driven by a stronger demographic growth, due to the productive capacity of the land. This implies, beside the soil fertility, adequate land management practices and efficient agricultural technologies. The authors also have to exclude any effect of the dependent variable on the instrument. Here, once again, land inequalities and the soil fertility seem to be unrelated, as the soil’s share of boulder clay stems from the Weichselian glaciation, which occurred approximately 18,000 years ago. Moreover, the sediment classification they use was made below the impact area of cultivation practices and technologies, which allows to infirm a potential effect of inequalities on this measure of land fertility. The second hypothesis to be respected is the instrument’s relevance. The authors need to convincingly describe how the instrument affects the endogenous variable. In our case, the soil composition represents a key determinant of the land’s productivity and thus is supposed to be positively correlated with the total production of an area measured by the variable TotalHK. Unlike the aforementioned exclusion restriction, this assumption can be tested statistically. It can be done, for instance, by verifying whether after estimating the first-stage specification (1), the coefficient of the instrumental variable is statistically significant or whether the F-statistic is superior to the conventionally fixed value of 10. \\[\\begin{equation} \\tag{1} ln(TotalHK)_{p}=α_{0}+βBoulderClay_{p}+λ_{r}+X_{p}γ+ϵ_{p} \\end{equation}\\] As we can see in column 2 of Table 1, the effect of the soil composition on total production is statistically significant at 1% level. Also, the value of the F-statistic is equal to 280. Hence, we can conclude that both steps necessary to ensure the relevance and the exogeneity of the instrument have been fulfilled. That said, they further estimate the second-stage specification (2). \\[\\begin{equation} \\tag{2} ΔTheil_{p}=α_{0}+β\\widehat{ln(TotalHK)}_{p}+λ_{r}+X_{p}γ+ϵ_{p} \\end{equation}\\] After estimating both the OLS and the second-stage IV specifications, the authors find statistically significant positive effects of land productivity on land inequalities during the agrarian reforms. To ascertain the robustness of these findings, they estimate additional specifications, using alternative measures of land inequalities – such as Gini index, an alternative to the Theil index – and of land productivity – the amount of barley paid in tax. As no significant change in the results was detected, we can conclude that the econometric estimation allowed them to confirm their predictions, exposed in the theoretical part of the paper. In the next part of our narrative, we will discuss some of the main figures of the paper. We will also indicate the necessary commands to replicate them using Stata. 3.1.3 From the article to practice: exploring the replication code 3.1.3.1 Getting started: database access and required packages In order to open the Stata database and execute the following lines of code, several packages need to be downloaded and installed. In this section, we guide you through the process of accessing the database and briefly refer to these packages. The Stata code to open the database uses a global variable dirData to store the path to the directory where the \"Dataset_AA_ALC_AT\" database is located. Users can change the value of the dirData variable based on the location of their own data directory. Thus, by simply modifying the value of dirData to match their personal path, the users can open the database without having to modify the rest of the code. Subsequently, several Stata packages are necessary to execute the replication code successfully. The first package needed is the estout package. This package allows to make regression tables using regressions previously stored in the Stata memory. The second package required is the ivreg2 package. This package allows to run instrumental variables regressions. The third package is the coefplot package. This package is used to create coefficients plots which visually represent the estimated coefficients and their confidence intervals. The fourth package is outreg2. This package is used to produce illustrative tables of regression outputs. This package is able to write LaTex-format tables. All the packages can be installed using the following lines of code. 3.1.3.2 Stata code in order to access to the database and to install packages ***Open the database*** global dirData &quot;C:\\Your Directory Here&quot; use &quot;${dirData}/Dataset_AA_ALC_AT&quot;, clear ***Install the required packages*** ssc install estout ssc install ivreg2 ssc install coefplot ssc install outreg2 3.1.3.3 Understanding the replication process: code analysis of Table 1 Table 1: IV estimation using total HK and the share of parish area classified as boulder clay ***Table 1*** eststo clear ***Column (1) : OLS*** eststo ols1: qui reg D.Theil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast /// i.region if year==1834, vce(robust) eststo ols2: qui reg D.AggTheil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast /// i.region i.year if year==1834, vce(robust) ***Column (2) : First-stage*** eststo fiv1: qui reg ln_TotalFarmHK MLmean ln_area LnDistCPH Lat Long LnDistCoast /// i.region if year==1834, vce(robust) ***Column (4) : Reduced form*** eststo ziv1: qui reg D.Theil_c MLmean ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834, vce(robust) ***Column (3) : Second-stage (with diffTheil)*** eststo ivdiff1: qui ivregress 2sls D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (5) : Second-stage (with diffAggTheil)*** eststo ivdiff2: qui ivregress 2sls D.AggTheil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (6) : Second-stage (with Gini)*** eststo ginihkdiff: qui ivregress 2sls D.Gini ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Formation of Table 1*** esttab ols1 fiv1 ivdiff1 ziv1 ivdiff2 ginihkdiff, se star(* 0.10 ** 0.05 *** 0.01) b(3) /// r2 var(15) model(12) wrap keep(MLmean ln_TotalFarmHK) mtitles(&quot;OLS&quot; &quot;first stage&quot; /// &quot;second stage (diffTheil)&quot; &quot;reduced form (diff Theil)&quot; &quot;second stage (diffAggTheil)&quot; /// &quot;second stage (diffGini)&quot; ) stats(N r2 Fstat, labels(&quot;Observations&quot; &quot;R-squared&quot; /// &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) indicate(&quot;Region FE = 2.region&quot; /// &quot;Geography = LnDistCPH&quot; , labels(Y N)) label OLS regressions by replicating the columns 1,2 and 4 of Table 1 ***Table 1*** eststo clear ***Column (1) : OLS*** eststo ols1: qui reg D.Theil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast /// i.region if year==1834, vce(robust) eststo ols2: qui reg D.AggTheil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast /// i.region i.year if year==1834, vce(robust) ***Column (2) : First-stage*** eststo fiv1: qui reg ln_TotalFarmHK MLmean ln_area LnDistCPH Lat Long LnDistCoast /// i.region if year==1834, vce(robust) ***Column (4) : Reduced form*** eststo ziv1: qui reg D.Theil_c MLmean ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834, vce(robust) Columns 1, 2, and 4 of Table 1 represent two Ordinary Least Squares (OLS) regressions, a first-stage, and a reduced form, respectively, representing distinct stages through OLS regressions. These different model specifications are often used in econometric analyses to examine causal relationships between variables. These columns offer a comprehensive understanding of underlying economic relationships and allow for nuanced interpretation of results. The singular variable differing across the two OLS regressions is the initial variable, corresponding to the dependent variable: the change in the Theil index from 1682 to 1834 D.Theil_c and the change in the Theil index of 1834 aggregated to the size categories of 1682 D.AggTheil_c, respectively. In line with our approach of conducting both the first-stage and the reduced form analyses, we introduce the instrumental variable MLmean in the two last regression equations. The replication process of these columns involves several crucial steps in estimating econometric models. Firstly, the use of the eststo clear command ensures a reset of previous estimation results, creating a clean environment before running new estimations. Subsequently, the inclusion of qui – for quietly – with the reg – for regress – function allows for the temporary storage of regression results in a named matrix, such as ols1 in the first model. Moreover, the model specification, denoted as D.Theil_c ln_TotalFarmHK ln_area LnDistCPH Lat Long LnDistCoast i.region, details the variables included in the regression equation. It is crucial to note that this specification is essential for understanding the relationships between key variables and the dependent variable – here D.Theil_c or D.AggTheil_c. Furthermore, the use of the conditional if year==1834 filters the data, retaining only observations where the year variable is equal to 1834. This filtering step enables a focus on a particular year – here 1834 –, allowing for a more targeted analysis. Finally, the vce(robust) option is specified to estimate robust standard errors, thereby addressing potential heteroskedasticity issues. These robust standard errors are particularly important to ensure the reliability of estimation results, especially when residuals exhibit heterogeneous variations. IV regressions by replicating columns 3,5 and 6 of Table 1 ***Table 1*** ***Column (3) : Second-stage (with diffTheil)*** eststo ivdiff1: qui ivregress 2sls D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (5) : Second-stage (with diffAggTheil)*** eststo ivdiff2: qui ivregress 2sls D.AggTheil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (6) : Second-stage (with Gini)*** eststo ginihkdiff: qui ivregress 2sls D.Gini ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] The columns 3, 5, and 6 which represent second-stages, are estimated through instrumental variable regressions. Similar to OLS regressions, the command eststo ivdiff1 : qui is used to store the estimation results in a matrix named ivdiff1, ensuring an organized storage of relevant information. Secondly, the function ivregress 2sls is applied to conduct a two-stage IV regression, as indicated by 2sls. Additionally, the explanatory variables D.Theilc lnarea LnDistCPH Lat Long LnDistCoast i.region mirror those used in the OLS regressions, providing continuity in the model specification. The only variable differing across each column specification is the initial variable, corresponding to the dependent variable : the change in the Theil index from 1682 to 1834 D.Theilc, the change in the Theil index of 1834 aggregated to the size categories of 1682 D.AggTheilc, and the change in the Gini coefficient instead of the Theil index D.Gini, respectively. This nuanced change reflects the distinct focus on different dependent variables in each instance, while the remaining explanatory variables remain constant, allowing for a systematic exploration of the impact of the selected variables on the varied dependent variables. The inclusion of (lnTotalFarmHK = MLmean) specifies the endogenous variable lnTotalFarmHK and its instrument MLmean. This crucial specification distinguishes the endogenous variable and its corresponding instrument, a fundamental aspect of instrumental variable estimation. Furthermore, to address heteroskedasticity and autocorrelation, the vce(robust) command employs a robust variance-covariance matrix, ensuring the reliability of standard errors in the estimation. The upcoming lines of code that we are about to describe initially seemed somewhat unclear to us. However, despite the initial ambiguity, they prove to be valuable and, upon closer examination, are now better understood. This is why we will take the time to provide a clear explanation of these lines. First, the line estat firststage is employed to display statistics from the first-stage of the IV regression. This step is typically utilized to assess the validity of instruments and ascertain their suitability for the estimation process. Secondly, storing the first-stage results in a matrix named ‘fstat’ is accomplished through the line mat fstat = r(singleresults). This matrix captures relevant statistics from the first-stage, providing a comprehensive view of the instrumental variable performance. Finally, the line estadd scalar Fstat = fstat[1,4] introduces a new scalar variable, Fstat, into the main regression results. This step allows to extract the F-statistic from the first-stage matrix fstat and assigns it to Fstat. The F-statistic is a crucial metric in instrumental variable regression, serving as a diagnostic tool to assess the overall validity of the instruments used. A high F-statistic, that is superior to 10, suggests that the instruments collectively have a strong explanatory power for the endogenous variable. In summary, these lines of code are essential for evaluating the quality and relevance of the instrumental variables employed in the two-stage IV regression. Formatting Table 1, an additional but optional step ***Table 1*** ***Formation of Table 1*** esttab ols1 fiv1 ivdiff1 ziv1 ivdiff2 ginihkdiff, se star(* 0.10 ** 0.05 *** 0.01) b(3) /// r2 var(15) model(12) wrap keep(MLmean ln_TotalFarmHK) mtitles(&quot;OLS&quot; &quot;first stage&quot; /// &quot;second stage (diffTheil)&quot; &quot;reduced form (diff Theil)&quot; &quot;second stage (diffAggTheil)&quot; /// &quot;second stage (diffGini)&quot; ) stats(N r2 Fstat, labels(&quot;Observations&quot; &quot;R-squared&quot; /// &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) indicate(&quot;Region FE = 2.region&quot; /// &quot;Geography = LnDistCPH&quot; , labels(Y N)) label The next stage in our analysis involves the creation of a comprehensive table summarizing the results of the previously conducted regressions. It is essential to note that this step does not stand alone, rather it complements the preceding two steps in which variables were defined and regressions were estimated. The resulting table serves as a visual representation of the relationships captured in the regression models, enhancing the interpretability and communicative power of the findings. First, the command esttab ols1 fiv1 ivdiff1 ziv1 ivdiff2 ginihkdiff specifies which models to include in the table, incorporating the Ordinary Least Squares (OLS), first-stage instrumental variable (IV), and second-stage IV regression results. To enhance the clarity of the table, additional formatting options are applied. The se star(* 0.10 ** 0.05 *** 0.01) command displays standard errors with significance stars, denoting significance levels with asterisks. Thirdly, the b(3) command allows to limit coefficient estimates to three decimal places, contributing to a cleaner presentation of results. Moreover, the inclusion of r2 displays the coefficient of determination, the R-squared, in the table, providing insights about the explanatory power of the model. The var(15) option limits the display of R-squared to 15 decimal places. Additionally, with model(12), we specify the maximum number of models to display in the table, accommodating up to 12 different model specifications. The wrap command facilitates the wrapping of long variable names onto multiple lines, ensuring readability. The keep(MLmean lnTotalFarmHK) option selectively includes only the variables MLmean and lnTotalFarmHK in the table, which are our instrument and endogenous variables. Furthermore, mtitles() assigns model titles to each specified model, contributing to a more informative and organized presentation. The stats(N r2 Fstat, labels(\"Observations\" \"R-squared\" \"KP F-statistic\") fmt(%9.0fc 2 2)) option adds relevant statistics to the table, including the number of observations (N), the R-squared, and the F-statistic. The indicate (\"Region FE = 2.region\" \"Geography = LnDistCPH\", labels(Y N)) command introduces indicator in a regression model. The first part, \"Region FE = 2.region\", includes fixed effects for a specific region, here denoted as 2, representing Jutland. This accounts for unobserved variation specific to Jutland. The second part, \"Geography = LnDistCPH\", introduces an indicator variable related to geography. The labels Y and N are assigned to indicate the presence or absence of fixed effects or controls in each observation, contributing to the model’s interpretability. This step allows for a nuanced interpretation of how fixed effects and geographic controls influence the results. Finally, the label option is appended to the esttab command, indicating that variable labels should be included in the table for clarity and precision. In the replication code provided by the authors, no specific instruction regarding the exportation of tables has been included. Therefore, we recommend users to add the command using \"NameOfYourTable.tex\" right after the last variable mentioned in esttab, just before the comma in front of se star, if they intend to export the table in LaTeX format. Alternatively, users can use the command using NameOfYourTable.txt\", at the same location in the code, if they prefer the table in a text format. This flexibility allows users to choose the desired output format for the tables based on their specific needs. This last step – which involves the formation of the table – can be used in the context of other studies to generate a clear and visually appealing table summarizing regression results. However, it is crucial to emphasize that this step is not indispensable for obtaining replication results. The results can be independently observed directly using the same code without the qui option, starting from reg or ivreg. This underscores the flexibility of the code, allowing researchers to directly access and analyze the regression outcomes without relying on the table generation step. While the table contributes to a more organized presentation, it is not a prerequisite for the replication process. Users can tailor their analysis based on their preferences and requirements before proceeding to this step. The study relies on an instrumental variable method that can be challenging to comprehend. Explaining the replication code of Table 1 is deemed essential for us, as it allows a detailed exploration and validation of the authors’ instrumental variable approach. This importance is further underscored by the fact that Table 1 showcases the primary results of the study. Having elucidated the intricacies of the Table 1 replication code, we will now transition to describing the replication code for Figure 5. 3.1.3.4 Understanding the replication process: code analysis of Figure 5 Figure 5 ***Figure 5*** eststo clear foreach x in 1682 1834 1850 1860 1873 1885 1895 { qui ivregress 2sls ln_Theil_`x&#39;c ln_area LnDistCPH Lat Long LnDistCoast i.region /// (ln_TotalFarmHK`x&#39; = MLmean) if year==`x&#39;, vce(clust ID) estimates store coef`x&#39; } coefplot coef*, vert yline(0) keep(ln_TotalFarmHK*) graphregion(color(white)) /// ciopts(recast(rcap) lcol(black)) mcolor(black) xtick(1(1)7) /// xlabel(1 &quot;1682&quot; 2 &quot;1834&quot; 3 &quot;1850&quot; 4 &quot;1860&quot; 5 &quot;1873&quot; 6 &quot;1885&quot; 7 &quot;1895&quot;, ) /// grid(b) legend(off) graph export &quot;outputfile.png&quot;, replace The provided Stata code segment serves to illustrate the coefficients for second-stage estimations conducted over the years 1682 to 1895. The dependent variable under consideration is ln(Theil), and the instrumental variable utilized is the share of boulder clay MLmean. The use of the natural logarithm for the dependent variable ln(Theil) in the second-stage regression is intentional. This choice allows for the presentation of estimates for the second-stage coefficients in levels, offering insight into the relationships over the years 1682 to 1895. The focus on ln(Theil) in different years underscores a preference for examining the absolute values of Theil index rather than changes in Theil index, providing a comprehensive perspective on the dynamics of the variable across the specified temporal range. Initially, eststo clear ensures a clean slate by clearing any previously stored estimation results. Subsequently, the foreach loop in the provided Stata code serves as an iterative mechanism, allowing the execution of specified commands for each value in the specified range or list. In this case, the loop iterates over the years 1682, 1834, 1850, 1860, 1873, 1885, and 1895. For each iteration, the code within the loop conducts a 2-stage least squares regression using the ivregress 2sls command, estimating a model for the given year. The model includes various independent variables such as lnTheil’x’c, lnarea, LnDistCPH, Lat, Long, LnDistCoast as well as the endogenous variable lnTotalFarmHK instrumented by MLmean and there are fixed effects for region. The purpose of the loop in this context is to efficiently run the same regression model for multiple years, automating the process and avoiding redundant code. This is particularly useful when dealing with time-series data or when conducting analyses for various time points. The resulting coefficient plot provides a concise and visual representation of the dynamics of the variable of interest across different years. The line estimates store coef’x’ facilitates the storage of estimation results in matrices named coef’x’, where x represents the specific year. Following the loop, the coefplot coef* command generates a coefficient plot based on the stored estimation results, specifically focusing on coefficients related to the variable lnTotalFarmHK across the years. In terms of visual representation, the plot includes a vertical line at 0 for reference with the vert yline(0) command, retains only coefficients related to lnTotalFarmHK with the keep(lnTotalFarmHK*) command, and employs a white background for enhanced clarity thanks to the graphregion(color(white)). Confidence intervals are displayed using a horizontal line in black with ciopts(recast(rcap) lcol(black)), and the markers – dots – representing coefficient estimates are colored black for visibility with the mcolor(black) option. Additionally, xtick(1(1)7) and xlabel(1 \"1682\" 2 \"1834\" 3 \"1850\" 4 \"1860\" 5 \"1873\" 6 \"1885\" 7 \"1895\"), allow stick marks and corresponding labels on the x-axis to be strategically positioned to represent each year from 1682 to 1895. Gridlines are incorporated for ease of interpretation grid(b), and the legend is turned off for a clean and uncluttered visual representation thanks to the legend(off) command. The final line of code, graph export \"outputfile.png\", replace, is added by us to the replication code for the purpose of exporting the coefficient plot as a PNG file. This command utilizes Stata’s graph export feature, allowing us to save the generated graph to an external file named “outputfile.png” in the current working directory. The option \"replace\" ensures that if a file with the same name already exists, it will be overwritten. This line of code enhances the replicability of the study by facilitating the export of the coefficient plot in a widely used PNG format for further analysis or inclusion in reports and presentations. This meticulous approach allows for a comprehensive exploration of coefficient dynamics over time, offering insights into the relationship between the dependent variable, ln(Theil), and the instrumental variable MLmean – share of boulder clay – with controls included, across the specified years. With the explanation of the coefficient plots for second-stage estimations complete, our attention now shifts to describing the replication code for Table A.3 found in the Appendix. 3.1.3.5 Understanding the replication process: code analysis of Table A.3 Table 3: Robustness check: Second-stage IV estimates using Gini coefficient ***Table A.3*** ***Creation of a loop*** eststo clear foreach x in 1682 1834 { eststo ginihk`x&#39;: ivregress 2sls gini`x&#39; ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==`x&#39; &amp; gini1834!=., vce(clust ID) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] } ***Formation of Table A.3*** esttab ginihk1682 ginihk1834, se star(* 0.10 ** 0.05 *** 0.01) b(3) r2 var(15) model(11) /// wrap keep(ln_TotalFarmHK) mtitles(&quot;2nd stage: Gini diff&quot; &quot;2nd stage: gini 1682&quot; /// &quot;2nd stage: gini 1834&quot;) stats(N r2 Fstat, labels(&quot;Observations&quot; &quot;R-squared&quot; /// &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) indicate(&quot;Region FE = 2.region&quot; /// &quot;Geography = LnDistCPH&quot; , labels(Y N)) label Table A.3 in the Appendix presents robustness check results, specifically second-stage instrumental variable estimates using the Gini coefficient. This additional analysis aims to verify the robustness of the findings by employing an alternative measure. The choice of the Gini coefficient not only enhances interpretability but also provides a basis for comparing and validating the study’s results against a broader scholarly context. Creation of a loop ***Table A.3*** ***Creation of a loop*** eststo clear foreach x in 1682 1834 { eststo ginihk`x&#39;: ivregress 2sls gini`x&#39; ln_area LnDistCPH Lat Long LnDistCoast /// i.region (ln_TotalFarmHK = MLmean) if year==`x&#39; &amp; gini1834!=., vce(clust ID) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] } Firstly, the use of the eststo clear command ensures a reset of previous estimation results, creating a clean environment before running new estimations. The loop designated by foreach x in 1682 1834 iterates over two specific years, namely 1682 and 1834. This looping mechanism, as explained in the preceding section – Section 3.3 –, provides a concise and efficient way to conduct repetitive tasks for multiple years. The eststo ginihk‘x’ command within the loop facilitates the storage of results in matrices named ginihk‘x’, with x representing the current year in each iteration. For a more comprehensive understanding of the loop creation and its purpose, referring to the preceding Section 3.3 is recommended. Moreover, the ivregress 2sls function is employed to conduct a two-stage instrumental variable (IV) regression. The model’s explanatory variables include D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast i.region. In specifying (ln_TotalFarmHK = MLmean), the endogenous variable – the natural logarithm of the total value of the land measured in barrel of hard grain of parish – is denoted as ln_TotalFarmHK and its instrument is identified as MLmean. To ensure that the analysis includes only observations for the specified year x where the variable gini1834 is not missing, the condition if year==x &amp; gini1834!=. is incorporated. Furthermore, the command vce(clust ID) is used to adjust standard errors in the regression model, accounting for within-cluster correlation. The code that we will now describe mirrors the structure found in Table 1. Following this, estat firststage is employed to display statistics from the first-stage of the IV regression. This step is crucial for assessing the validity of instruments. The subsequent lines involve the storage of first-stage results in a matrix named fstat using the mat fstat = r(singleresults) command. This matrix captures relevant statistics from the initial stage, providing insights into the instrumental variable performance. Finally, estadd scalar Fstat = fstat[1,4] introduces a new scalar variable Fstat into the main regression results. This step extracts the F-statistic from the first-stage matrix fstat and assigns it to Fstat. For a more comprehensive understanding of the F-statistics and its purpose, referring to the preceding Section 3.3.2 is recommended. Formatting table A.3, an additional but optional step ***Table A.3*** ***Formation of Table A.3*** esttab ginihk1682 ginihk1834, se star(* 0.10 ** 0.05 *** 0.01) b(3) r2 var(15) model(11) /// wrap keep(ln_TotalFarmHK) mtitles(&quot;2nd stage: Gini diff&quot; &quot;2nd stage: gini 1682&quot; /// &quot;2nd stage: gini 1834&quot;) stats(N r2 Fstat, labels(&quot;Observations&quot; &quot;R-squared&quot; /// &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) indicate(&quot;Region FE = 2.region&quot; /// &quot;Geography = LnDistCPH&quot; , labels(Y N)) label Moving forward in our analysis, we proceed to construct a comprehensive table that consolidates the outcomes of the earlier regression analyses. Importantly, it’s crucial to emphasize that this stage is not conducted in isolation, instead it builds upon the groundwork laid in the preceding step where variables were defined and regressions within the loop were executed. The resultant table serves as a visual representation, effectively summarizing the relationships captured in the regression models. This approach enhances the interpretability and communicative power of the analytical findings. The code employed in the creation of the comprehensive table A.3 aligns with the methodology elucidated in Section 3.2.3, specifically used for Table 1. Indeed, the Stata code provided encompasses the construction of a table using the esttab command, incorporating results from the two-stage instrumental variable regressions conducted for the years 1682 and 1834. Thus, the esttab ginihk1682 ginihk1834 line specifies the models whose results will be included in the table, representing these two specific years. To enhance the clarity and readability of the table, several formatting commands are employed. The se star(* 0.10 ** 0.05 *** 0.01) line introduces significance stars denoted by asterisks, indicating the levels of statistical significance. Additionally, b(3) limits the coefficient estimates to three decimal places. The inclusion of the coefficient of determination R-squared is facilitated by r2 command, with var(15) limiting the number of decimals for R-squared to 15 digits. The model(11) command specifies the maximum number of models displayed in the table, accommodating 11 different models. Moreover, the wrap command assists in managing long variable names, allowing them to span multiple lines for improved readability. The keep(lnTotalFarmHK) line specifies the variable lnTotalFarmHK to be included in the table. Furthermore, model titles are assigned using mtitles(), and additional statistics, such as the number of observations (N), R-squared, and the F-statistic, are incorporated with stats(N r2 Fstat, labels(\"Observations\" \"R-squared\" \"KP F-statistic\")fmt(%9.0fc 2 2)). The indicate(\"Region FE = 2.region\" \"Geography = LnDistCPH\", labels(Y N)) section introduces indicator variables in a regression model. The first part, \"Region FE = 2.region\", includes fixed effects for a specific region, here denoted as 2, representing Jutland. This accounts for unobserved variation specific to Jutland. The second part, \"Geography = LnDistCPH\" introduces an indicator variable related to geography. The labels Y and N are assigned to indicate the presence or absence of fixed effects or controls in each observation, contributing to the model’s interpretability. Finally, the label option appended to the end of the esttab command ensures that variable labels are included in the table, enhancing the interpretability of the presented results. Once again, in the provided replication code by the authors, no specific instruction regarding the exportation of tables has been included. Therefore, we recommend users to add the command using \"NameOfYourTable.tex\" right after the last variable mentioned in esttab, just before the comma in front of se star, if they intend to export the table in LaTeX format. Alternatively, users can use the command using NameOfYourTable.txt\", at the same location in the code, if they prefer the table in a text format. This flexibility allows users to choose the desired output format for the tables based on their specific needs. This last step to form a table can be employed in the context of other studies to generate a clear and visually appealing table summarizing regression results. However, it is crucial to emphasize that this step is not indispensable for obtaining replication results. 3.1.3.6 Understanding the replication process: code analysis of Table A.4 Table 4: IV estimation using barley payments and the share of parish area classified as boulder clay ***Table A.4*** eststo clear ***Column (1) : OLS*** eststo ols1: qui reg D.Theil_c BygLG ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834, vce(robust) eststo ols2: qui reg D.AggTheil_c BygLG ln_area LnDistCPH Lat Long LnDistCoast i.region /// i.year if year==1834, vce(robust) ***Column (2) : First-stage*** eststo fiv1: qui reg BygLG MLmean ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834 &amp; ln_TotalFarmHK!=., vce(robust) ***Column (3) : Second-stage (diffTheil)*** eststo ivdiff1: qui ivregress 2sls D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (BygLG = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (4) : Second-stage (diffAggTheil)*** eststo ivdiff2: qui ivregress 2sls D.AggTheil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (BygLG = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Formation of Table A.4*** esttab ols1 fiv1 ivdiff1 ivdiff2, se star(* 0.10 ** 0.05 *** 0.01) b(3) r2 var(15) /// model(11) wrap keep(MLmean BygLG) mtitles(&quot;OLS&quot; &quot;first stage&quot; /// &quot;second stage (diffTheil)&quot; &quot;second stage (diffAggTheil)&quot; ) stats(N r2 Fstat, /// labels(&quot;Observations&quot; &quot;R-squared&quot; &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) /// indicate(&quot;Region FE = 2.region&quot; &quot;Geography = LnDistCPH&quot; , labels(Y N)) label Transitioning to another robustness test, presented in Table A.4 in the Appendix, we explore an alternative measure of land quality. In this test, rather than considering the total value of the parish’s HK, the authors focus solely on the amount of barley paid in tax as an indicator of land quality. This measure is derived from the digitization of a map presented by Frandsen in 1988. The results of this robustness check closely resemble the main estimations of table 1, underscoring the consistency and reliability of the analytical findings. OLS regressions by replicating the columns 1 and 2 of Table A.4 ***Table A.4*** eststo clear ***Column (1) : OLS*** eststo ols1: qui reg D.Theil_c BygLG ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834, vce(robust) eststo ols2: qui reg D.AggTheil_c BygLG ln_area LnDistCPH Lat Long LnDistCoast i.region /// i.year if year==1834, vce(robust) ***Column (2) : First-stage*** eststo fiv1: qui reg BygLG MLmean ln_area LnDistCPH Lat Long LnDistCoast i.region /// if year==1834 &amp; ln_TotalFarmHK!=., vce(robust) In this analysis, we employ familiar Stata commands, akin to those utilized in Table 1, with the only variable differing across each column specification is the initial variable, corresponding to the dependent variable: the change in the Theil index from 1682 to 1834 D.Theilc, the change in the Theil index of 1834 aggregated to the size categories of 1682 D.AggTheilc, and the amount of barley paid in tax as an indicator of land quality BygLG, respectively. In line with our approach of conducting the first-stage analysis, we introduce the instrumental variable MLmean to the last regression equation. First, the command eststo ols1 is used to store the estimation results, and the results matrix is designated as ols1. The qui function, signifying quietly, enables the temporary storage of results in the matrix ols1 for the initial regression. Importantly, this function ensures that the results of the regression are not displayed at this point but are preserved under the name ols1 for subsequent utilization. This approach proves particularly beneficial when authors later aim to present a comprehensive table with multiple specifications. Moreover, the reg function is utilized for Ordinary Least Squares (OLS) estimation, running a regression with specified variables. In this instance, the regression equation includes Theilc BygLG lnarea LnDistCPH Lat Long LnDistCoast i.region. A conditional statement, if year==1834 &amp; lnTotalFarmHK!=., restricts the analysis to observations where the year variable is equal to 1834, and lnTotalFarmHK is not missing. This condition ensures a focused examination of data relevant to the specified year and variable condition. Furthermore, the use of the conditional if year==1834 filters the data, retaining only observations where the year variable is equal to 1834. Lastly, the vce(robust) command is incorporated to specify the use of robust standard errors. This adjustment is made to account for potential heteroskedasticity and correlation of errors, enhancing the reliability of the estimation results. IV regressions by replicating the columns 3 and 4 of the Table A.4 ***Table A.4*** ***Column (3) : Second-stage (diffTheil)*** eststo ivdiff1: qui ivregress 2sls D.Theil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (BygLG = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] ***Column (4) : Second-stage (diffAggTheil)*** eststo ivdiff2: qui ivregress 2sls D.AggTheil_c ln_area LnDistCPH Lat Long LnDistCoast /// i.region (BygLG = MLmean) if year==1834, vce(robust) estat firststage mat fstat = r(singleresults) estadd scalar Fstat = fstat[1,4] In this specific analysis, we focus on the second-stage of the estimations with two dependent variables: D.Theilc, representing the change in the Theil index from 1682 to 1834, D.AggTheilc, representing the change in the Theil index of 1834 aggregated to the size categories of 1682. The endogenous variable, the amount of barley paid in tax as an indicator of land quality BygLG, is instrumented by the share of boulder clay, utilizing MLmean as the instrumental variable. This nuanced approach allows us to enhance the robustness of the estimates and address potential biases in the endogenous variable. In executing the second-stage of our analysis, employing the eststo ivdiff1: qui command allows to store the estimation results in a matrix named ivdiff1. This matrix captures the outcomes of the two-stage instrumental variable regression facilitated by the ivregress 2sls command. The regression equation includes explanatory variables D.Theilc lnarea LnDistCPH Lat Long LnDistCoast i.region, mirroring the structure of OLS regressions. Moreover, the specification (BygLG = MLmean) designates BygLG as the endogenous variable, with MLmean serving as its instrumental variable. An instrumentalization that is crucial to address potential endogeneity concerns and fortifying the validity of the estimations. The use of the conditional if year==1834 filters the data, retaining only observations where the year variable is equal to 1834. To account for heteroskedasticity and autocorrelation, the vce(robust) command employs a robust variance-covariance matrix. Subsequently, we encounter the lines of code that initially appeared daunting, but now, through regular utilization, their functionality has become comprehensible. Indeed, the estat firststage command is utilized to display statistics from the first-stage of the IV regression, offering insights into the validity of the instrumental variables. Further, the mat fstat = r(singleresults) line stores the first-stage results in a matrix named fstat, facilitating a comprehensive view of the instrumental variable performance. Lastly, the estadd scalar Fstat = fstat[1,4] command introduces a new scalar variable Fstat into the main regression results. This step extracts the F-statistic from the first-stage matrix fstat and assigns it to Once again, for a more comprehensive understanding of the F-statistics and its purpose, referring to the preceding Section 3.2.2 is recommended. We would like to remind you that the results can be independently observed directly using the same code without the qui option, starting from reg or ivreg. This underscores the flexibility of the code, allowing researchers to directly access and analyze the regression outcomes without relying on the table generation step. Formatting Table A.4, an additional but optional step ***Table A.4*** ***Formation of Table A.4*** esttab ols1 fiv1 ivdiff1 ivdiff2, se star(* 0.10 ** 0.05 *** 0.01) b(3) r2 var(15) /// model(11) wrap keep(MLmean BygLG) mtitles(&quot;OLS&quot; &quot;first stage&quot; /// &quot;second stage (diffTheil)&quot; &quot;second stage (diffAggTheil)&quot; ) stats(N r2 Fstat, /// labels(&quot;Observations&quot; &quot;R-squared&quot; &quot;KP F-statistic&quot;) fmt(%9.0fc 2 2)) /// indicate(&quot;Region FE = 2.region&quot; &quot;Geography = LnDistCPH&quot; , labels(Y N)) label In the next step of our analysis, we construct a comprehensive table to concisely present the findings from our prior regression analyses. It’s important to emphasize that this stage isn’t isolated but rather complements the groundwork laid in the preceding steps, where variables were defined and regressions were conducted. The resultant table serves as a visual tool to depict the relationships identified in our regression models, thereby enhancing the clarity and communicative impact of our analytical insights. In this stage, we use the esttab command to generate a comprehensive table summarizing the results from various models, including OLS, first-stage IV, and second-stage IV regressions. The specified models, denoted as ols1 fiv1 ivdiff1 ivdiff2, capture different aspects of the regressions. The se star(* 0.10 ** 0.05 *** 0.01) option is employed to display standard errors with significance stars, where asterisks indicate different levels of significance with * for 0.10, ** for 0.05, and *** for 0.01. Setting b(3) limits, once again, the number of decimal places for coefficient estimates to 3, contributing to a cleaner presentation. Including the R-squared r2 and limiting the number of decimals for the R-squared to 15 digits var(15) provide additional insights into the explanatory power of the models. Furthermore, the model(11) option ensures that a maximum of 11 different models is displayed in the table. To accommodate long variable names, the wrap command is utilized, allowing for a more organized presentation. Moreover, the keep(MLmean BygLG) option specifies the variables to be included in the table, focusing on MLmean and BygLG. Model titles are designated using the mtitles() option for clarity and organization. Additional statistics, such as the number of observations (N), R-squared, and the F-statistic, are included using the stats(N r2 Fstat, labels(\"Observations\" \"R-squared\" \"KP F-statistic\")fmt(%9.0fc 2 2)) option, providing a more comprehensive overview. Moreover, indicator variables for fixed effects related to a specific region – here 2 representing Jutland – 2.region and geography LnDistCPH are incorporated using the indicate() option. The labels(Y N)) allows to specify these fixed effects as Y if the fixed effects are included or as N if they are not This offers further control in understanding the impact of these factors on the results. Finally, the label option ensures that variable labels are included in the table, enhancing the interpretability of the presented information. Once again, in the provided replication code by the authors, no specific instruction regarding the exportation of tables has been included. Therefore, we recommend users to add the command using \"NameOfYourTable.tex\" right after the last variable mentioned in esttab, just before the comma in front of se star, if they intend to export the table in LaTeX format. Alternatively, users can use the command using NameOfYourTable.txt\", at the same location in the code, if they prefer the table in a text format. This flexibility allows users to choose the desired output format for the tables based on their specific needs. This last step to form a table can be employed in the context of other studies to generate a clear and visually appealing table summarizing regression results. However, it is crucial to emphasize that this step is not indispensable for obtaining replication results. And although we conducted a multitude of regressions in Section 3.5.1, it is noteworthy that the final Table A.4 does not include the ols2 regression. Finally, the results can be independently used directly with the same code without the qui option, starting from reg or ivreg. This highlights the flexibility of the code, allowing researchers to directly access and analyze the regression outcomes without relying on the table generation step. While the table contributes to a more organized presentation, it is not a prerequisite for the replication process, providing users with the option to tailor their analysis based on specific preferences and requirements. Our thorough replication process aimed to transparently convey each step of the analysis, ensuring clarity and reproducibility. We hope that our detailed descriptions provide a comprehensive understanding of the code and methodology employed. Moreover, we hope that this clarity enhances the accessibility of the article and facilitates further examination and validation by other researchers. We trust that transparency is crucial for fostering rigorous and collaborative research practices. Authors of the replication: Auvray Adrien, Carette Anne-Laure, Tarlapan Alina Date: December 2023 "],["regression-discontinuity-design.html", "Chapter 4 Regression Discontinuity Design ", " Chapter 4 Regression Discontinuity Design "],["the-persistent-effects-of-perus-mining-mita.html", "4.1 The Persistent Effects of Peru’s Mining Mita", " 4.1 The Persistent Effects of Peru’s Mining Mita Dell, M. (2010). The Persistent Effects of Peru’s Mining Mita. Econometrica, 78(6), 1863-1903. Retrieved 2023-11-20, from http://www.jstor.org/stable/40928464 Highlights Dell (2010) examines the long run impacts of the mita, an extensive forced mining labor system in effect in Peru and Bolivia between 1573 and 1812. We will code a spatial regression discontinuity design that is used to compare outcomes of households on each side of the mita boundaries. This paper is a pioneering work in the application of spatial regression discontinuity designs and lays the foundations for a burgeoning literature on the impact of spatially delimited policies. In this document, we provide a detailed explanation on how to implement spatial RDD in your software Stata. A key element of our contribution consists in capturing Conley standard errors using the “x_ols” program (see Install x_ols line 142). In the appendix you will find the codes that allow the automatization of formatted output tables in a ready to publish style with minimal required manual manipulation using matrices and the “appendmodels\" program. Link to published article: https://www.jstor.org/stable/40928464 4.1.1 Introduction A useful method for analyzing the impact of a policy when it is implemented in a geographically delimited area is the Spatial Regression Discontinuity Design (Spatial RDD). The paper focuses on the causal effect of the mita, an extensive forced mining labor system in effect in Peru and Bolivia between 1573 and 1812, on household consumption and stunted growth in children. This identification strategy is based on a cut-off defined by geographic borders, specifically the assignment variables are examined by their distance from the mita boundary (the threshold in this research) using a discontinuity in longitude-latitude space. This framework is based on the assumption that all the characteristics of the treatment and the control groups, except the variables of interest, must vary smoothly at the mita cutoff to be able to do a comparison. Therefore, the treatment effect is firstly computed by using cubic polynomials in latitude and longitude in a multidimensional RD polynomial. Then, to strengthen the results, other two single geographical dimension specifications were used. The first one uses the cubic polynomial Euclidian distance to Potosì and the second one the cubic polynomial distance to the mita boundary. 4.1.2 Good Practices 4.1.2.1 Necessary libraries ssc install estout, replace x_ols (Appendix) appendmodels (Appendix) 4.1.2.2 Organization of the directories capture log close clear clear matrix global dirroot &quot;YOUR DIRECTORY GOES HERE&quot; global dirdata &quot;${dirroot}/&quot; global dirlogs &quot;${dirroot}/&quot; global dirtables &quot;${dirroot}/&quot; log using &quot;${dirlogs}/Replication_Dell.log&quot;, replace 4.1.2.3 Building the three different data frames use &quot;${dirdata}Dataset_Dell.dta&quot;, clear keep if gis_db==1 drop gis_db save &quot;${dirdata}gis_grid.dta&quot;, replace use &quot;${dirdata}Dataset_Dell.dta&quot;, clear keep if consumption_db==1 drop consumption_db save &quot;${dirdata}consumption.dta&quot;, replace use &quot;${dirdata}Dataset_Dell.dta&quot;, clear keep if height_db==1 drop height_db save &quot;${dirdata}height.dta&quot;, replace 4.1.3 Table 1 - Summary Statistics Table 1: Summary Statistics 4.1.3.1 Code for Table 1 A critical assumption for the identification strategy is that all the characteristics of the treatment and control vary smoothly at the mita cutoff. Table 1 presents summary statistics for sample characteristics within and outside of the mita boundaries. Different distances from the Mita boundaries are used in the selection of observations to include in each group. Reassuringly, there are no significant differences. We are going to use a loop to move through each distance specification. We are using for this the data gis_grid.dta. Then, with the command tabstat we are going to create a table with the mean and the number of observations for the elevation by group (pothuan_mita, which is 0 if it’s outside the Mita and 1 otherwise) and the same for the slope. This has the option not which is an abbreviation for nototal because we are not interested in the overall statistics. After this we’re going to regress the elevation and the slope with the dummy of pothuan_mita in order to capture the robust standard error of the difference in mean between the two groups. // Elevation and Slope foreach Y of num 100 75 50 25 { //Elevation use &quot;${dirdata}gis_grid.dta&quot;, clear drop if d_bnd&gt;`Y&#39; tabstat elev, by(pothuan_mita) statistics(mean n) not regress elev pothuan_mita, robust //Slope tabstat slope, by(pothuan_mita) statistics(mean n) not regress slope pothuan_mita, robust } We then repeat this with the quantity of indigenous people. We are going to be using the consumption.dta dataset. // Indigenous foreach Y of num 100 75 50 25 { use &quot;${dirdata}consumption.dta&quot;, clear tabstat QUE if (d_bnd&lt;`Y&#39; &amp; cusco!=1), by(pothuan_mita) statistics(mean n) not regress QUE pothuan_mita if (d_bnd&lt;`Y&#39; &amp; cusco!=1), robust cluster (ubigeo) } Finally, we are going to be capturing the Conley Standard Error. This is done using the program x_ols which we install at the beginning of the code. We are also going to do it for the elevation, slope and indigenous people. This command works by specifying the latitude (“xcord\"), longitude (”ycord\"), two cut points in each coordinate and the dependent (“elev\") and independent variable (”pothuan_mita\". xreg(2) and coord(2) are necessary, xreg denotes the number of regressor and coord the dimensions of the coordinates. For more information on this command: https://economics.uwo.ca/people/conley_docs/data_GMMWithCross_99/x_ols.ado //Conley Standard Errors // Elevation and Slope foreach Y of num 100 75 50 25 { // Elevation use &quot;${dirdata}gis_grid.dta&quot;, clear keep if elev&lt;. gen xcut1=1 gen ycut1=1 gen const=1 drop if d_bnd&gt;`Y&#39; x_ols xcord ycord xcut1 ycut1 elev const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 // Slope use &quot;${dirdata}gis_grid.dta&quot;, clear drop if slope==. gen xcut1=1 gen ycut1=1 gen const=1 drop if d_bnd&gt;`Y&#39; x_ols xcord ycord xcut1 ycut1 slope const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 } // Indigenous foreach Y of num 100 75 50 25 { use &quot;${dirdata}consumption.dta&quot;, clear drop if QUE==. drop if cusco==1 gen xcut1=1 gen ycut1=1 gen const=1 drop if d_bnd&gt;`Y&#39; x_ols x y xcut1 ycut1 QUE const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 } 4.1.4 Table 2 - Main Results Table 2: Main Results 4.1.4.1 Code for Table 2 The paper is interested in the impact of Mita on both economic outcomes (household consumption) and outcomes in terms of health (stunted growth) so both results are presented in the main table with the following code. Again, a loop is used to move through each distance specification. The loop ensures that the same regressions are run for each distance specification specified in the local Y. Within the loop, we use the two main datasets on consumption and height and run the same regressions on the two main dependent variables lhhequiv and desnu on the cubic polynomial of the observation’s district capital, controlling for relevant variables. The if conditions specify that the regression omits observations in Cusco and indicates the distance specification that should be used. The robust option specifies that heteroskedasticity-robust standard errors should be reported. The cluster (ubigeo) option indicates that standard errors should be clustered by district. local Y &quot;100 75 50&quot; foreach Y in `Y&#39; { // Log Consumption use &quot;${dirdata}consumption.dta&quot;, clear reg lhhequiv pothuan_mita x y x2 y2 xy x3 y3 x2y xy2 infants children adults elv_sh /// slope bfe4* if (cusco != 1 &amp; d_bnd &lt; `Y&#39;), robust cluster (ubigeo) // Stunted Growth use &quot;${dirdata}height.dta&quot;, clear reg desnu pothuan_mita x y x2 y2 xy x3 y3 x2y xy2 elv_sh slope bfe4* /// if (cusco != 1 &amp; d_bnd &lt; `Y&#39;), robust cluster (ubigeo) } 4.1.5 Table 3 - Robustness Results Table 3: Robustness 4.1.5.1 Code for Table 3 The multidimensional RD polynomial approach was a novel methodology at the time and therefore there was no empirical background able to state a priori why this strategy performs better. Given these concerns, two other single-dimension specifications were applied to examine the robustness of the findings. The first is the polynomial in distance to Potosi, which is likely to capture variation in unobservable characteristics but it is not the most precise approach in mapping RD setup. The second specification controls for a polynomial in distance to the Mita boundary, which is closer to traditional one-dimensional RD designs even if it lacks a historical explanation that states its relevancy. Looking at the results across the three specifications is not possible to reject that they are statistically identical and consequently admit the robustness of the main findings. Another time, we use the two main datasets on consumption and height to capture the causal effect on the dependent variables lhhequiv and desnu. In the loop, first, we consider the effect of the mita on the consumption by including in the regression the Euclidean distance to Potosi in its linear, quadratic, and cubic form (together with relevant control variables and excluding the observation in Cusco) within 50, 70 and 100 km (Y). The second regression is run using the same dependent variable and controls but using the distance to the mita boundary in its linear, quadratic, and cubic terms. The following two regressions have exactly the same specifications but as dependent variable is used stunted growth. Finally, the last two regressions are peculiar to the previous ones but use the observation at the three border limits (again 50, 75, and 100 km from the Mita border). local Y &quot;100 75 50&quot; foreach Y in `Y&#39; { //Log Consumption use &quot;${dirdata}consumption.dta&quot;, clear // Cubic polynomial in distance to Potosi reg lhhequiv pothuan_mita dpot dpot2 dpot3 infants children adults elv_sh slope bfe4* /// if (cusco!=1 &amp; d_bnd&lt;`Y&#39;),robust cluster (ubigeo) // Cubic polynomial in distance to the mita boundary reg lhhequiv pothuan_mita dbnd_sh dbnd_sh2 dbnd_sh3 infants children adults elv_sh /// slope bfe4* if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) // Stunted Growth use &quot;${dirdata}height.dta&quot;, clear // Cubic polynomial in distance to Potosi reg desnu pothuan_mita dpot dpot2 dpot3 elv_sh slope bfe4* if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), /// robust cluster (ubigeo) // Cubic polyonomial in distance to the mita boundary reg desnu pothuan_mita dbnd_sh dbnd_sh2 dbnd_sh3 elv_sh slope bfe4* /// if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) } reg desnu pothuan_mita dpot dpot2 dpot3 elv_sh slope bfe4* if (cusco!=1 &amp; border==1), /// robust cluster (ubigeo) reg desnu pothuan_mita dbnd_sh dbnd_sh2 dbnd_sh3 elv_sh slope bfe4* /// if (cusco!=1 &amp; border==1), robust cluster (ubigeo) 4.1.6 Appendix: Automatization of table outputs The following code allows the creation of each table without minimal manual manipulation. We use two possible approaches, one is through the use of matrices and the other through the use of “eststo\" with the”append models\" program written by Ben Jann (more information here: https://repec.sowi.unibe.ch/stata/estout/other/901.do). 4.1.6.1 Automatic Output for Table 1 eststo clear scalar drop _all clear matrix // Elevation foreach Y of num 100 75 50 25 { //Elevation use &quot;${dirdata}gis_grid.dta&quot;, clear drop if d_bnd&gt;`Y&#39; tabstat elev, by(pothuan_mita) statistics(mean) not save matrix insi_elev_`Y&#39;=r(Stat2) matrix outs_elev_`Y&#39;=r(Stat1) quietly regress elev pothuan_mita, robust matrix V=e(V) scalar se_elev_`Y&#39; = sqrt(V[1,1]) *Conley Standard Errors keep if elev&lt;. gen xcut1=1 gen ycut1=1 gen const=1 x_ols xcord ycord xcut1 ycut1 elev const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 scalar con_se_elev_`Y&#39;=con_se2 //Slope use &quot;${dirdata}gis_grid.dta&quot;, clear drop if d_bnd&gt;`Y&#39; tabstat slope, by(pothuan_mita) statistics(mean) not save matrix insi_slope_`Y&#39;=r(Stat2) matrix outs_slope_`Y&#39;=r(Stat1) tabstat slope, by(pothuan_mita) statistics(n) not save matrix N_insi_slope_`Y&#39;=r(Stat2) matrix N_outs_slope_`Y&#39;=r(Stat1) quietly regress slope pothuan_mita, robust matrix V=e(V) scalar se_slope_`Y&#39; = sqrt(V[1,1]) *Conley Standard Errors drop if slope==. gen xcut1=1 gen ycut1=1 gen const=1 x_ols xcord ycord xcut1 ycut1 slope const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 scalar con_se_slope_`Y&#39;=con_se2 } // Indigenous foreach Y of num 100 75 50 25 { use &quot;${dirdata}consumption.dta&quot;, clear tabstat QUE if (d_bnd&lt;`Y&#39; &amp; cusco!=1), by(pothuan_mita) statistics(mean) not save matrix insi_indig_`Y&#39;=r(Stat2)*100 matrix outs_indig_`Y&#39;=r(Stat1)*100 tabstat QUE if (d_bnd&lt;`Y&#39; &amp; cusco!=1), by(pothuan_mita) statistics(n) not save matrix N_insi_indig_`Y&#39;=r(Stat2) matrix N_outs_indig_`Y&#39;=r(Stat1) quietly regress QUE pothuan_mita if (d_bnd&lt;`Y&#39; &amp; cusco!=1), robust cluster (ubigeo) matrix V=e(V) scalar se_indig_`Y&#39; = sqrt(V[1,1])*100 drop if QUE==. drop if cusco==1 gen xcut1=1 gen ycut1=1 gen const=1 drop if d_bnd&gt;`Y&#39; x_ols x y xcut1 ycut1 QUE const pothuan_mita, xreg(2) coord(2) drop window epsilon dis1 dis2 scalar con_se_indig_`Y&#39;=con_se2*100 } // Inputting everything on a Matrix // Submatrix 1: Elevation matrix elev = (insi_elev_100, outs_elev_100, con_se_elev_100, insi_elev_75, /// outs_elev_75, con_se_elev_75, insi_elev_50, outs_elev_50, con_se_elev_50, /// insi_elev_25, outs_elev_25, con_se_elev_25 \\ ., ., se_elev_100, ., ., se_elev_75, ., /// ., se_elev_50, ., ., se_elev_25) // Submatrix 2: Slope matrix slope = (insi_slope_100, outs_slope_100, con_se_slope_100, insi_slope_75, /// outs_slope_75, con_se_slope_75, insi_slope_50, outs_slope_50, con_se_slope_50, /// insi_slope_25, outs_slope_25, con_se_slope_25 \\ ., ., se_slope_100, ., ., se_slope_75, /// ., ., se_slope_50, ., ., se_slope_25) // Submatrix 3: Number of observations for elevation and slope matrix num_slope = (N_insi_slope_100, N_outs_slope_100, ., N_insi_slope_75, /// N_outs_slope_75, ., N_insi_slope_50, N_outs_slope_50, ., N_insi_slope_25, /// N_outs_slope_25, .) // Submatrix 4: Indigenous matrix indig = (insi_indig_100, outs_indig_100, con_se_indig_100, insi_indig_75, /// outs_indig_75, con_se_indig_75, insi_indig_50, outs_indig_50, con_se_indig_50, /// insi_indig_25, outs_indig_25, con_se_indig_25 \\ ., ., se_indig_100, ., ., se_indig_75, /// ., ., se_indig_50, ., ., se_indig_25) // Submatrix 5: Number of observations for Indigenous matrix num_indig = (N_insi_indig_100, N_outs_indig_100, ., N_insi_indig_75, /// N_outs_indig_75, ., N_insi_indig_50, N_outs_indig_50, ., N_insi_indig_25, /// N_outs_indig_25, .) // Final Matrix matrix Table1 = (elev\\slope\\num_slope\\indig\\num_indig) matrix rownames Table1 = Elevation &quot;.&quot; Slope &quot;.&quot; Observations %Indigenous &quot;.&quot; Observations matrix colnames Table1 = Inside Outside &quot;Conley SE/SE&quot; Inside Outside &quot;Conley SE/SE&quot; /// Inside Outside &quot;Conley SE/SE&quot; Inside Outside &quot;Conley SE/SE&quot; // Show in command esttab matrix(Table1, fmt(&quot;0 0 2 0 0 2 0 0 &quot; &quot;0 0 2 0 0 2 0 0 &quot; &quot;2 2 2 2 0 2 2&quot;)), /// refcat(Elevation &quot;GIS Measures&quot; %Indigenous &quot;&quot;, nolab) /// coef(. &quot; &quot;) /// title(&quot;Table 1: Summary Statistics&quot;) nomti /// addn(&quot;The unit of observation is 20 × 20 km grid cells for the geospatial measures and the household for % indigenous. Conley standard errors for the difference in means between mita and non-mita observations are first in each SE columns. Robust standard errors for the difference in means are second. For % indigenous, the robust standard errors are corrected for clustering at the district level. The geospatial measures are calculated using elevation data at 30 arc second (1 km) resolution (SRTM (2000)). The unit of measure for elevation is 1000 meters and for slope is degrees. A household is indigenous if its members primarily speak an indigenous language in the home (ENAHO (2001)). In the first three columns, the sample includes only observations located less than 100 km from the mita boundary, and this threshold is reduced to 75, 50, and finally 25 km in the succeeding columns.&quot;) replace 4.1.6.2 Automatic Output for Table 2 eststo clear scalar drop _all clear matrix local Y &quot;100 75 50&quot; local j = 1 foreach Y in `Y&#39; { // Log Consumption use &quot;${dirdata}consumption.dta&quot;, clear quietly reg lhhequiv pothuan_mita x y x2 y2 xy x3 y3 x2y xy2 infants children adults /// elv_sh slope bfe4* if (cusco != 1 &amp; d_bnd &lt; `Y&#39;), robust cluster (ubigeo) local i = 1 scalar Nclusters_`j&#39;_`i&#39; =e(N_clust) scalar N_`j&#39;_`i&#39; =e(N) scalar R_`j&#39;_`i&#39; =e(r2) matrix V=e(V) matrix B=e(b) scalar se_`j&#39;_`i&#39; = sqrt(V[1,1]) scalar b_`j&#39;_`i&#39; = (B[1,1]) local i = `i&#39; + 1 // Stunted Growth use &quot;${dirdata}height.dta&quot;, clear quietly reg desnu pothuan_mita x y x2 y2 xy x3 y3 x2y xy2 elv_sh slope bfe4* /// if (cusco != 1 &amp; d_bnd &lt; `Y&#39;), robust cluster (ubigeo) scalar Nclusters_`j&#39;_`i&#39; =e(N_clust) scalar N_`j&#39;_`i&#39; =e(N) scalar R_`j&#39;_`i&#39; =e(r2) matrix V=e(V) matrix B=e(b) scalar se_`j&#39;_`i&#39; = sqrt(V[1,1]) scalar b_`j&#39;_`i&#39; = (B[1,1]) local j = `j&#39; + 1 } matrix Main = (b_1_1, b_2_1, b_3_1 \\ se_1_1, se_2_1, se_3_1 \\ R_1_1, R_2_1, R_3_1 \\ /// N_1_1, N_2_1, N_3_1 \\ Nclusters_1_1, Nclusters_2_1, Nclusters_3_1 \\ b_1_2, b_2_2, /// b_3_2 \\ se_1_2, se_2_2, se_3_2 \\ R_1_2, R_2_2, R_3_2 \\ N_1_2, N_2_2, N_3_2 \\ /// Nclusters_1_2, Nclusters_2_2, Nclusters_3_2) matrix rownames Main = Mita1 &quot;.&quot; R2 &quot;Number of Observations&quot; &quot;Number of Clusters&quot; Mita2 /// &quot;.&quot; R2 &quot;Number of Observations&quot; &quot;Number of Clusters&quot; matrix colnames Main = &quot;&lt;100 km of Bound.&quot; &quot;&lt;75 km of Bound.&quot; &quot;&lt;50 km of Bound.&quot; esttab matrix(Main, fmt(&quot;3 3 3 0 0 3 3 3 0 0&quot;)), /// model(17) /// varwidth(40) /// not /// refcat(Mita1 &quot;Log Equiv. Household Consumption (2001)&quot; /// Mita2 &quot;Stunted Growth, Children 6-9 (2005)&quot;, nolab) /// coef(. &quot; &quot; Mita1 &quot;Mita&quot; Mita2 &quot;Mita&quot;) nomti /// title(&quot;Table 2: Main Results for replication&quot;) 4.1.6.3 Automatic Output for Table 3 eststo clear scalar drop _all clear matrix local Y &quot;100 75 50&quot; local i=1 foreach Y in `Y&#39; { //Log Consumption use &quot;${dirdata}consumption.dta&quot;, clear ren pothuan_mita row1 //Cubic polynomial in distance to Potosi eststo id`i&#39;: quietly reg lhhequiv row1 dpot dpot2 dpot3 infants children adults /// elv_sh slope bfe4* if (cusco!=1 &amp; d_bnd&lt;`Y&#39;),robust cluster (ubigeo) quietly matrix R_`i&#39; =e(r2) quietly scalar Nclusters_`i&#39; = e(N_clust) quietly scalar N_`i&#39; = e(N) //Cubic polynomial in distance to the mita boundary ren row1 row2 local i = `i&#39;+ 1 eststo id`i&#39;: quietly reg lhhequiv row2 dbnd_sh dbnd_sh2 dbnd_sh3 infants children /// adults elv_sh slope bfe4* if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) quietly matrix R_`i&#39; =e(r2) // Stunted Growth use &quot;${dirdata}height.dta&quot;, clear //Cubic polynomial in distance to Potosi ren pothuan_mita row1 local i = `i&#39;+ 1 eststo id`i&#39;: quietly reg desnu row1 dpot dpot2 dpot3 elv_sh slope bfe4* /// if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) quietly matrix R_`i&#39; =e(r2) quietly scalar Nclusters_`i&#39; = e(N_clust) quietly scalar N_`i&#39; = e(N) //Cubic polyonomial in distance to the mita boundary ren row1 row2 local i = `i&#39;+ 1 eststo id`i&#39;: quietly reg desnu row2 dbnd_sh dbnd_sh2 dbnd_sh3 elv_sh slope bfe4* /// if (cusco!=1 &amp; d_bnd&lt;`Y&#39;), robust cluster (ubigeo) quietly matrix R_`i&#39; =e(r2) local i = `i&#39;+ 1 } use &quot;${dirdata}height.dta&quot;, clear ren pothuan_mita row1 eststo id13: quietly reg desnu row1 dpot dpot2 dpot3 elv_sh slope bfe4* /// if (cusco!=1 &amp; border==1), robust cluster (ubigeo) quietly matrix R_13 =e(r2) quietly scalar Nclusters_13 = e(N_clust) quietly scalar N_13 = e(N) ren row1 row2 eststo id14: quietly reg desnu row2 dbnd_sh dbnd_sh2 dbnd_sh3 elv_sh slope bfe4* /// if (cusco!=1 &amp; border==1), robust cluster (ubigeo) quietly matrix R_14 =e(r2) local i=&quot;1 5 9 3 7 11 13&quot; local j=&quot;2 6 10 4 8 12 14&quot; foreach i in `i&#39; { quietly matrix coln R_`i&#39; = R_1 } foreach j in `j&#39; { quietly matrix coln R_`j&#39; = R_2 } // Append Models local i=&quot;1 5 9 3 7 11 13&quot; local k=1 foreach i in `i&#39; { eststo c`k&#39;: appendmodels id`i&#39; id`++i&#39; quietly estadd local geoc &quot;Yes&quot; quietly estadd local bound &quot;Yes&quot; quietly estadd local obs = (N_`--i&#39;) quietly estadd local clu = (Nclusters_`i&#39;) estadd matrix R2_1=R_`i&#39; estadd matrix R2_2=R_`++i&#39; local k = `k&#39; + 1 } esttab c1 c2 c3 c4 c5 c6 c7, /// keep(row1 row2 R_1 R_2) /// coeflabels(row1 &quot;Mita&quot; row2 &quot;Mita&quot; R_1 &quot;R2&quot; R_2 &quot;R2&quot;) /// cells(b(star fmt(3)) se(par fmt(3)) R2_1(fmt(3)) R2_2(fmt(3))) /// star(* 0.1 ** 0.05 *** 0.01) noobs /// title(&quot;TABLE II LIVING STANDARDS&quot;) /// ml(&quot;&lt;100 km of Bound&quot; &quot;&lt;75 km of Bound&quot; &quot;&lt;50 km of Bound&quot; &quot;&lt;100 km of Bound&quot; /// &quot;&lt;75 km of Bound&quot; &quot;&lt;50 km of Bound&quot; &quot;Border District&quot;) /// model(17) varwidth(30) /// posth(&quot;Sample within: &quot; `&quot;{hline @width}&quot;&#39;) hlinechar(&quot;-&quot;) /// mgroups(&quot;&quot; &quot;Log Equiv. Household Consumption (2001)&quot; /// &quot;Stunted Growth, Children 6-9 (2005)&quot;, pattern(1 1 0 0 1 0 0) span) /// s(geoc bound clu obs, labels(&quot;Geo. controls&quot; /// &quot;Boundary F.E.s&quot; &quot;Clusters&quot; &quot;Observations&quot;)) /// order(row1 R_1 row2 R_2) /// refcat(row1 &quot;Panel B. Cubic Polynomial in Distance to Potosi&quot; row2 /// &quot;Panel C. Cubic Polynomial in Distance to Mita Boundary&quot;, nolabel) nonote /// addnote(&quot;The unit of observation is the household in columns 1–3 and the individual in columns 4–7. Robust standard errors, adjusted for clustering by district, are in parentheses. The dependent variable is log equivalent household consumption (ENAHO (2001)) in columns 1–3, and a dummy equal to 1 if the child has stunted growth and equal to 0 otherwise in columns 4–7 (Ministro de Educación (2005a)). Mita is an indicator equal to 1 if the household&#39;s district contributed to the mita and equal to 0 otherwise (Saignes (1984), Amat y Juniet (1947, pp. 249, 284)). Panel B includes a cubic polynomial in Euclidean distance from the observation&#39;s district capital to Potosí, and panel C includes a cubic polynomial in Euclidean distance to the nearest point on the mita boundary. All regressions include controls for elevation and slope, as well as boundary segment fixed effects (F.E.s). Columns 1–3 include demographic controls for the number of infants, children, and adults in the household. In columns 1 and 4, the sample includes observations whose district capitals are located within 100 km of the mita boundary, and this threshold is reduced to 75 and 50 km in the succeeding columns. Column 7 includes only observations whose districts border the mita boundary. 78% of the observations are in mita districts in column 1, 71% in column 2, 68% in column 3, 78% in column 4, 71% in column 5, 68% in column 6, and 58% in column 7. Coefficients that are significantly different from zero are denoted by the following system: *10%, **5%, and ***1%.&quot;) tex replace 4.1.6.4 Install “x_ols” and “appendmodels” programs x_ols capt prog drop x_ols program define x_ols version 6.0 #delimit ; /*sets `;&#39; as end of line*/ /*FIRST I TAKE INFO. FROM COMMAND LINE AND ORGANIZE IT*/ local varlist &quot;req ex min(1)&quot;; /*must specify at least one variable... all must be existing in memory*/ local options &quot;xreg(int -1) COord(int -1)&quot;; /* # indep. var, dimension of location coordinates*/ parse &quot;`*&#39;&quot;; /*separate options and variables*/ if `xreg&#39;&lt;1{; if `xreg&#39;==-1{; di in red &quot;option xreg() required!!!&quot;; exit 198}; di in red &quot;xreg(`xreg&#39;) is invalid&quot;; exit 198}; if `coord&#39;&lt;1{; if `coord&#39;==-1{; di in red &quot;option coord() required!!!&quot;; exit 198}; di in red &quot;coord(`coord&#39;) is invalid&quot;; exit 198}; /*Separate input variables: coordinates, cutoffs, dependent, regressors*/ parse &quot;`varlist&#39;&quot;, parse(&quot; &quot;); local a=1; while `a&#39;&lt;=`coord&#39;{; tempvar coord`a&#39;; gen `coord`a&#39;&#39;=``a&#39;&#39;; /*get coordinates*/ local a=`a&#39;+1}; local aa=1; while `aa&#39;&lt;=`coord&#39;{; tempvar cut`aa&#39;; gen `cut`aa&#39;&#39;=``a&#39;&#39;; /*get cutoffs*/ local a=`a&#39;+1; local aa=`aa&#39;+1}; tempvar Y; gen `Y&#39;=``a&#39;&#39;; /*get dep variable*/ local depend : word `a&#39; of `varlist&#39;; local a=`a&#39;+1; local b=1; while `b&#39;&lt;=`xreg&#39;{; tempvar X`b&#39;; local ind`b&#39; : word `a&#39; of `varlist&#39;; gen `X`b&#39;&#39;= ``a&#39;&#39;; local a=`a&#39;+1; local b=`b&#39;+1}; /*get indep var(s)...rest of list*/ /*NOW I RUN THE REGRESSION AND COMPUTE THE COV MATRIX*/ quietly{; /*so that steps are not printed on screen*/ /*(1) RUN REGRESSION*/ tempname XX XX_N invXX invN; scalar `invN&#39;=1/_N; if `xreg&#39;==1 {; reg `Y&#39; `X1&#39;, noconstant robust; mat accum `XX&#39;=`X1&#39;,noconstant; mat `XX_N&#39;=`XX&#39;*`invN&#39;; mat `invXX&#39;=inv(`XX_N&#39;)}; /* creates (X&#39;X/N)^(-1)*/ else{; reg `Y&#39; `X1&#39;-`X`xreg&#39;&#39;, noconstant; mat accum `XX&#39;=`X1&#39;-`X`xreg&#39;&#39;,noconstant; mat `XX_N&#39;=`XX&#39;*`invN&#39;; mat `invXX&#39;=inv(`XX_N&#39;)}; /* creates (X&#39;X/N)^(-1)*/ predict epsilon,residuals; /* OLS residuals*/ /*(2) COMPUTE CORRECTED COVARIANCE MATRIX*/ tempname XUUX XUUX1 XUUX2 XUUXt; tempvar XUUk; mat `XUUX&#39;=J(`xreg&#39;,`xreg&#39;,0); gen `XUUk&#39;=0; gen window=1; /*initializes mat.s/var.s to be used*/ local i=1; while `i&#39;&lt;=_N{; /*loop through all observations*/ local d=1; replace window=1; while `d&#39;&lt;=`coord&#39;{; /*loop through coordinates*/ if `i&#39;==1{; gen dis`d&#39;=0}; replace dis`d&#39;=abs(`coord`d&#39;&#39;-`coord`d&#39;&#39;[`i&#39;]); replace window=window*(1-dis`d&#39;/`cut`d&#39;&#39;); replace window=0 if dis`d&#39;&gt;=`cut`d&#39;&#39;; local d=`d&#39;+1}; /*create window*/ capture mat drop `XUUX2&#39;; local k=1; while `k&#39;&lt;=`xreg&#39;{; replace `XUUk&#39;=`X`k&#39;&#39;[`i&#39;]*epsilon*epsilon[`i&#39;]*window; mat vecaccum `XUUX1&#39;=`XUUk&#39; `X1&#39;-`X`xreg&#39;&#39;, noconstant; mat `XUUX2&#39;=nullmat(`XUUX2&#39;) \\ `XUUX1&#39;; local k=`k&#39;+1}; mat `XUUXt&#39;=`XUUX2&#39;&#39;; mat `XUUX1&#39;=`XUUX2&#39;+`XUUXt&#39;; scalar fix=.5; /*to correct for double-counting*/ mat `XUUX1&#39;=`XUUX1&#39;*fix; mat `XUUX&#39;=`XUUX&#39;+`XUUX1&#39;; local i=`i&#39;+1}; mat `XUUX&#39;=`XUUX&#39;*`invN&#39;; }; /*end quietly command*/ tempname V VV; mat `V&#39;=`invXX&#39;*`XUUX&#39;; mat `VV&#39;=`V&#39;*`invXX&#39;; matrix cov_dep=`VV&#39;*`invN&#39;; /*corrected covariance matrix*/ /*THIS PART CREATES AND PRINTS THE OUTPUT TABLE IN STATA*/ local z=1; local v=`a&#39;; di _newline(2) _skip(5) &quot;Results for Cross Sectional OLS corrected for Spatial Dependence&quot;; di _newline _col(35) &quot; number of observations= &quot; _result(1); di &quot; Dependent Variable= &quot; &quot;`depend&#39;&quot;; di _newline &quot;variable&quot; _col(13) &quot;ols estimates&quot; _col(29) &quot;White s.e.&quot; _col(42) &quot;s.e. corrected for spatial dependence&quot;; di &quot;--------&quot; _col(13) &quot;-------------&quot; _col(29) &quot;----------&quot; _col(42) &quot;-------------------------------------&quot;; while `z&#39;&lt;=`xreg&#39;{; tempvar se1`z&#39; se2`z&#39;; local beta`z&#39;=_b[`X`z&#39;&#39;]; local se`z&#39;=_se[`X`z&#39;&#39;]; gen `se1`z&#39;&#39;=cov_dep[`z&#39;,`z&#39;]; gen `se2`z&#39;&#39;=sqrt(`se1`z&#39;&#39;); di &quot;`ind`z&#39;&#39;&quot; _col(13) `beta`z&#39;&#39; _col(29) `se`z&#39;&#39; _col(42) `se2`z&#39;&#39;; scalar con_se`z&#39;=`se2`z&#39;&#39;; // ADDED BY THE REPLICATORS: This line is to // capture Conley S.E. local z=`z&#39;+1}; end appendmodels capt prog drop appendmodels program appendmodels, eclass *! version 1.0.0 14aug2007 Ben Jann // using first equation of model version 8 syntax namelist tempname b V tmp foreach name of local namelist { qui est restore `name&#39; mat `tmp&#39; = e(b) local eq1: coleq `tmp&#39; gettoken eq1 : eq1 mat `tmp&#39; = `tmp&#39;[1,&quot;`eq1&#39;:&quot;] local cons = colnumb(`tmp&#39;,&quot;_cons&quot;) if `cons&#39;&lt;. &amp; `cons&#39;&gt;1 { mat `tmp&#39; = `tmp&#39;[1,1..`cons&#39;-1] } mat `b&#39; = nullmat(`b&#39;) , `tmp&#39; mat `tmp&#39; = e(V) mat `tmp&#39; = `tmp&#39;[&quot;`eq1&#39;:&quot;,&quot;`eq1&#39;:&quot;] if `cons&#39;&lt;. &amp; `cons&#39;&gt;1 { mat `tmp&#39; = `tmp&#39;[1..`cons&#39;-1,1..`cons&#39;-1] } capt confirm matrix `V&#39; if _rc { mat `V&#39; = `tmp&#39; } else { mat `V&#39; = /// ( `V&#39; , J(rowsof(`V&#39;),colsof(`tmp&#39;),0) ) \\ /// ( J(rowsof(`tmp&#39;),colsof(`V&#39;),0) , `tmp&#39; ) } } local names: colfullnames `b&#39; mat coln `V&#39; = `names&#39; mat rown `V&#39; = `names&#39; eret post `b&#39; `V&#39; eret local cmd &quot;whatever&quot; end; exit; Authors of the replication: Alejandro Arciniegas Herrera, Marcella De Giovanni, Anselm Rabaté, Kenan Topalovic Date: December 2023 "],["political-fragmentation-and-government-stability-evidence-from-local-governments-in-spain.html", "4.2 Political Fragmentation and Government Stability: Evidence from Local Governments in Spain", " 4.2 Political Fragmentation and Government Stability: Evidence from Local Governments in Spain Carozzi, F., Cipullo, Da. &amp; Repetto, L. (2022). ”Political Fragmentation and Government Stability: Evidence from Local Governments in Spain.” American Economic Journal: Applied Economics, 14 (2): 23-50. Highlights The paper investigates the impact of political fragmentation on government stability. The authors employ a fuzzy Regression Discontinuity Design (RDD) model. It belongs to the the regression discontinuity design broad methodology, which is a quasi experimental method used to estimate the causal effect of an intervention over a threshold or a cutoff point. It enables the comparison of individuals immediately above and below this specific cutoff point to assess the impact of a treatment on a particular outcome. In a fuzzy RDD design the assignment rule determining how units are selected into the treatment is probabilistic. The probability of treatment exhibits a discontinuity at the cutoff, though not as a definitive 0 to 1 jump. In regards to the original replication package, we present here a simplified version of its main elements for an easy replication with only one dataset and one do-file. Through this exercise, on top of standard Stata commands, we present a more in-depth explanation on how to make results’ tables using the “esttab” command, make more complex graphs with the “twoway” command and, specially, discover some commands from the “rdrobust” package, key for RDD analysis in Stata. The research paper can be found here. The original replication package can be found here. 4.2.1 Introduction The replication of research papers plays a pivotal role for Economics students. It allows them to learn about the implementation of econometrics methods, and to enhance the use of statistical software programs to further develop empirical works. This document provides a replication exercise for a fuzzy regression discontinuity design (RDD) model that estimates the impact of political fragmentation on government stability. It builds on a paper by Carozzi, Cipullo and Repetto (2022). Specifically, this project aims to provide a comprehensive guide on executing an RDD model in Stata. It details the execution of the model through the use of commands and code, while explaining the fundamental concepts of this methodology in the process. The content is structured in six parts. Section one provides an explanation on RDD methodology and a summary of the paper. Section two briefly explains how to set up your Stata environment to be able to do smoothly run this replication. Section three explains how to obtain and present summary statistics. Section four addresses the empirical strategy, explaining how to plot discontinuity graphs. Section five presents the main results, while showing how to perform RDD regressions and store and present results in tables. Finally, section six discusses the importance of robustness checks and provides explanations on the helpful commands to perform them in the context of the presented methodology. 4.2.2 RDD Methodology and Paper Summary The regression discontinuity design (RDD) is a quasi experimental method used to estimate the causal effect of an intervention over a threshold or a cutoff point. It enables the comparison of individuals immediately above and below this specific cutoff point to assess the impact of a treatment on a particular outcome. The method relies on the assumption that individuals near the cutoff point share similar observed and unobserved characteristics. An RDD may adopt a fuzzy design, wherein the assignment rule determining how units are selected into the treatment is probabilistic. The probability of treatment exhibits a discontinuity at the cutoff, though not as a definitive 0 to 1 jump. The fuzziness in the design derives from factors such as imperfect compliance or manipulation of the selected threshold. In terms of results, an RDD estimates local average treatment effects around the cutoff point, where treatment and comparison individuals are most similar. In the paper we replicate, the authors adopt a fuzzy RDD model in order to study how political fragmentation affects government stability. The data employed is a panel of Spanish municipalities from 1979 to 2014. The time dimension corresponds to each legislature, indexed by the year of the corresponding municipal election (1979 to 2011). The main data sources consist of electoral records, data on individual mayors and mayoral changes, municipal demographics (population, density, etc.), and data on the composition of regional and national governments. Electoral outcomes in municipal elections were obtained from the Ministry of Internal Affairs and residential registry. The sample consists of municipalities with more than 250 inhabitants for a total of 51,000 elections. The final dataset includes 42,259 elections because of additional data restrictions. To obtain causal estimates of the effect of fragmentation – measured as the number of parties in the council – on government stability, they exploit the existence of a 5% vote-share threshold for admission to the local council. This threshold causes parties with vote-shares just below 5% to be excluded from the council, generating exogenous variation in the number of parties with representation. Results show that each additional party with representation in the parliament increases the probability that the incumbent government is unseated by 5 percentage points. 4.2.3 Good Practices Before Starting In this section, we advise on two procedures before starting an empirical analysis in Stata. First, the creation of folders, which assures an organized storage of the analysis’ inputs and outputs. Second, the installation of all packages required to successfully carry out the replication. Folder creation Create the appropriate folders and globals to store your datasets and results. A global is a named storage location that can hold a value or a string of text. We use the command “global” to define it and the prefix “$” to access it. clear all set more off global dt &quot;C:\\Users\\Dell\\Desktop\\laurine_stata_files\\CCR&quot; Package Installation To carry out the replication of this paper, the following packages must be installed: ssc install missings // Provides tools and commands for working with missing data. ssc install rdrobust // Provides tools and commands to execute a regression discontinuity // design methodology. ssc install ivreg2 // Extends Stata’s built-in instrumental variables (IV) estimation // capabilities. ssc install estout // Provides additional formatting and exporting options for regression // results. ssc install outreg2 // Provides additional options formatting regression results for // output in tables. ssc install ranktest // Provides additional tests for rank-related issues, such as rank // correlation coefficients. 4.2.4 Descriptive Statistics After setting the Stata environment, we can begin to explore the data characteristics through the computing of descriptive statistics to achieve the replication of Table 1. The dataset provided for this replication exercise contains two samples: The first sample, used to make the descriptive statistics, consists of a town panel by election year. Therefore, each observation corresponds to a town on an specific election year. To access it, either keep at the beginning only the observations for which the sample indicator is equal to 1 or state it as a condition at the end of every estimation code line. We suggest using the first option: keep if Descrip_sample == 1 The second sample, used to produce the main results and robustness tests for the paper, consists of a party dataset, where in each observation a party appears once per town and election year. To access it, either keep at the beginning only the observations for which the sample indicator is equal to 1 or state it as a condition at the end of every estimation code line. We suggest using the first option: keep if Mainresults_sample == 1 Table 1: Descriptive Statistics Now, in order to replicate each panel from Table 1. Descriptive Statistics we first open the dataset and, as stated before, keep only the observations needed for the replication of the descriptive statistics: use &quot;$dt\\Dataset_CCR.dta&quot;, clear keep if Descrip_sample == 1 Panel A. General information We now use the ”preserve” command to create a copy of the dataset in the Stata memory, so that any changes or modifications will only affect this temporary copy. This is done as subsequently the variable “election year” is dropped and the town duplicates too. This leaves us with a new dataset that corresponds to one observation per town and their respective population, surface and number of elections averages measures. preserve drop election_year duplicates drop id_towns, force Now, we compute some summary statistics (mean, standard deviation, minimum, and maximum) using the “tabstat” command and store them in a matrix with the help of the “eststo: estpost” commands. After, we restore the dataset to its initial state before the “preserve” command was initialized. eststo clear //Clear any previously stored matrix eststo panelA: estpost tabstat population_average surface number_elections, /// stat(mean sd min max) columns(statistics) restore Finally, we can replicate the table of Panel A of the Descriptive statistics table using the “esttab” command, which takes stored results from one or more estimation commands and formats them into a table. esttab panelA, cells((mean(fmt(%18.2fc)) sd(fmt(2)) min(fmt(1)) max(fmt(0)))) nomtitles /// nonum collabels(&quot;Mean&quot; &quot;Std. dev.&quot; &quot;Min&quot; &quot;Max&quot;) /// title(&quot;TABLE 1. DESCRIPTIVE STATISTICS&quot;) /// refcat(population_average &quot;A. General information&quot;, nolabel) /// obslast varwidth(35) label /// addnotes(Notes: &quot;Panel A provides descriptives at the municipal level for all municipalities that appear at least once in our sample.&quot;) Explanation of “esttab” syntaxis and options: cells: specifies which results to show. nomtitles: removes the variable names in the columns’ titles. nonum: removes the numbering rows from the columns. collabels: adds custom column labels. title: adds a title for the table. refcat: creates an additional line with descriptions above some certain variables /groups of variables. obslast: puts the number of observations last. varwidth: specifies the width of the variable names displayed in the table. label: shows labels instead of variable names. addnotes: adds footnotes to the table. The same computation of the summary statistics, storage and tabulation process is repeated for the remaining panels. Panel B. Municipal Elections and Local Government eststo panelB: estpost tabstat nparties nparties_seats seats_total mocion_5_100 /// abs_majority PP_mayor PSOE_mayor IU_mayor CIU_mayor , stat(mean sd min max) /// columns(statistics) esttab panelB, cells((mean(fmt(%18.2fc)) sd(fmt(2)) min(fmt(1)) max(fmt(0)) )) nomtitles /// nonum collabels(&quot;Mean&quot; &quot;Std. dev.&quot; &quot;Min&quot; &quot;Max&quot;) /// refcat(nparties &quot;B. Municipal Elections and Local Government&quot;, nolabel) /// obslast varwidth(35) label /// addnotes(Notes: &quot;Panel B provides descriptives on electoral outcomes at the municipality-council level.&quot;) Panel C.1 - Local Government - Stable Mayor eststo panelC1: estpost tabstat abs_majority nparties_seats if mocion_5==0, /// stat(mean sd min max) columns(statistics) esttab panelC1, cells((mean(fmt(%18.2fc)) sd(fmt(2)) min(fmt(1)) max(fmt(0)) )) /// nomtitles nonum collabels(&quot;Mean&quot; &quot;Std. dev.&quot; &quot;Min&quot; &quot;Max&quot;) refcat(abs_majority /// &quot;C1. Local Government - Stable Mayor&quot;, nolabel) obslast varwidth(35) label /// addnotes(Notes: &quot;Panel C splits this sample into councils that approved at least one vote of no confidence during the term (C2), and those that did not(C1)&quot;) Panel C.2 - Local Government - Vote of No Confidence eststo panelC2: estpost tabstat abs_majority nparties_seats if mocion_5==1, /// stat(mean sd min max) columns(statistics) esttab panelC2, cells((mean(fmt(%18.2fc)) sd(fmt(2)) min(fmt(1)) max(fmt(0)) )) /// nomtitles nonum collabels(&quot;Mean&quot; &quot;Std. dev.&quot; &quot;Min&quot; &quot;Max&quot;) /// refcat(abs_majority &quot;C2. Local Government - Vote of No Confidence&quot;, nolabel) /// obslast varwidth(35) label /// addnotes(Notes: &quot;Panel C splits this sample into councils that approved at least one vote of no confidence during the term (C2), and those that did not (C1)&quot;) 4.2.5 Empirical Strategy After having carried out the descriptive statistics analysis, in this section we dive into the empirical strategy of the research paper. First, we describe the model specification, then we provide explanations on how to plot RDD graphs and, finally, we address how to perform the first stage. The paper employs the following main specification: \\[\\begin{equation} \\tag{1} Y_{it} = α_{1} + τ_{1}N_{it} + β_{1}V_{pit} + β_{2}V_{pit}D_{pit} + π_{it} \\end{equation}\\] Yit an indicator equal to 1 if the mayor of municipality i is unseated and replaced by a new mayor during term t – to the measure of fragmentation. This corresponds to the variable “mocion 5” in the dataset. Nit the number of parties with seats in the council. This corresponds to the variable “nparties seats” in the dataset. Vpit the running variable, representing the difference between the vote-share of each party p and 5% in each municipality i and for each term t. This corresponds to the variable “rv” in the dataset. The number of parties N is instrumented with an indicator D equal 1 for a party being above the 5% threshold. This corresponds to the variable “d” in the dataset. First stage estimation: \\[\\begin{equation} \\tag{2} N_{it} = α_{0} + γ_{1}D_{pit} + δ_{1}V_{pit} + δ_{2}V_{pit}D_{pit} + u_{pit} \\end{equation}\\] Figure 1 shows both the first stage and the reduced form graphs. These figures are relevant on RDD analysis, since they provide a visual representation of the existence of a discontinuity at the threshold. Figure 1: The effect of fragmentation on instability - first stage and reduced form (RD graph) In order to replicate these graphs, the mains results and the robustness checks, first we are going to keep only the observations needed. use &quot;$dt\\Dataset_CCR.dta&quot;, clear keep if Mainresults_sample == 1 Figure 1: Top Panel Since this is a fuzzy design, the first panel plots the running variable against the variable that it will instrument. In this case, the running variable against the number of party seats in council. In order to do an RDD plot, it is necessary to establish a bandwidth (to take into account only observations close to the threshold) and if there are many, sort the observations into bins to make them easier to plot. preserve local increment = 0.0025 // This is done to create the bins of the running variable, which // refer to intervals in which the data is grouped or divided. egen rv_bin = cut(rv), at(-0.025(`increment&#39;)0.025) // Create a new variable rv_bin that // categorizes the variable rv into 7 // bins based on the specified range // and increment // Generate x-axis points as the average within the bin: egen nparties_mean=mean(nparties_seats) if nparties_seats != ., by(rv_bin) replace rv_bin = rv_bin+`increment&#39;/2 // Center bins in the midpoint instead of left-end Plotting the First Stage Graph Now we create a two-way graph with linear fit confidence intervals (lfitci) and scatter points (scatter) twoway (lfitci nparties_seats rv if rv&lt;=0 &amp; rv&gt;=-0.025, lwidth(thick) /// fintensity(inten80) lcolor(gs6) alcolor(none) acolor(gs12*0.1) fcolor(gs14) /// alp(dash) ) (lfitci nparties_seats rv if rv&gt;0 &amp; rv&lt;=0.025 , lwidth(thick) /// fintensity(inten80) lcolor(gs6) alcolor(none) acolor(gs12*0.1) fcolor(gs14) /// alp(dash)) (scatter nparties_mean rv_bin, msymbol(O) mlcolor(gs11) /// mfcolor(gs15) msize(medlarge ) xlabel(-.025(.025).025, labsize(large))) /// (scatteri 3 0 4 0, recast(line) ) , ylabel(3(.5)4, labsize(large)) /// ytitle(N. of parties, size(large)) xtitle(Distance to threshold, /// size(large)) legend(off) graph export &quot;$dt/First_stage_025_linear.png&quot;, replace restore Explanation of twoway syntaxis and options: lwidth(thick): specifies the line width as thick. intensity(inten80): sets the intensity of the confidence interval to 80%. lcolor(gs6): sets the line color to grayscale code 6. alcolor(none): specifies no color for the area under the line. **acolor(gs12*0.1): sets the color of the confidence interval area to grayscale code 12 with 10% opacity. fcolor(gs14): sets the color of the fit line to grayscale code 14. alp(dash): specifies a dashed line for the fit. msymbol(O): specifies circular markers for the scatter plot. mlcolor(gs11): sets the line color of the markers to grayscale code 11. mfcolor(gs15): sets the fill color of the markers to grayscale code 15. msize(medlarge): sets the marker size to mediumlarge. xlabel(-.025(.025).025, labsize(large)): specifies custom x-axis labels. scatteri 3 0 4 0 specifies a vertical line segment between the points (0, 3) and (0, 4). recast(line): instructs Stata to recast the scatter plot as a line plot. graph export**: exports the graph in a specified folder and designated format. Figure 1: Bottom Panel For the second stage we repeat the same but for the reduced form, plotting directly the Probability of No confidence Votes against the the running variable (Distance to threshold). preserve local increment = 0.0025 egen rv_bin = cut(rv), at(-0.025(`increment&#39;)0.025) egen mocion_mean=mean(mocion_5) if mocion_5 != ., by(rv_bin) replace rv_bin = rv_bin+`increment&#39;/2 // Reduced form graph twoway /// (lfitci mocion_5 rv if rv&lt;=0 &amp; rv&gt;=-0.025, lwidth(thick) fintensity(inten80) /// lcolor(gs6) alcolor(none) acolor(gs12*0.1) fcolor(gs14) alp(dash) ) /// (lfitci mocion_5 rv if rv&gt;0 &amp; rv&lt;=0.025 , lwidth(thick) fintensity(inten80) /// lcolor(gs6) alcolor(none) acolor(gs12*0.1) fcolor(gs14) alp(dash)) /// (scatter mocion_mean rv_bin, msymbol(O) mlcolor(gs11) mfcolor(gs15) msize(medlarge ) /// xlabel(-.025(.025).025, labsize(large))) (scatteri 0 0 0.06 0, recast(line) ) /// , ylabel(0(.02).06, labsize(large)) ytitle(P(No-confidence vote), size(large)) /// xtitle(Distance to threshold, size(large)) legend(off) graph export &quot;$dt/RF_mocion_025_linear.png&quot;, replace restore 4.2.6 Main Results Now, we address how to perform the 2SLS estimation present the results on tables. Table 2 reports the estimates of the reduced-form and second-stage coefficients, respectively in Panel A and Panel B. Table 2. Panel A: RF and 2SLS estimates - P(no confidence vote) To run only this part, import the dataset again and keep only the observations from the Main Results sample: use &quot;$dt\\Dataset_CCR.dta&quot;, clear keep if Mainresults_sample == 1 Then, create globals for the regression to be able to easily add as controls the surface and population (in logs) of the municipality, include election year and council seat fixed effects, and include the running variable “rv” and its interaction with the indicator D “rv d”. global weights_on = 1 global controls &quot;log_pop log_surface&quot; global fixed_effects &quot;i.election_year i.seats_total&quot; global rv &quot;rv rv_d&quot; Additionally, if we want to run the regression with each observation weighted by the inverse of the number of parties running in the election, as the authors of the paper do, we store the global $weights_on = 1. If this is the case, the next command lines sorts the data by town and the election year and generates a new variable which counts the number of observations within each combination of id_town and election_year, and then creates the weights. if $weights_on == 1 { bys id_towns election_year: gen count = _N g weights = 1/nparties global weights_bw &quot;weights(weights)&quot; global weights_reg &quot;[aw = weights]&quot; } if $weights_on == 0 { global weights_bw &quot;&quot; global weights_reg &quot;&quot; } Panel A: Reduced-Form results Before running the regressions, it is important to establish a bandwidth, which will restrict the sample used based on the distance of the observations to the threshold. We use the command “rdbwselect” for bandwidth selection using the robust regression discontinuity design methods. The option c(0): indicates that the running variable has a discontinuity at zero; p(1): specifies the order of the polynomial used in the local polynomial regression; fuzzy(): indicates the fuzzy regression discontinuity design and specifies the variable to be used as the fuzzy instrument; vce(): specifies that the standard errors should be clustered at the level specified; masspoints(off): specifies that mass points are turned off in the estimation; covs($controls): indicates that additional covariates should be included in the regression. eststo clear rdbwselect mocion_5 rv, c(0) p(1) fuzzy(nparties_seats) vce(cluster id_towns) /// kernel(uni) $weights_bw masspoints(off) covs($controls) local CCT_bw_l = e(h_mserd) // stores the value of the estimated bandwidth Now, we run each regression of our outcome variable on the running variable, the indicator and their interaction, restricting the analysis to the observations where the absolute value of the running variable is less than the bandwidth that was previously computed and stored in “CCT bw l”. //Simple regression. eststo: reg mocion_5 $rv d $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; Additionally, “estadd scalar” is used to add a scalar to the estimation results, and it is assigned the value of the previously calculated and stored bandwidth. Then an scalar is also added with the value of the mean of the variable mocion 5. summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) The addition of controls and fixed effects results in the different columns of the main results, which are stored in a matrix using the “eststo” command. //For this column, we include controls, but not fixed effects. eststo: reg mocion_5 $rv d $controls $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) //For this column, we include fixed effects, but not controls. eststo: reg mocion_5 $rv d $fixed_effects $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, /// cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) //For this column, we include both controls and fixed effects. eststo: reg mocion_5 $rv d $controls $fixed_effects $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, /// cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) We create the first part of the table with the main results using “esttab” (see pag.7) esttab, se(3) nolabel b(3) sfmt(0) keep(d) nostar nonotes /// refcat(d &quot;A. Reduced-form results&quot;, nolabel) /// s(vmean band N, label(&quot;Mean of dep.var.&quot; &quot;Bandwidth&quot; &quot;Obs.&quot; ) /// fmt(3 3 0)) coeflabels(d &quot;Above threshold&quot;) varwidth(35) /// title(&quot;Table 2. Reduced-form and 2SLS Estimates - Fragmentation and Stability&quot;) /// mtitles(&quot;Mayor uns.&quot; &quot;Mayor uns.&quot; &quot;Mayors uns.&quot; &quot;Mayor uns.&quot;) Explanation of the esttab syntax: se(3): displays standard errors instead of t-statistics with 3 decimal points. nolabel: to not use labels instead of variables names. b: displays point estimates. sfmt(): sets format(s) for scalars. keep: keep individual coefficients. nostar: suppress stars in the table footer. nonotes: suppress notes in the table footer. s: specifies the statistics or summary measures you want to include in the output. Table 2. Panel B: 2SLS results - Fragmentation and Stability Notes: Reduced-form and 2SLS estimates of the effect of the number of parties on the probability of unseating the mayor (equation 2). Sample restricted to elections in which the largest party receives less than 40 percent of valid votes. The dependent variable is an indicator taking value 1 if the mayor was unseated by a vote of no confidence during the legislature. Controls and FE are included as indicated in each column. Controls: surface and population (in logs). FE: number of available seats and election year fixed effects. The optimal bandwidth is calculated using the CCT method. Standard errors are clustered at the municipality level. Now, we replicate the different columns of Panel B, this time using an instrumental variable. As in Panel A, the bandwidth selection is done first with the “rdbwselect” command: eststo clear rdbwselect mocion_5 rv, c(0) p(1) fuzzy(nparties_seats) vce(cluster id_towns) /// kernel(uni) $weights_bw masspoints(off) covs($controls) local CCT_bw_l = e(h_mserd) Then, we estimate the second stage of the main specification with the help of the command “ivreg”. The instrument “d” for number of party seats in council is then specified in parenthesis (nparties seats = d). The estimation is also conditional on the running variable’s absolute value being less than the bandwidth that was previously computed and stored in “CCT bw l”. //Simple regression. eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, /// cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) Additional to the scalars with the bandwidth and the mean of the dependant variable, we use “estadd local” to add a local macro labeled as FE and controls with the sign “N” or “Y” to the estimation results. This to add to the output table to indicate whether that estimation contained Fixed Effects and/or controls. quietly estadd local FE &quot;N&quot; quietly estadd local controls &quot;N&quot; //For this column, we include controls, but not fixed effects. eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $controls $weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) quietly estadd local FE &quot;N&quot; quietly estadd local controls &quot;Y&quot; //For this column, we include fixed effects, but not controls. eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $fixed_effects $weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) partial($fixed_effects) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) quietly estadd local FE &quot;Y&quot; quietly estadd local controls &quot;N&quot; //For this column, we include both controls and fixed effects. eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $controls /// $fixed_effects $weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) /// partial($fixed_effects) estadd scalar band = `CCT_bw_l&#39; summ mocion_5 if e(sample), mean estadd scalar vmean = r(mean) quietly estadd local FE &quot;Y&quot; quietly estadd local controls &quot;Y&quot; To replicate the Panel B of Table 3 we use a similar code to the one presented previously to recreate Panel A: esttab, se(3) nolabel b(3) sfmt(0) keep(nparties_seats) nostar nonotes /// refcat(nparties_seats &quot;B. 2SLS results&quot;, nolabel) s(vmean band N FE controls, /// label(&quot;Mean of dep.var.&quot; &quot;Bandwidth&quot; &quot;Obs.&quot; &quot;Fixed Effects&quot; &quot;Controls&quot;) fmt(3 3 0)) /// coeflabels(nparties_seats &quot;N. Parties&quot;) varwidth(35) /// title(&quot;Table 2. Reduced-form and 2SLS Estimates - Fragmentation and Stability&quot;) /// mtitles(&quot;Mayor uns.&quot; &quot;Mayor uns.&quot; &quot;Mayors uns.&quot; &quot;Mayor uns.&quot;) /// addnotes(&quot;Notes: Reduced-form and 2SLS estimates of the effect of the number of parties on the probability of unseating the mayor (equation 2). Sample restricted to elections in which the largest party receives less than 40 percent of valid votes. The dependent variable is an indicator taking value 1 if the mayor was unseated by a vote of no confidence during the legislature. Controls and FE are included as indicated in each column. Controls: surface and population (in logs). FE: number of available seats and election year fixed effects. The optimal bandwidth is calculated using the CCT method.Standard errors are clustered at the municipality level.&quot;) 4.2.7 Robustness Checks In this last section, we explain how to perform two standard robustness checks for the RDD methodology among the five proposed by the paper, this to assess the sensitivity of the results to changes in model specifications or changes in the definition of the sample. By doing so, we ensure that the specific choices made during the analysis do not unduly influence the conclusions. As stated in Table 3, first, we select the municipalities with 17 or more seats in council to verify that the fragmentation effect persists when concentrating solely on the set of compliers, in which the sample is restricted to municipality-election pairs where the 5 percent threshold is likely to be enforceable. (i.e. municipalities with 17 or more seats in the council). Additionally, we present the “global quadratic polynomial” to be able to capture possible nonlinearities in the conditional expectation of the outcome, although it requires us to rely on more observations that are far from the threshold. Table 3. Robustness Checks To run only this part, previously import the dataset again, keep only the observations from the Main Results sample and run again the globals and weights presented in page 14 of this document. A. Total Seats over 17 In the first case, we only keep the sample in which the number of Council seats is greater than 17 to check the robustness. We store both regression results and IV regression results to be shown in different columns (see page 6 for the esttab and eststo explanations). eststo clear preserve keep if seats_total&gt;=17 rdbwselect mocion_5 rv, c(0) p(1) fuzzy(nparties_seats) vce(cluster id_towns) /// kernel(uni) $weights_bw covs($controls) local CCT_bw_l = e(h_mserd) local band1 = round(`CCT_bw_l&#39;,0.001) eststo: reg nparties_seats d $rv $fixed_effects $controls $weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd local band `band1&#39; eststo: ivreg2 mocion_5 $rv (nparties_seats = d) $fixed_effects $controls$weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) restore esttab, se(3) nolabel b(3) sfmt(0) keep(d nparties_seats) nostar nonotes model(35) /// refcat(d &quot;A. Large Councils Only (#seats &gt;= 17)&quot;, nolabel) s(band N, /// label(&quot;Bandwidth&quot; &quot;Obs.&quot;) fmt(3 0)) /// coeflabels(nparties_seats &quot;N. Parties&quot; d &quot;Above threshold&quot;) varwidth(36) /// title(&quot;Table 5. ROBUSTNESS CHECKS&quot;) /// mtitles(&quot;First-Stage&quot; &quot;2SLS Estimate (N. Parties)&quot;) /// addnotes(&quot;Notes: Column 1 shows the first-stage estimate of our instrument when estimating equation 3. Column 2 reports the associated 2SLS estimate of the effect of number of parties on stability obtained by estimating equation 2. Each panel corresponds to a different robustness check: A) estimates obtained restricting the sample to municipalities with 17 or more seats in the council. Standard errors clustered at the municipality level. Bandwidths obtained using the CCT method in all panels.&quot;) Table 3. Robustness Checks E. Global Quadratic Polynomial This check considers a global quadratic polynomial, where the estimates are obtained using a large 5% bandwidth and a quadratic polynomial in the running variable. To do this, first we generate two different variables which are the square of ”rv” (rv 2) and the square of ”rv” interacting with ”d” (rv d 2). Additionally, we define a global to store these values and use them in the regressions. Finally, we define and store a local macro with a 5% bandwidth. eststo clear preserve local CCT_bw_l = 0.05 gen rv_2=rv^2 gen rv_2_d=(rv^2)*d global rv_2 &quot;rv rv_d rv_2 rv_2_d&quot; local band5 = round(`CCT_bw_l&#39;,0.001) eststo: reg nparties_seats d $rv_2 $fixed_effects $controls $weights_reg /// if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) estadd local band `band5&#39; eststo: ivreg2 mocion_5 $rv_2 (nparties_seats = d) $fixed_effects /// $controls$weights_reg if abs(rv)&lt;`CCT_bw_l&#39;, cluster(id_towns) restore esttab, se(3) nolabel b(3) sfmt(0) keep(d nparties_seats) nostar nonotes model(35) /// refcat(d &quot;E. Global Quadratic Polynomial&quot;, nolabel) s(band N, /// label(&quot;Bandwidth&quot; &quot;Obs.&quot;) fmt(3 0)) /// coeflabels(nparties_seats &quot;N. Parties&quot; d&quot;Above threshold&quot;) varwidth(36) /// title(&quot;Table 5. ROBUSTNESS CHECKS&quot;) /// mtitles(&quot;First-Stage&quot; &quot;2SLS Estimate (N. Parties)&quot;) /// addnotes(&quot;Notes: Column 1 shows the first-stage estimate of our instrument when estimating equation 3. Column 2 reports the associated 2SLS estimate of the effect of number of parties on stability obtained by estimating equation 2. Each panel corresponds to a different robustness check: E) estimates obtained using a large 5% bandwidth and a quadratic polynomial in the running variable. Standard errors clustered at the municipality level. Bandwidths obtained using the CCT method in all panels.&quot;) Authors of the replication: Carolina Arboleda Lenis, Elif C¸anga, Mariana Navarro Torres, Martina Pugliese Date: December 2023 "]]
